@book{2013,
  title = {R-Help -- {{Main R Mailing List}}: {{Primary Help}}},
  year = {2013}
}

@book{2014,
  title = {{{WTF Visualizations}}},
  year = {2014}
}

@book{academictechnologyservices2013,
  title = {Comparing {{SAS}}, {{Stata}}, and {{SPSS}}},
  author = {{Academic Technology Services}},
  year = {2013},
  month = apr
}

@article{achituv2013,
  title = {Algorithms as Structural Metaphors: {{Reflections}} on the Digital-Cultural Feedback Loop},
  author = {Achituv, Romy},
  year = {2013},
  journal = {LEONARDO},
  volume = {46},
  number = {2},
  pages = {171--177}
}

@book{adhikaridenero2020,
  title = {Computational and {{Inferential Thinking}}: {{The Foundations}} of {{Data Science}}},
  shorttitle = {Computational and {{Inferential Thinking}}},
  author = {Adhikari, Ani and DeNero, John},
  year = {2020},
  urldate = {2021-07-19},
  abstract = {Computational and Inferential Thinking: The Foundations of Data Science  By Ani Adhikari and John DeNero with contributions by David Wagner and Henry Milner.  T},
  file = {/Users/amcnamara/Zotero/storage/FGBK9W9Y/intro.html}
}

@article{adhikarietal2021,
  title = {Interleaving {{Computational}} and {{Inferential Thinking}}: {{Data Science}} for {{Undergraduates}} at {{Berkeley}}},
  shorttitle = {Interleaving {{Computational}} and {{Inferential Thinking}}},
  author = {Adhikari, Ani and DeNero, John and Jordan, Michael I.},
  year = {2021},
  month = mar,
  journal = {arXiv:2102.09391 [cs]},
  eprint = {2102.09391},
  primaryclass = {cs},
  urldate = {2021-07-19},
  abstract = {The undergraduate data science curriculum at the University of California, Berkeley is anchored in five new courses that emphasize computational thinking, inferential thinking, and working on real-world problems. We believe that interleaving these elements within our core courses is essential to preparing students to engage in data-driven inquiry at the scale that contemporary scientific and industrial applications demand. This new curriculum is already reshaping the undergraduate experience at Berkeley, where these courses have become some of the most popular on campus and have led to a surging interest in a new undergraduate major and minor program in data science.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society},
  file = {/Users/amcnamara/Zotero/storage/E7RK4CX5/Adhikari et al. - 2021 - Interleaving Computational and Inferential Thinkin.pdf;/Users/amcnamara/Zotero/storage/EYGJQYBG/2102.html}
}

@incollection{agre1995,
  title = {The {{Social}} and {{Interactional Dimensions}} of {{Human-Computer Interfaces}}},
  author = {Agre, Philip E.},
  editor = {Thomas, Peter J.},
  year = {1995},
  pages = {67--106},
  publisher = {Cambridge University Press},
  chapter = {Conceptions of the User in Computer Systems Design}
}

@article{aisch2015,
  title = {The Best and Worst Places to Grow up: {{How}} Your Area Compares},
  author = {Aisch, Gregor and Bluth, Eric and Bloch, Matthew and Cox, Amanda and Quealy, Kevin},
  year = {2015},
  month = may,
  journal = {The New York Times}
}

@book{alexander1964,
  title = {Notes on the {{Synthesis}} of {{Form}}},
  author = {Alexander, Christopher},
  year = {1964},
  publisher = {Harvard University Press}
}

@inproceedings{alexandronetal2014,
  title = {Scenario-Based Programming: Reducing the Cognitive Load, Fostering Abstract Thinking},
  shorttitle = {Scenario-Based Programming},
  booktitle = {Companion {{Proceedings}} of the 36th {{International Conference}} on {{Software Engineering}}},
  author = {Alexandron, Giora and Armoni, Michal and Gordon, Michal and Harel, David},
  year = {2014},
  month = may,
  series = {{{ICSE Companion}} 2014},
  pages = {311--320},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2591062.2591167},
  urldate = {2022-01-24},
  abstract = {We examine how students work in scenario-based and object- oriented programming (OOP) languages, and qualitatively analyze the use of abstraction through the prism of the dif- ferences between the paradigms. The findings indicate that when working in a scenario-based language, programmers think on a higher level of abstraction than when working with OOP languages. This is explained by other findings, which suggest how the declarative, incremental nature of scenario-based programming facilitates separation of con- cerns, and how it supports a kind of programming that al- lows programmers to work with a less detailed mental model of the system they develop. The findings shed light on how declarative approaches can reduce the cognitive load involved in programming, and how scenario-based program- ming might solve some of the difficulties involved in the use of declarative languages. This is applicable to the design of learning materials, and to the design of programming lan- guages and tools.},
  isbn = {978-1-4503-2768-8},
  keywords = {Abstraction,Scenario-Based Programming},
  file = {/Users/amcnamara/Zotero/storage/3G8LTAYY/Alexandron et al. - 2014 - Scenario-based programming reducing the cognitive.pdf}
}

@techreport{aliagaetal2005,
  title = {Guidelines for {{Assessment}} and {{Instruction}} in {{Statistics Education}}: {{College Report}}},
  author = {Aliaga, Martha and Cobb, George and Cuff, Carolyn and Garfield, Joan and Gould, Robert and Lock, Robin H. and Moore, Tom and Rossman, Allan and Stephenson, Bob and Utts, Jessica and Velleman, Paul F. and Witmer, Jeffrey A.},
  year = {2005},
  institution = {American Statistical Association}
}

@book{allaire2014,
  title = {Manipulate: {{Interactive}} Plots for {{RStudio}}},
  author = {Allaire, J. J.},
  year = {2014}
}

@misc{allaireetal2022,
  title = {Quarto: {{Open}} Source Tools for Scientific and Technical Publishing},
  author = {Allaire, J. J. and Dervieux, Christophe and Sheidegger, Carlos and Teague, Charles and Xie, Yihui},
  year = {2022},
  urldate = {2022-07-20},
  abstract = {Quarto{\textregistered} is an open-source scientific and technical publishing system built on Pandoc.},
  howpublished = {https://quarto.org/},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/HKW564AH/quarto.org.html}
}

@article{allenseaman2007,
  title = {Likert {{Scales}} and {{Data Analyses}}},
  author = {Allen, Elaine and Seaman, Christopher},
  year = {2007},
  journal = {Quality progress},
  volume = {40},
  number = {7},
  pages = {64--65},
  urldate = {2021-10-01},
  file = {/Users/amcnamara/Zotero/storage/SP3CKGMA/likert-scales-and-data-analyses.html}
}

@incollection{allison2009,
  title = {The {{SAGE Handbook}} of {{Quantitative Methods}} in {{Psychology}}},
  author = {Allison, Paul D.},
  editor = {Millsap, Roger E. and {Maydeu-Olivares}, Alberto},
  year = {2009},
  publisher = {Sage Publications},
  chapter = {Missing Data},
  keywords = {to read}
}

@article{alspaughetal2019,
  title = {Futzing and {{Moseying}}: {{Interviews}} with {{Professional Data Analysts}} on {{Exploration Practices}}},
  shorttitle = {Futzing and {{Moseying}}},
  author = {Alspaugh, Sara and Zokaei, Nava and Liu, Andrea and Jin, Cindy and Hearst, Marti A.},
  year = {2019},
  month = jan,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {25},
  number = {1},
  pages = {22--31},
  issn = {1077-2626, 1941-0506, 2160-9306},
  doi = {10.1109/TVCG.2018.2865040},
  urldate = {2021-07-18},
  file = {/Users/amcnamara/Zotero/storage/IE8AEQXW/Alspaugh et al. - 2019 - Futzing and Moseying Interviews with Professional.pdf}
}

@article{alvarado2012,
  title = {Increasing Women's Participation in Computing at {{Harvy Mudd College}}},
  author = {Alvarado, Christine and Dodds, Zachary and {Libeskind-Hadas}, Ran},
  year = {2012},
  journal = {ACM Inroads},
  volume = {3},
  number = {4},
  pages = {55--64}
}

@book{AMA2006,
  title = {Beyond {{Crossroads}}: {{Implementing Mathematics Standards}} in the {{First Two}} Years of College},
  year = {2006},
  publisher = {American Mathematical Association of Two-Year Colleges},
  keywords = {college,education,mathematics}
}

@misc{americanstatisticalassociation2023,
  title = {Whats {{Going}} on in This {{Graph}}},
  author = {{American Statistical Association}},
  year = {2023},
  journal = {Default},
  urldate = {2023-01-11},
  howpublished = {https://www.amstat.org/whats-going-on-in-this-graph},
  file = {/Users/amcnamara/Zotero/storage/WEXZZWS7/whats-going-on-in-this-graph.html}
}

@article{anderson2001,
  title = {Permutation Tests for Linear Models},
  author = {Anderson, Marti J. and Robinson, John},
  year = {2001},
  journal = {Australian \& New Zealand Journal of Statistics},
  volume = {43},
  number = {1},
  pages = {75--88},
  abstract = {Several approximate permutation tests have been proposed for tests of partial regression coefficients in a linear model based on sample partial correlations. This paper begins with an explanation and notation for an exact test. It then compares the distributions of the test statistics under the various permutation methods proposed, and shows that the partial correlations under permutation are asymptotically jointly normal with means 0 and variances 1. The method of Freedman \& Lane (1983) is found to have asymptotic correlation 1 with the exact test, and the other methods are found to have smaller correlations with this test. Under local alternatives the critical values of all the appropriate permutation tests converge to the same constant, so they all have the same asymptotic power. Simulations demonstrate these theoretical results.},
  keywords = {statistics}
}

@book{anderson2014,
  title = {Post {{Industrial Journalism}}: {{Adapting}} to the {{Present}}},
  author = {Anderson, Chris and Bell, Emily and Shirky, Clay},
  year = {2014},
  month = dec
}

@techreport{angelli2013,
  title = {The {{Somerville STEAM Academy Innovation Plan}}},
  author = {Angelli, Chris and Desimone, Leo and Duffy, Shauna-Lynn and Gregory, Rob and Mikhak, Bakhtiar and Oteri, John and Resnick, Alec and Rogers, Chris and Santos, Marcus and Sweeting, Adam and Tatterson, Lisa and Woods, Karen},
  year = {2013},
  institution = {The Somerville STEAM Academy Innovation Planning Committee}
}

@book{apachesoftwarefoundation2016,
  title = {Apache {{Zeppelin}}},
  author = {{Apache Software Foundation}},
  year = {2016}
}

@inproceedings{armstrong2014,
  title = {Visualizing {{Statistical Mix Effects}} and {{Simpson}}'s {{Paradox}}},
  booktitle = {Proceedings of {{IEEE InfoVis}} 2014, {{IEEE}}},
  author = {Armstrong, Zan and Wattenberg, Martin},
  year = {2014},
  abstract = {We discuss how ``mix effects'' can surprise users of visualizations and potentially lead them to incorrect conclusions. This statistical issue (also known as ``omitted variable bias'' or, in extreme cases, as ``Simpson's paradox'') is widespread and can affect any visualization in which the quantity of interest is an aggregated value such as a weighted sum or average. Our first contribution is to document how mix effects can be a serious issue for visualizations, and we analyze how mix effects can cause problems in a variety of popular visualization techniques, from bar charts to treemaps. Our second contribution is a new technique, the ``comet chart,'' that is meant to ameliorate some of these issues.}
}

@techreport{arnold2015,
  title = {The {{StatRep}} System for Reproducible Research},
  author = {Arnold, Tim and Kuhfeld, Warren F.},
  year = {2015},
  institution = {SAS Institute, Inc}
}

@article{arnoldetal2022,
  title = {Bringing {{Complex Data Into}} the {{Classroom}}},
  author = {Arnold, Pip and Bargagliotti, Anna and Franklin, Christine and Gould, Robert},
  year = {2022},
  month = jul,
  journal = {Harvard Data Science Review},
  volume = {4},
  number = {3},
  issn = {2644-2353, 2688-8513},
  doi = {10.1162/99608f92.4ec90534},
  urldate = {2023-07-07},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/X7KWTHHD/Arnold et al. - 2022 - Bringing Complex Data Into the Classroom.pdf}
}

@article{arnoldfranklin2021,
  title = {What {{Makes}} a {{Good Statistical Question}}?},
  author = {Arnold, Pip and Franklin, Christine},
  year = {2021},
  month = jan,
  journal = {Journal of Statistics and Data Science Education},
  volume = {29},
  number = {1},
  pages = {122--130},
  publisher = {Taylor \& Francis},
  issn = {null},
  doi = {10.1080/26939169.2021.1877582},
  urldate = {2023-06-07},
  abstract = {The statistical problem-solving process is key to the statistics curriculum at the school level, post-secondary, and in statistical practice. The process has four main components: formulate questions, collect data, analyze data, and interpret results. The Pre-K-12 Guidelines for Assessment and Instruction in Statistics Education (GAISE) emphasizes the importance of distinguishing between a question that anticipates a deterministic answer and a question that anticipates an answer based on data that will vary, referred to as a statistical question. This article expands upon the Pre-K-12 GAISE distinction of a statistical question by addressing and identifying the different types of statistical questions used across the four components of the statistical problem-solving process and the importance of interrogating these different statistical question types. Since the publication of the original Pre-K-12 GAISE document, research has helped to clarify the purposes of questioning at each component of the process, to clarify the language of questioning, and to develop criteria for answering the question, ``What makes a good statistical question?''},
  keywords = {Investigative questions,Statistical problem-solving process,Statistical questions},
  file = {/Users/amcnamara/Zotero/storage/G38DF8FU/Arnold and Franklin - 2021 - What Makes a Good Statistical Question.pdf}
}

@book{aruliah2012,
  title = {Best {{Practices}} for {{Scientific Computing}}},
  author = {Aruliah, D. A. and Brown, C. Titus and Hong, Neil P. Chue and David, Matt and Guy, Richard T. and Haddock, Steven H. D. and Huff, Katy and Mitchell, Ian and Plumbley, Mark and Waugh, Ben and White, Ethan P. and Wilson, Greg and Wilson, Paul},
  year = {2012},
  month = oct,
  abstract = {Scientists spend an increasing amount of time building and using software. However, most scientists are never taught how to do this efficiently. As a result, many are unaware of tools and practices that would allow them to write more reliable and maintainable code with less effort. We describe a set of best practices for scientific software development that have solid foundations in research and experience, and that improve scientists' productivity and the reliability of their software.}
}

@article{asimov1985,
  title = {The Grand Tour: {{A}} Tool for Viewing Multidimensional Data},
  author = {Asimov, Daniel},
  year = {1985},
  journal = {SIAM Journal on Scientific and Statistical Computation},
  keywords = {data,data visualization,statistics}
}

@article{atkinson2013,
  title = {Downscaling in Remote Sensing},
  author = {Atkinson, Peter M.},
  year = {2013},
  journal = {International Journal of Applied Earth Observation and Geoinformation},
  volume = {22},
  pages = {106--114},
  abstract = {Downscaling has an important role to play in remote sensing. It allows prediction at a finer spatial resolution than that of the input imagery, based on either (i) assumptions or prior knowledge about the character of the target spatial variation coupled with spatial optimisation, (ii) spatial prediction through interpolation or (iii) direct information on the relation between spatial resolutions in the form of a regression model. Two classes of goal can be distinguished based on whether continua are predicted (through downscaling or area-to-point prediction) or categories are predicted (super-resolution mapping), in both cases from continuous input data. This paper reviews a range of techniques for both goals, focusing on area-to-point kriging and downscaling cokriging in the former case and spatial optimisation techniques and multiple point geostatistics in the latter case. Several issues are discussed including the information content of training data, including training images, the need for model-based uncertainty information to accompany downscaling predictions, and the fundamental limits on the representativeness of downscaling predictions. The paper ends with a look towards the grand challenge of downscaling in the context of time-series image stacks. The challenge here is to use all the available information to produce a down- scaled series of images that is coherent between images and, thus, which helps to distinguish real changes (signal) from noise.}
}

@book{authorcitation2015,
  title = {Title},
  author = {{Author Citation}},
  year = {2015}
}

@article{ayres2006,
  title = {Impact of Reducing Intrinsic Cognitive Load on Learning in a Mathematical Domain},
  author = {Ayres, Paul},
  year = {2006},
  month = apr,
  journal = {Applied Cognitive Psychology},
  volume = {20},
  number = {3},
  pages = {287--298},
  issn = {0888-4080, 1099-0720},
  doi = {10.1002/acp.1245},
  urldate = {2021-07-21},
  langid = {english}
}

@book{bachewickham2014,
  title = {\texttt{magrittr}: A Forward-Pipe Operator for R},
  author = {Bache, Stefan Milton and Wickham, Hadley},
  year = {2014}
}

@article{baglin2013,
  title = {Applying a Theoretical Model for Explaining the Development of Technological Skills in Statistics Education},
  author = {Baglin, James},
  year = {2013},
  journal = {Technology Innovations in Statistics Education},
  volume = {7},
  number = {2}
}

@article{baglin2013a,
  title = {Comparing {{Training Approaches}} for {{Technological Skill Development}} in {{Introductory Statistics Courses}}},
  author = {Baglin, James and Costa, Cliff Da},
  year = {2013},
  journal = {Technology Innovations in Statistics Education},
  volume = {7},
  number = {1},
  abstract = {Technology has transformed the modern introductory statistics course, but little is known about how students develop the skills required to use this technology. This study compares two different training approaches for learning to operate statistical software packages. Guided training (GT) uses direct instruction and explicit guidance during training, whereas active-exploratory training types, such as error-management training (EMT), promote self-directed exploration. Previous studies in general software training suggest that EMT outperforms GT at promoting adaptive skill transfer. This study recruited a sample of 115 psychology students enrolled in introductory statistics courses that ran concurrently across two campuses. These students completed weekly,one-hour training sessions learning to use the statistical package SPSS. In the final week of the semester, students completed an SPSS certification task to measure adaptive skill transfer. The EMT and GT approach was implemented in Campus A and B respectively. Due to non-random allocation, the covariates of gender, personal access, statistical knowledge, and training progress were taken into account when modeling adaptive transfer between training approaches. After controlling for these covariates, no difference in adaptive transfer was found between training approaches. The results suggest that improving access to statistical packages may provide a more powerful way to improve the development of technological skills over using different training approaches.}
}

@inproceedings{baker2002,
  title = {The Resilience of Overgeneralization of Knowledge about Data Representations},
  booktitle = {American {{Educational Research Association Conference}}},
  author = {Baker, Shaun and Corbett, Albert T. and Koedinger, Kenneth R.},
  year = {2002},
  keywords = {to read}
}

@inproceedings{baker2004,
  title = {Learning to {{Distinguish Between Representations}} of {{Data}}: {{A Cognitive Tutor}} That Uses {{Contrasting Cases}}},
  booktitle = {Proceedings of the 6th {{International Conference}} on {{Learning Sciences}}.},
  author = {Baker, Ryan Shuan and Corbett, Albert T. and Koedinger, Kenneth R},
  year = {2004},
  publisher = {International Society of the Learning Sciences},
  abstract = {Students often fail to learn crucial distinctions between different representations of data. For instance, many students learning about scatterplots consistently create representations which have the surface features of scatterplots but with informational content more appropriate for discrete bar graphs. Schwartz and Bransford (1998) have found that combining feature- based conceptual instruction with contrasting cases is an effective way to help students make conceptual distinctions. We adapt their approach to the domain of data representation and incorporate it into a cognitive tutoring curriculum. We show that this new curriculum improves learning more than a curriculum where the contrasts are not present.},
  keywords = {data,education,statistics}
}

@article{baker2017,
  title = {Scientific Computing: {{Code}} Alert},
  author = {Baker, Monya},
  year = {2017},
  journal = {Nature Jobs},
  pages = {563--565}
}

@inproceedings{bakker2002,
  title = {Route-Type and Landscape-Type Software for Learning Statistical Data Analysis},
  booktitle = {Proceedings of the 6th {{International Conference}} on {{Teaching Statistics}}},
  author = {Bakker, Arthur},
  year = {2002},
  abstract = {This paper contrasts two types of educational tools: a route-type series of so-called statistical minitools (Cobb et al., 1997) and a landscape-type construction tool, named Tinkerplots (Konold \& Miller, 2001). The design of the minitools is based on a hypothetical learning trajectory (Simon, 1995). Tinkerplots is being designed in collaboration with five mathematics curricula and is open to different approaches. Citing experiences from classroom-based research with students aged ten to thirteen, I show how characteristics of the two types of tools influence the instructional decisions that software designers, curriculum authors, and teachers have to make.}
}

@phdthesis{bakker2004,
  title = {Design Research in Statistics Education: {{On}} Symbolizing and Computer Tools},
  author = {Bakker, Arthur},
  year = {2004},
  month = may,
  school = {Universiteit Utrecht},
  keywords = {computer science,education,statistics,technology}
}

@article{bakker2005,
  title = {Diagrammatic Reasoning as the Basis for Developing Concepts: {{A}} Semiotic Analysis of Students' Learning about Statistical Distribution},
  author = {Bakker, Arthur and Hoffmann, Michael H. G.},
  year = {2005},
  journal = {Educational Studies in Mathematics},
  volume = {60},
  pages = {333--358},
  abstract = {In recent years, semiotics has become an innovative theoretical framework in mathematics education. The purpose of this article is to show that semiotics can be used to explain learning as a process of experimenting with and communicating about one's own representations (in particular `diagrams') of mathematical problems. As a paradigmatic example, we apply a Peircean semiotic framework to answer the question of how students develop a notion of `distribution' in a statistics course by `diagrammatic reasoning' and by forming `hypostatic abstractions', that is by forming new mathematical objects which can be used as means for communication and further reasoning. Peirce's semiotic terminology is used as an alternative to concepts such as modeling, symbolizing, and reification. We will show that it is a precise instrument of analysis with regard to the complexity of learning and communicating in mathematics classrooms.}
}

@article{bakker2011,
  title = {Lessons from Inferentialism for Statistics Education},
  author = {Bakker, Arthur and Derry, Jan},
  year = {2011},
  journal = {Mathematical thinking and learning},
  volume = {13},
  number = {1},
  pages = {5--26},
  keywords = {education,statistics}
}

@incollection{bakker2014,
  title = {Using {{Tools}} for {{Learning Mathematics}} and {{Statistics}}},
  author = {Bakker, Arthur},
  editor = {Wassong, T. and others},
  year = {2014},
  number = {11},
  publisher = {Springer Fachmedien Wiesbaden},
  abstract = {The availability of technology influences what people need to know about statistics, but do they need to know less, more or something different? As one piece in the jigsaw puzzle of this quest, this chapter focuses on the question of what student laboratory technicians in vocational education need to learn about statistics in the presence of technology. Through interviews with interns, intern supervisors and teachers, a questionnaire administered to interns, and workplace observations I have identified what statistical knowledge is taught and required. The knowledge required turned out to diverge across laboratories and to be highly influenced by the degree to which work is mediated by technology. For example, calibration and validation of measurement instruments is based on linear regression, but is often automated. Many computations are carried out on Excel sheets, but not all schools dedicate enough instruction time on spreadsheets. At least 30\% of the interns (N=300) felt insufficiently prepared in terms of mathematics or statistics.},
  chapter = {Implications of technology on what students need to know about statistics}
}

@article{ball2012,
  title = {Teaching Integrity in Empirical Research: {{A}} Protocol for Documenting Data Management and Analysis},
  author = {Ball, Richard and Medeiros, Norm},
  year = {2012},
  journal = {The Journal of Economic Education},
  volume = {43},
  number = {2}
}

@inproceedings{bardzell2012,
  title = {Critical Design and Critical Theory: {{The}} Challenge of Designing for Provocation},
  booktitle = {{{DIS}} 2012: {{In}} the Wild},
  author = {Bardzell, Shaowen and Bardzell, Jeffrey and Forlizzi, Jodi and Zimmerman, John and Antanitis, John},
  year = {2012}
}

@article{bargagliottietal2020,
  title = {Undergraduate {{Learning Outcomes}} for {{Achieving Data Acumen}}},
  author = {Bargagliotti, Anna and Binder, Wendy and Blakesley, Lance and Eusufzai, Zaki and Fitzpatrick, Ben and Ford, Maire and Huchting, Karen and Larson, Suzanne and Miric, Natasha and Rovetti, Robert and Seal, Kala and Zachariah, Thomas},
  year = {2020},
  month = may,
  journal = {Journal of Statistics Education},
  volume = {28},
  number = {2},
  pages = {197--211},
  issn = {1069-1898},
  doi = {10.1080/10691898.2020.1776653},
  urldate = {2021-07-23},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/AUNH7LBG/Bargagliotti et al. - 2020 - Undergraduate Learning Outcomes for Achieving Data.pdf}
}

@book{bargagliottietal2020a,
  title = {Pre-K-12 Guidelines for Assessment and Instruction in Statistics Education II (GAISE II)},
  author = {Bargagliotti, Anna and Franklin, Christine and Arnold, Pip and Gould, Robert and Johnson, Sheri and Perez, Leticia and Spangler, Denise A},
  year = {2020},
  publisher = {National Council of Teachers of Mathematics}
}

@article{bartrametal2022,
  title = {Untidy {{Data}}: {{The Unreasonable Effectiveness}} of {{Tables}}},
  shorttitle = {Untidy {{Data}}},
  author = {Bartram, Lyn and Correll, Michael and Tory, Melanie},
  year = {2022},
  month = jan,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {28},
  number = {1},
  pages = {686--696},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2021.3114830},
  abstract = {Working with data in table form is usually considered a preparatory and tedious step in the sensemaking pipeline; a way of getting the data ready for more sophisticated visualization and analytical tools. But for many people, spreadsheets --- the quintessential table tool --- remain a critical part of their information ecosystem, allowing them to interact with their data in ways that are hidden or abstracted in more complex tools. This is particularly true for data workers [61], people who work with data as part of their job but do not identify as professional analysts or data scientists. We report on a qualitative study of how these workers interact with and reason about their data. Our findings show that data tables serve a broader purpose beyond data cleanup at the initial stage of a linear analytic flow: users want to see and ``get their hands on'' the underlying data throughout the analytics process, reshaping and augmenting it to support sensemaking. They reorganize, mark up, layer on levels of detail, and spawn alternatives within the context of the base data. These direct interactions and human-readable table representations form a rich and cognitively important part of building understanding of what the data mean and what they can do with it. We argue that interactive tables are an important visualization idiom in their own right; that the direct data interaction they afford offers a fertile design space for visual analytics; and that sense making can be enriched by more flexible human-data interaction than is currently supported in visual analytics tools.},
  keywords = {Affordances,Analytics,Annotations,Cleaning,Data practices,Data visualization,Data workers,Interview study,Organizations,Sensemaking,Tabular data,Tools,Visual analytics,Visualization},
  file = {/Users/amcnamara/Zotero/storage/J2M2XJX8/Bartram et al. - 2022 - Untidy Data The Unreasonable Effectiveness of Tab.pdf;/Users/amcnamara/Zotero/storage/VZVTXLKU/9555457.html}
}

@inproceedings{bataneroetal1997,
  title = {{{EVOLUTION OF STUDENTS}}' {{UNDERSTANDING OF STATISTICAL ASSOCIATION IN A COMPUTER-BASED TEACHING ENVIRONMENT}}},
  booktitle = {Proceedings of the 1996 {{IASE Round Table Conference}}},
  author = {Batanero, Carmen and Estepa, Antonio and Godino, Juan D},
  year = {1997}
}

@article{batesetal2015,
  title = {Fitting {{Linear Mixed-Effects Models Using}} {\textbf{Lme4}}},
  author = {Bates, Douglas and M{\"a}chler, Martin and Bolker, Ben and Walker, Steve},
  year = {2015},
  journal = {Journal of Statistical Software},
  volume = {67},
  number = {1},
  issn = {1548-7660},
  doi = {10.18637/jss.v067.i01},
  urldate = {2021-10-18},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/97G4SJRA/Bates et al. - 2015 - Fitting Linear Mixed-Effects Models Using lme4.pdf}
}

@book{baumer2016,
  title = {Lab: {{Randomization}} and the {{Bootstrap}}},
  author = {Baumer, Ben and McNamara, Amelia},
  year = {2016}
}

@book{baumer2016a,
  title = {Modern Data Science with {{R}}},
  author = {Baumer, Ben and Kaplan, Daniel T. and Horton, Nicholas J.},
  year = {2016},
  publisher = {Chapman \& Hall/CRC}
}

@article{baumeretal2014,
  title = {R {{Markdown}}: {{Integrating A Reproducible Analysis Tool}} into {{Introductory Statistics}}},
  author = {Baumer, Ben and {\c C}etinkaya-Rundel, Mine and Bray, Andrew and Loi, Linda and Horton, Nicholas J.},
  year = {2014},
  journal = {Technology Innovations in Statistics Education},
  volume = {8},
  number = {1},
  abstract = {Nolan and Temple Lang argue that ``the ability to express statistical computations is an essential skill.'' A key related capacity is the ability to conduct and present data analysis in a way that another person can understand and replicate. The copy-and-paste workflow that is an artifact of antiquated user-interface design makes reproducibility of statistical analysis more difficult, especially as data become increasingly complex and statistical methods become increasingly sophisticated. R Markdown is a new technology that makes creating fully-reproducible statistical analysis simple and painless. It provides a solution suitable not only for cutting edge research, but also for use in an introductory statistics course. We present evidence that R Markdown can be used effectively in introductory statistics courses, and discuss its role in the rapidly-changing world of statistical computation},
  keywords = {to read}
}

@article{bbcvisualanddatajournalism2019,
  title = {How the {{BBC Visual}} and {{Data Journalism}} Team Works with Graphics in {{R}}},
  author = {{BBC Visual and Data Journalism}},
  year = {2019},
  month = feb,
  journal = {Medium.com},
  urldate = {2022-03-30},
  abstract = {Over the past year, we`ve fundamentally changed how we produce graphics.},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/XS54JUNH/how-the-bbc-visual-and-data-journalism-team-works-with-graphics-in-r-ed0b35693535.html}
}

@article{beattyetal2019,
  title = {Analysis of {{Student Use}} of {{Video}} in a {{Flipped Classroom}}},
  author = {Beatty, Brian J. and Merchant, Zahira and Albert, Michael},
  year = {2019},
  month = jul,
  journal = {TechTrends},
  volume = {63},
  number = {4},
  pages = {376--385},
  issn = {8756-3894, 1559-7075},
  doi = {10.1007/s11528-017-0169-1},
  urldate = {2021-11-05},
  langid = {english}
}

@article{becker1987,
  title = {Dynamic Graphics for Data Analysis},
  author = {Becker, Richard A. and Cleveland, William S. and Wilks, Allan R.},
  year = {1987},
  journal = {Statistical Science},
  volume = {2},
  number = {4},
  pages = {355--383},
  keywords = {to read}
}

@techreport{becker1994,
  title = {A {{Brief History}} of {{S}}},
  author = {Becker, Richard A.},
  year = {1994},
  institution = {AT\&T Bell Laboratories}
}

@article{beecham2017,
  title = {Map {{LineUps}}: {{Effects}} of Spatial Structure on Graphical Inference},
  shorttitle = {Map {{LineUps}}},
  author = {Beecham, Roger and Dykes, Jason and Meulemans, Wouter and Slingsby, Aidan and Turkay, Cagatay and Wood, Jo},
  year = {2017},
  month = jan,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {23},
  number = {1},
  pages = {391--400},
  issn = {1077-2626},
  doi = {10.1109/TVCG.2016.2598862},
  urldate = {2021-06-10},
  file = {/Users/amcnamara/Zotero/storage/3M65NPIM/Beecham et al. - 2017 - Map LineUps Effects of spatial structure on graph.pdf}
}

@article{behrens1997,
  title = {Principles and Procedures of Exploratory Data Analysis},
  author = {Behrens, John T.},
  year = {1997},
  month = jun,
  journal = {Psychological Methods},
  volume = {2},
  number = {2},
  pages = {131--160},
  publisher = {American Psychological Association},
  address = {Washington, US},
  issn = {1082-989X},
  doi = {10.1037/1082-989X.2.2.131},
  urldate = {2023-07-07},
  abstract = {Exploratory data analysis (EDA) is a well-established statistical tradition that provides conceptual and computational tools for discovering patterns to foster hypothesis development and refinement. These tools and attitudes complement the use of significance and hypothesis tests used in confirmatory data analysis (CDA). Although EDA complements rather than replaces CDA, use of CDA without EDA is seldom warranted. Even when well-specified theories are held, EDA helps one interpret the results of CDA and may reveal unexpected or misleading patterns in the data. This article introduces the central heuristics and computational tools of EDA and contrasts it with CDA and exploratory statistics in general. EDA techniques are illustrated using previously published psychological data. Changes in statistical training and practice are recommended to incorporate these tools. (PsycINFO Database Record (c) 2016 APA, all rights reserved) (Source: journal abstract)},
  copyright = {{\copyright} 1997, American Psychological Association},
  langid = {english},
  keywords = {Hypothesis Testing,Statistical Analysis (major),Statistical Data (major),Statistical Significance,Theory Formulation (major)},
  annotation = {(US)}
}

@book{bell2010,
  title = {Computer {{Science Unplugged}}},
  author = {Bell, Tim and Witten, Ian H. and Fellows, Mike},
  year = {2010}
}

@article{bellinisaibeneetal2020,
  title = {R-{{Ladies Global}}, a Worldwide Organisation to Promote Gender Diversity in the {{R}} Community.},
  author = {Bellini Saibene, Yanina and Vitolo, Claudia and LeDell, Erin and Frick, Hannah and Acion, Laura},
  year = {2020},
  month = may,
  pages = {20530},
  doi = {10.5194/egusphere-egu2020-20530},
  urldate = {2023-01-11},
  abstract = {Having programming skills is becoming increasingly important for many geoscientists who wish to make their research as reproducible as it can possibly be. One of the most common languages of choice is the R language for statistical computing.The R community, as other programming communities, suffers from underrepresentation of women and minority genders (e.g., trans men, non-binary, etc) in every role and area of participation, whether as leaders, package developers, conference speakers, conference participants, educators, or users.As a diversity initiative, the mission of R-Ladies is to achieve proportionate representation by encouraging, inspiring, and empowering people of genders currently underrepresented in the R community. R-Ladies' primary focus, therefore, is on supporting underrepresented-gender R enthusiasts to achieve their programming potential, by building a collaborative global network of R leaders, mentors, learners, and developers to facilitate individual and collective progress worldwide.R-Ladies Global received funding for the first time in 2016, from the R Consortium (r-consortium.org, a Linux Foundation Project) and was quickly promoted to be a top-level project due to "its big commitment within the R community".The organization is articulated into 'chapters', groups hosting events in cities or remotely, the latter for the benefit of everyone, regardless of geographic location and personal circumstances. To date, R-Ladies fosters the development of 180 chapters organizing more than 2000 events in 50 countries around the world with more than 60,000 members and over 70,000 followers across the various Twitter accounts.In this presentation, we will illustrate all the activities R-Ladies runs to support minority genders: from meetups (in-person meetings, where individuals can learn about new technologies and algorithms free of charge) to the R-Ladies directory (https://rladies.org/directory/), the abstract reviewers' network (tinyurl.com/rladiesrevs), the Slack channels, the mentorship program, and much more.},
  annotation = {ADS Bibcode: 2020EGUGA..2220530B}
}

@article{ben-zvi2000,
  title = {Toward {{Understanding}} the {{Role}} of {{Technological Tools}} in {{Statistical Learning}}},
  author = {{Ben-Zvi}, Dani},
  year = {2000},
  journal = {Mathematical thinking and learning},
  volume = {2},
  number = {1\&2},
  pages = {127--155},
  abstract = {This article begins with some context setting on new views of statistics and statistical education. These views are reflected, in particular, in the introduction of exploratory data analysis (EDA) into the statistics curriculum. Then, a detailed example of EDA learning activity in the middle school is introduced, which makes use of the power of the spreadsheet to mediate students' construction of meanings for statistical conceptions. Through this example, I endeavor to illustrate how an attempt at serious integration of computers in teaching and learning statistics brings about a cascade of changes in curriculum materials, classroom praxis, and students' ways of learning. A theoretical discussion follows that underpins the impact of technological tools on teaching and learning statistics by emphasizing how the computer lends itself to supporting cognitive and sociocultural processes. Subsequently, I present a sample of educational technologies, which represents the sorts of software that have typically been used in statistics instruction: statistical packages (tools), microworlds, tutorials, resources (including Internet resources), and teachers' metatools. Finally, certain implications and recommendations for the use of computers in the statistical educational milieu are suggested.},
  keywords = {to read}
}

@article{ben-zvi2002,
  title = {Seventh Grade Students' Sense Making of Data and Data Representations},
  author = {{Ben-Zvi}, Dani},
  year = {2002},
  journal = {ICOTS6},
  keywords = {data visualization,education,statistics}
}

@incollection{ben-zvi2004,
  title = {The {{Challenge}} of {{Developing Statistical Literacy}}, {{Reasoning}} and {{Thinking}}},
  author = {{Ben-Zvi}, Dani},
  editor = {{Ben-Zvi}, Dani and Garfield, Joan},
  year = {2004},
  publisher = {Kluwer Academic Publishers},
  chapter = {Reasoning about data analysis}
}

@book{ben-zvi2004a,
  title = {The {{Challenge}} of {{Developing Statistical Literacy}}, {{Reasoning}} and {{Thinking}}},
  editor = {{Ben-Zvi}, Dani and Garfield, Joan},
  year = {2004},
  publisher = {Kluwer Academic Publishers},
  keywords = {education,statistics}
}

@article{bendergrouven1997,
  title = {Ordinal {{Logistic Regression}} in {{Medical Research}}},
  author = {Bender, Ralf and Grouven, Ulrich},
  year = {1997},
  journal = {Journal of the Royal College of Physicians of London},
  volume = {31},
  number = {5},
  pages = {546--551},
  issn = {0035-8819},
  urldate = {2021-10-01},
  abstract = {Medical research workers are making increasing use of logistic regression analysis for binary and ordinal data. The purpose of this paper is to give a non-technical introduction to logistic regression models for ordinal response variables. We address issues such as the global concept and interpetation of logistic models, the model building procedure from a practical point of view, and the assessment of the model adequacy. For illustrative purposes we apply these methods to real data of a study investigating the association between glycosylated haemoglobin and retinopathy. We give some recommendations for the use and assessment of ordinal logistic regression models in medical research.},
  pmcid = {PMC5420958},
  pmid = {9429194},
  file = {/Users/amcnamara/Zotero/storage/ZHQI36FW/Bender and Grouven - 1997 - Ordinal Logistic Regression in Medical Research.pdf}
}

@article{berghawila2021,
  title = {Some Teaching Resources Using {{R}} with Illustrative Examples Exploring {{{\textsc{COVID}}}} -19 Data},
  shorttitle = {Some Teaching Resources Using {{R}} with Illustrative Examples Exploring},
  author = {Berg, Arthur and Hawila, Nour},
  year = {2021},
  month = jul,
  journal = {Teaching Statistics},
  volume = {43},
  number = {S1},
  issn = {0141-982X, 1467-9639},
  doi = {10.1111/test.12258},
  urldate = {2021-07-27},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/WE9BQA5H/Berg and Hawila - 2021 - Some teaching resources using R with illustrative .pdf}
}

@inproceedings{bernstein2010,
  title = {Soylent: {{A}} Word Processor with a Crowd Inside},
  booktitle = {{{UIST}}'10},
  author = {Bernstein, Michael S. and Little, Greg and Miller, Robert C. and Hartmann, Bj{\"o}rn and Ackerman, Mark S. and Karger, David R. and Crowell, David and Panovich, Katrina},
  year = {2010}
}

@techreport{berret2016,
  title = {Teaching Data and Computational Journalism},
  author = {Berret, Charles and Phillips, Cheryl},
  year = {2016},
  institution = {{Columbia Journalism School and Stanford University}}
}

@article{berry2011,
  title = {The {{Computational}} Turn: Thinking about the Digital Humanities},
  author = {Berry, David M.},
  year = {2011},
  journal = {Culture Machine},
  volume = {12},
  keywords = {computer science,digital humanities}
}

@book{bertin1983,
  title = {Semiology of {{Graphics}}},
  author = {Bertin, Jacques},
  year = {1983},
  publisher = {University of Wisconsin Press},
  keywords = {to buy}
}

@inproceedings{bertinietal2020,
  title = {Why {{Shouldn}}'t {{All Charts Be Scatter Plots}}? {{Beyond Precision-Driven Visualizations}}},
  shorttitle = {Why {{Shouldn}}'t {{All Charts Be Scatter Plots}}?},
  booktitle = {2020 {{IEEE Visualization Conference}} ({{VIS}})},
  author = {Bertini, Enrico and Correll, Michael and Franconeri, Steven},
  year = {2020},
  month = oct,
  pages = {206--210},
  doi = {10.1109/VIS47514.2020.00048},
  abstract = {A central tenet of information visualization research and practice is the notion of visual variable effectiveness, or the perceptual precision at which values are decoded given visual channels of encoding. Formative work from Cleveland \& McGill has shown that position along a common axis is the most effective visual variable for comparing individual values. One natural conclusion is that any chart that is not a dot plot or scatterplot is deficient and should be avoided. In this paper we refute a caricature of this "scatterplots only" argument as a way to call for new perspectives on how information visualization is researched, taught, and evaluated.},
  keywords = {concepts and paradigms,Data visualization,Guidelines,Human-centered computing,Lead,Pediatrics,Sparks,Tools,Visualization,Visualization theory},
  file = {/Users/amcnamara/Zotero/storage/4LLVAW8D/Bertini et al. - 2020 - Why Shouldn’t All Charts Be Scatter Plots Beyond .pdf;/Users/amcnamara/Zotero/storage/9IWMIZTT/9331277.html}
}

@article{best1991,
  title = {Statistics {{Programs Designed}} for the {{Macintosh}}: {{Data Desk}}, {{Exstatix}}, {{Fastat}}, {{JMP}}, {{StatView II}}, and {{SuperANOVA}}},
  author = {Best, Al M. and Morganstein, David},
  year = {1991},
  journal = {The American Statistician},
  volume = {45},
  number = {4}
}

@techreport{bezanson2015,
  title = {Julia: {{A}} Fresh Approach to Numerical Computing},
  author = {Bezanson, Jess and Edelman, Alan and Karpinski, Stefan and Shah, Viral B.},
  year = {2015},
  institution = {{MIT and Julia Computing}}
}

@inproceedings{biehler1994,
  title = {Probabilistic Thinking, Statistical Reasoning and the Search for Causes- Do We Need a Probabilistic Revolution after We Have Taught Data Analysis?},
  booktitle = {Research Papers from {{ICOTS}} 4},
  author = {Biehler, Rolf},
  editor = {Garfield, Joan},
  year = {1994},
  publisher = {International Conference on Teaching Statistics},
  keywords = {education,statistics}
}

@article{biehler1997,
  title = {Software for {{Learning}} and for {{Doing Statistics}}},
  author = {Biehler, Rolf},
  year = {1997},
  journal = {International Statistical Review},
  volume = {65},
  number = {2},
  pages = {167--189},
  abstract = {The community of statisticians and statistics educators should take responsibility for the evaluation and improvement of software quality from the perspective of education. The paper will develop a perspective, an ideal system of requirements to critically evaluate existing software and to produce future software more adequate both for learning and doing statistics in introductory courses. Different kinds of tools and microworlds are needed. After discussing general requirements for such programs, a prototypical ideal software system will be presented in detail. It will be illustrated how such a system could be used to construct learning environments and to support elementary data analysis with exploratory working style.}
}

@inproceedings{biehler1997a,
  title = {Students'difficulties in Practicing Computer-Supported Data Analysis: Some Hypothetical Generalizations from Results of Two Exploratory Studies},
  booktitle = {Proceedings of the 1996 {{IASE Round Table Conference}}},
  author = {Biehler, Rolf},
  year = {1997}
}

@inproceedings{biehler2001,
  title = {Students' Difficulties in Practicing Computer-Supported Data Analysis: {{Some}} Hypothetical Generalizations from Results of Two Exploratory Studies},
  booktitle = {Background Readings for {{SRTL-2}}},
  author = {Biehler, Rolf},
  year = {2001},
  keywords = {to read}
}

@inproceedings{biehler2001a,
  title = {Students' Difficulties in Practicing Computer-Supported Data Analysis: Some Hypothetical Generalizations from Results of Two Exploratory Studies},
  booktitle = {Background Readings for {{SRTL-2}}},
  author = {Biehler, Rolf},
  year = {2001},
  pages = {169--190},
  keywords = {to read}
}

@inproceedings{biehler2003,
  title = {Interrelated Learning and Working Environments for Supporting the Use of Computer Tools in Introductory Classes},
  booktitle = {{{IASE}} Satellite Conference on Statistics Education and the Internet},
  author = {Biehler, Rolf},
  year = {2003},
  publisher = {International Statistical Institute},
  keywords = {computer science,education,statistics,technology}
}

@incollection{biehler2013,
  title = {Third {{International Handbook}} of {{Mathematics Education}}},
  author = {Biehler, Rolf and {Ben-Zvi}, Dani and Bakker, Arthur and Makar, Katie},
  editor = {Clemenets, M. A. and Bishop, Alan and Keitel, Christine and Kilpatrick, Jeremy and Leung, Frederick Koon-Shing},
  year = {2013},
  pages = {643--689},
  publisher = {Springer Science + Business Media},
  abstract = {Abstract The purpose of this chapter is to provide an updated overview of digital technologies relevant to statistics education, and to summarize what is currently known about how these new technologies can support the development of students' statistical reasoning at the school level. A brief literature review of trends in statistics education is followed by a section on the history of technologies in statistics and statistics education. Next, an overview of various types of technological tools highlights their benefits, purposes and limitations for developing students' statistical reasoning. We further discuss different learning environments that capitalize on these tools with examples from research and practice. Dynamic data analysis software applications for secondary students such as Fathom and TinkerPlots are discussed in detail. Examples are provided to illustrate innovative uses of technology. In the future, these uses may also be supported by a wider range of new tools still to be developed. To summarize some of the findings, the role of digital technologies in statistical reasoning is metaphorically compared with traveling between data and conclusions, where these tools represent fast modes of transport. Finally, we suggest future directions for technology in research and practice of developing students' statistical reasoning in technology-enhanced learning environments.},
  chapter = {Technology for Enhancing Statistical Reasoning at the School Level}
}

@incollection{biehleretal2015,
  title = {Preservice {{Teachers}}' {{Reasoning}} about {{Uncertainty}} in the {{Context}} of {{Randomization Tests}}},
  author = {Biehler, Rolf and Frischemeier, Daniel and Podworny, Susanne},
  year = {2015},
  month = aug,
  pages = {129--162},
  abstract = {We investigate the reasoning of preservice teachers about uncertainty in the context of randomization tests facilitated by TinkerPlots™. This method of hypothesis testing is a widely used method to look beyond the comparison of two groups and to generalize findings beyond a sample. To support preservice teachers while they conducted these tests, we developed two courses to lead them to randomization tests: One course moves from data analysis to randomization tests; the other from simulations to randomization tests. A video study which includes the observation of preservice teachers while conducting randomization tests with TinkerPlots™ placed at the end of both courses will be the focus of this chapter. Finally, this chapter will give insights into how the process of randomization tests can be supported for learners and will outline a group of German preservice teachers' encounters with randomization tests. Keywords: Informal inferential reasoning; Randomization tests; preservice teachers; TinkerPlots™},
  file = {/Users/amcnamara/Zotero/storage/DDZW3CS7/Biehler et al. - 2015 - Preservice tachers' reasoning about uncertainty in.pdf}
}

@article{biehlerfleischer2021,
  title = {Introducing Students to Machine Learning with Decision Trees Using {{CODAP}} and {{Jupyter Notebooks}}},
  author = {Biehler, Rolf and Fleischer, Yannik},
  year = {2021},
  month = jul,
  journal = {Teaching Statistics},
  volume = {43},
  number = {S1},
  issn = {0141-982X, 1467-9639},
  doi = {10.1111/test.12279},
  urldate = {2021-07-27},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/FBLZ8ACN/Biehler and Fleischer - 2021 - Introducing students to machine learning with deci.pdf}
}

@inproceedings{bigelowetal2014,
  title = {Reflections on How Designers Design with Data},
  booktitle = {Proceedings of the 2014 {{International Working Conference}} on {{Advanced Visual Interfaces}}},
  author = {Bigelow, Alex and Drucker, Steven and Fisher, Danyel and Meyer, Miriah},
  year = {2014},
  month = may,
  series = {{{AVI}} '14},
  pages = {17--24},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2598153.2598175},
  urldate = {2023-07-10},
  abstract = {In recent years many popular data visualizations have emerged that are created largely by designers whose main area of expertise is not computer science. Designers generate these visualizations using a handful of design tools and environments. To better inform the development of tools intended for designers working with data, we set out to understand designers' challenges and perspectives. We interviewed professional designers, conducted observations of designers working with data in the lab, and observed designers working with data in team settings in the wild. A set of patterns emerged from these observations from which we extract a number of themes that provide a new perspective on design considerations for visualization tool creators, as well as on known engineering problems.},
  isbn = {978-1-4503-2775-6},
  keywords = {design practice,infographics,visualization},
  file = {/Users/amcnamara/Zotero/storage/IKJYVZ3E/Bigelow et al. - 2014 - Reflections on how designers design with data.pdf}
}

@article{bionetal2018,
  title = {How {{R Helps Airbnb Make}} the {{Most}} of Its {{Data}}},
  author = {Bion, Ricardo and Chang, Robert and Goodman, Jason},
  year = {2018},
  month = jan,
  journal = {The American Statistician},
  volume = {72},
  number = {1},
  pages = {46--52},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2017.1392362},
  urldate = {2022-03-30},
  langid = {english}
}

@book{bivand2008,
  title = {Applied Spatial Data Analysis with {{R}}},
  author = {Bivand, Roger S and Pebesma, Edzer J and {Gomez-Rubio}, Virgilio and Pebesma, Edzer Jan},
  year = {2008},
  volume = {747248717},
  publisher = {Springer}
}

@book{bjerede,
  title = {Learning Is Personal: {{Stories}} of {{Android}} Tablet Use in the 5th Grade},
  author = {Bjerede, Marie and Bondi, Tzaddi},
  keywords = {computer science,education},
  annotation = {Published: learninguntethered.com}
}

@inproceedings{blockragan2020,
  title = {Micro-Entries: {{Encouraging Deeper Evaluation}} of {{Mental Models Over Time}} for {{Interactive Data Systems}}},
  shorttitle = {Micro-Entries},
  booktitle = {2020 {{IEEE Workshop}} on {{Evaluation}} and {{Beyond}} - {{Methodological Approaches}} to {{Visualization}} ({{BELIV}})},
  author = {Block, Jeremy E. and Ragan, Eric D.},
  year = {2020},
  month = oct,
  pages = {38--47},
  doi = {10.1109/BELIV51497.2020.00012},
  abstract = {Many interactive data systems combine visual representations of data with embedded algorithmic support for automation and data exploration. To effectively support transparent and explainable data systems, it is important for researchers and designers to know how users understand the system. We discuss the evaluation of users' mental models of system logic. Mental models are challenging to capture and analyze. While common evaluation methods aim to approximate the user's final mental model after a period of system usage, user understanding continuously evolves as users interact with a system over time. In this paper, we review many common mental model measurement techniques, discuss tradeoffs, and recommend methods for deeper, more meaningful evaluation of mental models when using interactive data analysis and visualization systems. We present guidelines for evaluating mental models over time to help track the evolution of specific model updates and how they may map to the particular use of interface features and data queries. By asking users to describe what they know and how they know it, researchers can collect structured, time-ordered insight into a user's conceptualization process while also helping guide users to their own discoveries.},
  keywords = {Cognition,Cognitive science,Data models,Data systems,Data visualization,Human-centered computing,Machine learning algorithms,Task analysis,Visualization,Visualization design and evaluation methods},
  file = {/Users/amcnamara/Zotero/storage/8FA68SVI/Block and Ragan - 2020 - Micro-entries Encouraging Deeper Evaluation of Me.pdf}
}

@incollection{board2011,
  title = {Computer {{Science}}: {{Principles}}},
  author = {Board, The College},
  year = {2011},
  publisher = {The College Board},
  chapter = {Big Ideas and Key Concepts, Learning Objectives and Evidence Statements},
  keywords = {computer science,education}
}

@incollection{board2011a,
  title = {Computer {{Science}}: {{Principles}}},
  author = {Board, The College},
  year = {2011},
  publisher = {The College Board},
  chapter = {Computational Thinking Practices, Big Ideas, Key Concepts and Supporting Concepts},
  keywords = {computer science,education}
}

@book{board2011b,
  title = {The 7th {{Annual AP Report}} to the {{Nation}}},
  author = {Board, The College},
  year = {2011},
  month = feb,
  keywords = {education}
}

@book{board2011c,
  title = {The 7th {{Annual AP Report}} to the {{Nation}}: {{Subject Supplement}}},
  author = {Board, The College},
  year = {2011},
  month = feb,
  keywords = {education}
}

@book{board2015,
  title = {Mobilize 2014-2015 {{Mathematics Curriculum}}},
  author = {Board, Odette and Gould, Robert and Amaya, Pamela and Estevez, Heidi},
  year = {2015},
  publisher = {{Office of Curriculum, Instruction, and School Support}}
}

@misc{bodwinetal2022,
  title = {Materials for "{{Looks}} Okay to Me": {{A}} Study of Best Practice in Data Analysis Code Review.},
  shorttitle = {Materials for "{{Looks}} Okay to Me"},
  author = {Bodwin, Kelly and Flores Siaca, Ian and Robinson, Derek},
  year = {2022},
  month = apr,
  doi = {10.5281/ZENODO.6413343},
  urldate = {2022-07-20},
  abstract = {R Markdown and Jupyter notebooks for data analysis study. Protocols for subject interviews for parallel studies.},
  copyright = {Creative Commons Attribution 4.0 International, Open Access},
  howpublished = {Zenodo},
  keywords = {Data Science Education,jupyter,R Markdown,Reproducible Research}
}

@article{boelsetal2019,
  title = {Conceptual Difficulties When Interpreting Histograms: {{A}} Review},
  shorttitle = {Conceptual Difficulties When Interpreting Histograms},
  author = {Boels, Lonneke and Bakker, Arthur and Van Dooren, Wim and Drijvers, Paul},
  year = {2019},
  month = nov,
  journal = {Educational Research Review},
  volume = {28},
  pages = {100291},
  issn = {1747-938X},
  doi = {10.1016/j.edurev.2019.100291},
  urldate = {2023-06-14},
  abstract = {Histograms are widely used and appear easy to understand. Research nevertheless indicates that students, teachers and researchers often misinterpret these graphical representations. Hence, the research question addressed in this paper is: What are the conceptual difficulties that become manifest in the common misinterpretations people have when constructing or interpreting histograms? To identify these conceptual difficulties, we conducted a narrative systematic literature review and identified 86 publications reporting or containing misinterpretations. The misinterpretations were clustered and---through abduction---connected to difficulties with statistical concepts. The analysis revealed that most of these conceptual difficulties relate to two big ideas in statistics: data (e.g., number of variables and measurement level) and distribution (shape, centre and variability or spread). These big ideas are depicted differently in histograms compared to, for example, case-value plots. Our overview can help teachers and researchers to address common misinterpretations more generally instead of remediating them each individually.},
  langid = {english},
  keywords = {Big ideas of statistics,Histogram,Misconception,Statistical concepts,Statistical knowledge for teaching (SKT),Statistics education},
  file = {/Users/amcnamara/Zotero/storage/E8CZ7UBM/Boels et al. - 2019 - Conceptual difficulties when interpreting histogra.pdf}
}

@article{bolandnicholson1996,
  title = {The Statistics and Probability Curriculum at the Secondary School Level in the {{USA}}, {{Ireland}} and the {{UK}}},
  author = {Boland, Philip J and Nicholson, James},
  year = {1996},
  month = jun,
  journal = {The Statistician},
  volume = {45},
  number = {4},
  pages = {437--446},
  abstract = {Statistics and probability are included to varying degrees in the mathematics curriculum of most countries at the secondary school level. The rapidly changing nature of these subjects together with the explosion in computing power and software has, however, led to some changes in the emphasis given in these subjects. In particular in some countries there is an effort to emphasize more the aspects of data analysis and simulation. In this paper we briefly discuss some of the changes in curriculum that have recently taken place in the USA, Ireland and the UK (excluding Scotland) and the progress that is being made in implementing them.},
  keywords = {Curriculum standards for school mathematica,data handling,education,quantitative literacy project,secondary school probability and statistics,statistics}
}

@article{bondetal2012,
  title = {Students' Perceptions of Statistics: {{An}} Exploration of Attitudes, Conceptualizations, and Content Knowledge of Statistics},
  author = {Bond, Marjorie E. and Perkins, Susan N. and Ramirez, Caroline},
  year = {2012},
  journal = {Statistics Education Research Journal},
  volume = {11},
  number = {2},
  abstract = {Although statistics education research has focused on students' learning and conceptual understanding of statistics, researchers have only recently begun investigating students' perceptions of statistics. The term perception describes the overlap between cognitive and non- cognitive factors. In this mixed-methods study, undergraduate students provided their perceptions of statistics and completed the Survey of Students' Attitudes Toward Statistics-36 (SATS-36). The qualitative data suggest students had basic knowledge of what the word statistics meant, but with varying depths of understanding and conceptualization of statistics. Quantitative analysis also examined the relationship between students' perceptions of statistics and attitudes toward statistics. We found no significant difference in mean pre- or post-SATS scores across conceptualization and content knowledge categories. The implications of these findings for education and research are discussed.}
}

@article{bonney2009,
  title = {Citizen {{Science}}: {{A}} Developing Tool for Expanding Science Knowledge and Scientific Literacy},
  author = {Bonney, Rick and Cooper, Caren B. and Dickinson, Janis and Kelling, Steve and Phillips, Tina and Rosenberg, Kenneth V. and Shirk, Jennifer},
  year = {2009},
  journal = {BioScience},
  volume = {59},
  number = {11},
  pages = {977--984},
  abstract = {Citizen science enlists the public in collecting large quantities of data across an array of habitats and locations over long spans of time. Citizen science projects have been remarkably successful in advancing scientific knowledge, and contributions from citizen scientists now provide a vast quantity of data about species occurrence and distribution around the world. Most citizen science projects also strive to help participants learn about the organisms they are observing and to experience the process by which scientific investigations are conducted. Developing and implementing public data-collection projects that yield both scientific and educational outcomes requires significant effort. This article describes the model for building and operating citizen science projects that has evolved at the Cornell Lab of Ornithology over the past two decades. We hope that our model will inform the fields of biodiversity monitoring, biological research, and science education while providing a window into the culture of citizen science.},
  keywords = {citizen science,participatory sensing,statistics}
}

@book{bornat,
  title = {Camels and Humps: {{A}} Retraction},
  author = {Bornat, Richard}
}

@article{bostock2011,
  title = {D3: {{Data-driven}} Documents},
  author = {Bostock, Michael and Ogievetsky, Vadim and Heer, Jeffrey},
  year = {2011},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {17},
  number = {12}
}

@article{bostock2012,
  title = {One {{Report}}, {{Diverging Perspectives}}},
  author = {Bostock, Michael and Carter, Shan and Cox, Amanda and Quealy, Kevin},
  year = {2012},
  month = oct,
  journal = {The New York Times}
}

@book{bostock2012a,
  title = {Scatterplot {{Matrix Brushing}}},
  author = {Bostock, Mike},
  year = {2012},
  month = nov
}

@book{bostock2013,
  title = {D3.Js: {{Data-driven}} Documents},
  author = {Bostock, Mike},
  year = {2013}
}

@article{bostock2014,
  title = {Is It Better to Rent or Buy?},
  author = {Bostock, Mike and Carter, Shan and Tse, Archie},
  year = {2014},
  journal = {The New York Times}
}

@book{bostock2015,
  title = {Mbostock's Blocks},
  author = {Bostock, Mike},
  year = {2015}
}

@book{bostock2017,
  title = {D3.Express},
  author = {Bostock, Mike},
  year = {2017},
  month = apr
}

@article{boy2014,
  title = {A Principled Way of Assessing Visualization Literacy},
  author = {Boy, Jeremy and Rensink, Ronald A. and Bertini, Enrico and Fekete, Jean-Daniel},
  year = {2014},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {20},
  number = {12},
  abstract = {We describe a method for assessing the visualization literacy (VL) of a user. Assessing how well people understand visualizations has great value for research (e.g., to avoid confounds), for design (e.g., to best determine the capabilities of an audience), for teaching (e. g., to assess the level of new students), and for recruiting (e. g., to assess the level of interviewees). This paper proposes a method for assessing VL based on Item Response Theory. It describes the design and evaluation of two VL tests for line graphs, and presents the extension of the method to bar charts and scatterplots. Finally, it discusses the reimplementation of these tests for fast, effective, and scalable web-based use.},
  keywords = {experimental,IEEEviz,theoretical}
}

@article{boyd2011,
  title = {Six {{Provocations}} for {{Big Data}}},
  author = {{boyd}, danah and Crawford, Kate},
  year = {2011},
  journal = {Social Science Research Network},
  keywords = {big data,digital humanities,statistics}
}

@article{boyd2012,
  title = {Critical {{Questions}} for {{Big Data}}},
  author = {{boyd}, danah and Crawford, Kate},
  year = {2012},
  journal = {Information, Communication \& Society},
  volume = {15},
  number = {5},
  pages = {662--679},
  abstract = {The era of Big Data has begun. Computer scientists, physicists, economists, mathematicians, political scientists, bio-informaticists, sociologists, and other scholars are clamoring for access to the massive quantities of information produced by and about people, things, and their interactions. Diverse groups argue about the potential benefits and costs of analyzing genetic sequences, social media interactions, health records, phone logs, government records, and other digital traces left by people. Significant questions emerge. Will large-scale search data help us create better tools, services, and public goods? Or will it usher in a new wave of privacy incursions and invasive marketing? Will data analytics help us understand online communities and political movements? Or will it be used to track protesters and suppress speech? Will it transform how we study human communication and culture, or narrow the palette of research options and alter what `research' means? Given the rise of Big Data as a socio-technical phenomenon, we argue that it is necessary to critically interrogate its assumptions and biases. In this article, we offer six provocations to spark conversations about the issues of Big Data: a cultural, techno- logical, and scholarly phenomenon that rests on the interplay of technology, analysis, and mythology that provokes extensive utopian and dystopian rhetoric.},
  keywords = {to read}
}

@article{brabham2009,
  title = {Crowdsourcing the Public Participation Process for Planning Projects},
  author = {Brabham, Daren C.},
  year = {2009},
  journal = {Planning Theory},
  volume = {8},
  pages = {242--262},
  abstract = {Public involvement is a central concern for urban planners, but the challenge for planners is how best to implement such programs, given many difficulties inherent in the typical public involvement process. The medium of the Web enables us to harness collective intellect among a population in ways face-to-face planning meetings cannot. This article argues that the crowdsourcing model, a successful, Web-based, distributed problem solving and production model for business, is an appropriate model for enabling the citizen participation process in public planning projects. This article begins with an exploration of the challenges of public participation in urban planning projects, particularly in the harnessing of creative solutions. An explanation of the theories of collective intelligence and crowd wisdom follows, arguing for the medium of the Web as an appropriate technology for harnessing far-flung genius. An exploration of crowdsourcing in a hypothetical neighborhood planning example, along with a consideration of the challenges of implementing crowdsourcing, concludes the article.},
  keywords = {crowdsourcing,planning}
}

@book{bransford2000,
  title = {How {{People Learn}}: {{Brain}}, {{Mind}}, {{Experience}}, and {{School}}},
  editor = {Bransford, John D. and Brown, Ann L. and Cocking, Rodney R.},
  year = {2000},
  publisher = {National Academy Press}
}

@manual{brayetal2021,
  type = {manual},
  title = {\texttt{infer}: Tidy Statistical Inference},
  author = {Bray, Andrew and Ismay, Chester and Chasnovski, Evgeni and Baumer, Ben and Cetinkaya-Rundel, Mine},
  year = {2021},
  annotation = {\{R package version 1.0.0\}}
}

@article{breiman2001,
  title = {Statistical {{Modeling}}: {{The}} Two Cultures},
  author = {Breiman, Leo},
  year = {2001},
  journal = {Statistical Science},
  volume = {16},
  number = {3},
  pages = {199--231},
  abstract = {Abstract. There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown.The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.},
  keywords = {to read}
}

@unpublished{broman2015,
  title = {Initial Steps toward Reproducible Research},
  author = {Broman, Karl},
  year = {2015}
}

@unpublished{broman2016,
  title = {Steps toward Reproducible Research},
  author = {Broman, Karl},
  year = {2016}
}

@article{broman2017,
  title = {Data Organization in Spreadsheets},
  author = {Broman, Karl and Woo, Kara},
  year = {2017},
  journal = {PeerJ Preprints}
}

@article{brown1978,
  title = {Diagnostic Models for Procedural Bugs in Basic Mathematical Skills},
  author = {Brown, John Seely and Burton, Richard R.},
  year = {1978},
  journal = {Cognitive Science},
  volume = {2},
  pages = {155--192},
  abstract = {A new diagnostic modeling system for automatically synthesizing a deep-structure model of a student's misconceptions or bugs in his basic mathematical skills provides a mechanism for explaining why a student is making a mistake as opposed to simply identifying the mistake. This report is divided into four sections: The first provides examples of the problems that must be handled by a diagnostic model. It then introduces procedural works as a general framework for representing the knowledge underlying a skill. The challenge in designing this presentation is to find one that facilitates the discovery of misconceptions or bugs existing in a particular student's encoding of this knowledge. The second section discusses some of the pedagogical issues that have emerged from the use of diagnostic models within an instructional system. This discussion is framed in the context of a computer-based tutoring/gaming system developed to teach students and student teachers how to diagnose bugs strategically as well as how to provide a better understanding of the underlying structure of arithmetic skills. The third section describes our uses of an executable network as a tool for automatically diagnosing student behavior, for automatically generating "diagnostic" tests, .and for judging the diagnostic quality of a given exam. Included in this section is a discussion of the success of this system in diagnosing 1300 school students from a data base of 20.000 test items. The last =!ion discusses future research directions.},
  keywords = {to read}
}

@article{brown1986,
  title = {An Experimental Study of People Creating Spreadsheets},
  author = {Brown, Polly S. and Gould, John D.},
  year = {1986},
  journal = {ACM Transactions on Office Information Systems},
  volume = {5},
  number = {3},
  pages = {258--272}
}

@incollection{brown2020,
  title = {Chapter 1: {{Introduction}} to {{Digital Literacy}}},
  shorttitle = {Chapter 1},
  booktitle = {Digital {{Citizenship Toolkit}}},
  author = {Brown, Cheryl},
  year = {2020},
  publisher = {Ryerson University},
  urldate = {2024-01-14},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/XREEUBN6/chapter-1.html}
}

@book{bruton1998,
  title = {Science {{Content Standards}} for {{California Public Schools}}: {{Kindergarten Through Grade Twelve}}},
  editor = {Bruton, Sheila and Ong, Faye},
  year = {1998},
  publisher = {California State Board of Education},
  keywords = {education,science}
}

@book{bryan,
  title = {Practical {{Data Science}} for {{Stats}}},
  editor = {Bryan, Jennifer and Wickham, Hadley}
}

@book{bryan2016,
  title = {@{{JennyBryan}} ``{{I}}'m Seeking {{TRUE}}, Crazy Spreadsheet Stories. {{Happy}} to Get the Actual Sheet or Just a Description of the Crazy. {{Also}}: {{I}} Can Keep a Secret.''},
  author = {Bryan, Jennifer},
  year = {2016}
}

@book{bryan2016a,
  title = {Spreadsheets},
  author = {Bryan, Jennifer},
  year = {2016},
  annotation = {Published: useR! Conference, https://github.com/jennybc/2016-06\_spreadsheets}
}

@book{bryanetal2021,
  title = {Happy {{Git}} and {{GitHub}} for the {{useR}}},
  author = {Bryan, Jenny and The STAT 545 TAs and Hester, Jim},
  year = {2021},
  urldate = {2021-09-27},
  abstract = {Using Git and GitHub with R, Rstudio, and R Markdown},
  file = {/Users/amcnamara/Zotero/storage/IANCDL47/happygitwithr.com.html}
}

@inproceedings{bryant2000,
  title = {{{ODS}}, {{YES}}! {{Odious}}, {{NO}}! {{An}} Introduction to the {{SAS Output Delivery System}}},
  booktitle = {Proceedings of the {{Twenty-Fifth Annual SAS Users Group International Conference}}},
  author = {Bryant, Lara and Muller, Sally and Pass, Ray},
  year = {2000}
}

@incollection{buckheit1995,
  title = {Wavelets and {{Statistics}}},
  author = {Buckheit, Jonathan B. and Donoho, David L.},
  editor = {Antoniadis, A. and Oppenheim, G.},
  year = {1995},
  volume = {103},
  pages = {55--81},
  publisher = {Springer},
  abstract = {WAVELAB is a library of MATLAB routines for wavelet analysis, wavelet-packet analysis, cosine-packet analysis and matching pursuit. The library is available free of charge over the Internet. Versions are provided for Macintosh, UNIX and Windows machines. WAVELAB makes available, in onc package, all the code to reproduce all the figures in our published wavelet articles. The interested reader can inspect the source code to see exactly what algorithms were used, how parameters were set in producing our figures, and can then modify the source to produce variations on our results. WAVELAB has been developed, in part, because of exhortations by Jon Claerbout of Stanford that computational scientists should engage in "really reproducible" research.},
  chapter = {WaveLab and Reproducible Research}
}

@article{buhrmester2011,
  title = {Amazon's {{Mechanical Turk}}: A New Source of Inexpensive, yet High-Quality Data?},
  author = {Buhrmester, Michael and Kwang, Tracy and Gosling, Samuel D.},
  year = {2011},
  journal = {Perspectives on psychological science},
  volume = {6},
  number = {3},
  pages = {3--5},
  abstract = {Amazon's Mechanical Turk (MTurk) is a relatively new website that contains the major elements required to conduct research: an integrated participant compensation system; a large participant pool; and a streamlined process of study design, participant recruitment, and data collection. In this article, we describe and evaluate the potential contributions of MTurk to psychology and other social sciences. Findings indicate that (a) MTurk participants are slightly more demographically diverse than are standard Internet samples and are significantly more diverse than typical American college samples; (b) participation is affected by compensation rate and task length, but participants can still be recruited rapidly and inexpensively; (c) realistic compensation rates do not affect data quality; and (d) the data obtained are at least as reliable as those obtained via traditional methods. Overall, MTurk can be used to obtain high-quality data inexpensively and rapidly.},
  keywords = {data,mTurk}
}

@inproceedings{buja1986,
  title = {Grand {{Tour Methods}}: {{An Outline}}},
  booktitle = {Proceedings of the {{Seventeenth Symposium}} on {{The Interface}}},
  author = {Buja, Andreas and Asimov, Daniel},
  editor = {Allen, D. M.},
  year = {1986},
  pages = {63--67},
  publisher = {Elsevier Science Publishers B. V. (North-Holland)},
  keywords = {computer science,data visualization,statistics}
}

@article{buja2009,
  title = {Statistical Inference for Exploratory Data Analysis and Model Diagnostics},
  author = {Buja, Andreas and Cook, Dianne and Hofmann, Heike and Lawrence, Michael and Lee, Eun-Kyung and Swayne, Deborah F. and Wickham, Hadley},
  year = {2009},
  journal = {Philosophical Transactions of the Royal Society A},
  volume = {367},
  pages = {4361--4383},
  abstract = {We propose to furnish visual statistical methods with an inferential framework and protocol, modeled on confirmatory statistical testing. In this framework, plots take on the role of test statistics, and human cognition the role of statistical tests. Statistical significance of `discoveries' is measured by having the human viewer compare the plot of the real dataset with collections of plots of simulated datasets. A simple but rigorous protocol that provides inferential validity is modeled after the `lineup' popular from criminal legal procedures. Another protocol modeled after the `Rorschach' inkblot test, well known from (pop-)psychology, will help analysts acclimatize to random variability before being exposed to the plot of the real data. The proposed protocols will be useful for exploratory data analysis, with reference datasets simulated by using a null assumption that structure is absent. The framework is also useful for model diagnostics in which case reference datasets are simulated from the model in question. This latter point follows up on previous proposals. Adopting the protocols will mean an adjustment in working procedures for data analysts, adding more rigor, and teachers might find that incorporating these protocols into the curriculum improves their students' statistical thinking.}
}

@article{burke2006,
  title = {Participatory Sensing},
  author = {Burke, J. and Estrin, D. and Hansen, M. and Parker, A. and Ramanathan, N. and Reddy, S. and Srivastava, M. B.},
  year = {2006},
  journal = {WSW'06 at SenSys '06}
}

@inproceedings{burnsetal2023,
  title = {Who {{Do We Mean When We Talk About Visualization Novices}}?},
  booktitle = {Proceedings of the 2023 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Burns, Alyxander and Lee, Christiana and Chawla, Ria and Peck, Evan and Mahyar, Narges},
  year = {2023},
  month = apr,
  series = {{{CHI}} '23},
  pages = {1--16},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3544548.3581524},
  urldate = {2023-07-10},
  abstract = {As more people rely on visualization to inform their personal and collective decisions, researchers have focused on a broader range of audiences, including ``novices.'' But successfully applying, interrogating, or advancing visualization research for novices demands a clear understanding of what ``novice'' means in theory and practice. Misinterpreting who a ``novice'' is could lead to misapplying guidelines and overgeneralizing results. In this paper, we investigated how visualization researchers define novices and how they evaluate visualizations intended for novices. We analyzed 79 visualization papers that used ``novice,'' ``non-expert,'' ``laypeople,'' or ``general public'' in their titles or abstracts. We found ambiguity within papers and disagreement between papers regarding what defines a novice. Furthermore, we found a mismatch between the broad language describing novices and the narrow population representing them in evaluations (i.e., young people, students, and US residents). We suggest directions for inclusively supporting novices in both theory and practice.},
  isbn = {978-1-4503-9421-5},
  keywords = {audience,critical analyses,data visualization,research methodology},
  file = {/Users/amcnamara/Zotero/storage/H3YV8E6I/Burns et al. - 2023 - Who Do We Mean When We Talk About Visualization No.pdf}
}

@book{burrill1994,
  title = {Teaching Statistics: {{Guidelines}} for Elementary through High School},
  editor = {Burrill, Gail},
  year = {1994},
  publisher = {Dale Seymour Publications},
  keywords = {education,statistics}
}

@book{buxton2007,
  title = {Sketching User Experiences: {{Getting}} the Design Right and the Right Design},
  author = {Buxton, Bill},
  year = {2007},
  publisher = {Morgan Kaufmann Publishers},
  keywords = {to buy}
}

@book{caffo2015,
  title = {Johns {{Hopkins University Data Science Coursera Sequence}}},
  author = {Caffo, Brian and Leek, Jeff and Peng, Roger D.},
  year = {2015}
}

@book{cairo2013,
  title = {The {{Functional Art}}: {{An}} Introduction to Information Graphics and Visualization},
  author = {Cairo, Alberto},
  year = {2013},
  publisher = {New Riders}
}

@book{calmez2013,
  title = {Explorative Authoring of Active Web Content in a Mobile Environment},
  author = {Calmez, Conrad and Hesse, Hubert and Siegmund, Benjamin and Stamm, Sebastian and Thomschke, Astris and Hirshfeld, Robert and Ingalls, Dan and Lincke, Jens},
  year = {2013},
  volume = {72},
  publisher = {Universit{\"a}tsverlag Potsdam}
}

@book{cannon2012,
  title = {{{STAT2}}: {{Modeling}} with {{Regression}} and {{ANOVA}} (2nd Edition)},
  author = {Cannon, Ann R. and Cobb, George and Hartlaub, Bradley A. and Segler, Julie M. and Lock, Robin H. and Moore, Thomas L. and Rossman, Allan and Witmer, Jeffrey A.},
  year = {2012},
  publisher = {W.H. Freeman}
}

@book{carchedi2015,
  title = {Swirlstats},
  author = {Carchedi, Nick and Bauer, Bill and Grdina, Gina and Kross, Sean and Schouwenaars, Filip and L{\'e}onard, Alexandre},
  year = {2015}
}

@article{carifioperla2008,
  title = {Resolving the 50-Year Debate around Using and Misusing {{Likert}} Scales},
  author = {Carifio, James and Perla, Rocco},
  year = {2008},
  month = dec,
  journal = {Medical Education},
  volume = {42},
  number = {12},
  pages = {1150--1152},
  issn = {03080110, 13652923},
  doi = {10.1111/j.1365-2923.2008.03172.x},
  urldate = {2021-10-01},
  langid = {english}
}

@article{carter2010,
  title = {Budget {{Puzzle}}: {{You Fix}} the {{Budget}}},
  author = {Carter, Shan and Ericson, Matthew and Leonhard, David and Marsh, Bill and Quealy, Kevin},
  year = {2010},
  month = nov,
  journal = {The New York Times}
}

@inproceedings{caruana2006,
  title = {Mining {{Citizen Science Data}} to {{Predict Prevalence}} of {{Wild Bird Species}}},
  booktitle = {Proceedings of the 12th {{ACM SIGKDD International Conference}} on {{Knowledge Discovery}} and {{Data Mining}}},
  author = {Caruana, Rich and Elhaway, Mohamed and Munson, Art and Riedewald, Mirek and Sorokina, Daria and Fink, Daniel and Hochachka, Wesley M. and Kelling, Steve},
  year = {2006},
  pages = {909--915},
  abstract = {The Cornell Laboratory of Ornithology's mission is to interpret and conserve the earth's biological diversity through research, education, and citizen science focused on birds. Over the years, the Lab has accumulated one of the largest and longest-running collections of environmental data sets in existence. The data sets are not only large, but also have many attributes, contain many missing values, and potentially are very noisy. The ecologists are interested in identifying which features have the strongest effect on the distribution and abundance of bird species as well as describing the forms of these relationships. We show how data mining can be successfully applied, enabling the ecologists to discover unanticipated relationships. We compare a variety of methods for measuring attribute importance with respect to the probability of a bird being observed at a feeder and present initial results for the impact of important attributes on bird prevalence.},
  keywords = {citizen science,participatory sensing,statistics}
}

@book{carveretal2016,
  title = {Guidelines for Assessment and Instruction in Statistics Education College Report 2016},
  author = {GAISE College Report ASA Revision Committee and Everson, Michelle and Gabrosek, John and Horton, Nicholas J. and Lock, Robin H. and Mocko, Megan and Rossman, Allan and Rowell, Ginger Holmes and Velleman, Paul and Witmer, Jeffrey A. and Wood, Beverly},
  year = {2016},
  publisher = {American Statistical Association}
}

@book{carverklahr2013,
  title = {Cognition and {{Instruction}}: {{Twenty-five Years}} of {{Progress}}},
  shorttitle = {Cognition and {{Instruction}}},
  author = {Carver, Sharon M. and Klahr, David},
  year = {2013},
  month = jun,
  publisher = {Psychology Press},
  abstract = {This volume is based on papers presented at the 30th Carnegie Mellon Symposium on Cognition. This particular symposium was conceived in reference to the 1974 symposium entitled Cognition and Instruction. In the 25 years since that symposium, reciprocal relationships have been forged between psychology and education, research and practice, and laboratory and classroom learning contexts. Synergistic advances in theories, empirical findings, and instructional practice have been facilitated by the establishment of new interdisciplinary journals, teacher education courses, funding initiatives, and research institutes. So, with all of this activity, where is the field of cognition and instruction? How much progress has been made in 25 years? What remains to be done? This volume proposes and illustrates some exciting and challenging answers to these questions.   Chapters in this volume describe advances and challenges in four areas, including development and instruction, teachers and instructional strategies, tools for learning from instruction, and social contexts of instruction and learning. Detailed analyses of tasks, subjects' knowledge and processes, and the changes in performance over time have led to new understanding of learners' representations, their use of multiple strategies, and the important role of metacognitive processes. New methods for assessing and tracking the development and elaboration of knowledge structures and processing strategies have yielded new conceptualizations of the process of change. Detailed cognitive analysis of expert teachers, as well as a direct focus on enhancing teachers' cognitive models of learners and use of effective instructional strategies, are other areas that have seen tremendous growth and refinement in the past 25 years. Similarly, the strong impact of curriculum materials and activities based on a thorough cognitive analysis of the task has been extended to the use of technological tools for learning, such as intelligent tutors and complex computer based instructional interfaces. Both the shift to conducting a significant portion of the cognition and instruction research in real classrooms and the increased collaboration between academics and educators have brought the role of the social context to center stage.},
  isbn = {978-1-135-64899-2},
  langid = {english},
  keywords = {Psychology / Cognitive Psychology \& Cognition,Psychology / General}
}

@book{case2016,
  title = {Explorable {{Explanations}}},
  author = {Case, Nicky},
  year = {2016},
  month = may
}

@article{cass2014,
  title = {Interactive: {{The Top Programming Languages}}: {{IEEE Spectrum}}'s 2014 {{Rating}}},
  author = {Cass, Stephen and Diakopoulos, Nick and Romero, Joshua J.},
  year = {2014},
  month = jul,
  journal = {IEEE Spectrum},
  abstract = {This app ranks the popularity of dozens of programming languages. You can filter them by listing only those most relevant to particular sectors, such as web or embedded programming. Rankings are created by weighting and combining 12 metrics from 10 sources. We offer preset weightings for those interested in what's trending or most looked for by employers, or you can take complete control and create your own custom ranking by adjusting each metric's weighting yourself. (Read about our method and sources)}
}

@unpublished{cetinkaya-rundel,
  title = {The {{TIER Documentation Protocol}} v2.0 for {{R}}},
  author = {{\c C}etinkaya-Rundel, Mine}
}

@book{cetinkaya-rundel2014,
  title = {{{ShinyEd}}},
  author = {{\c C}etinkaya-Rundel, Mine},
  year = {2014}
}

@article{cetinkaya-rundel2018,
  title = {Infrastructure and {{Tools}} for {{Teaching Computing Throughout}} the {{Statistical Curriculum}}},
  author = {{\c C}etinkaya-Rundel, Mine and Rundel, Colin},
  year = {2018},
  month = jan,
  journal = {The American Statistician},
  volume = {72},
  number = {1},
  pages = {58--65},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2017.1397549},
  urldate = {2021-06-24},
  langid = {english}
}

@article{cetinkaya-rundel2021,
  title = {A {{Fresh Look}} at {{Introductory Data Science}}},
  author = {{\c C}etinkaya-Rundel, Mine and Ellison, Victoria},
  year = {2021},
  month = jan,
  journal = {Journal of Statistics and Data Science Education},
  volume = {29},
  number = {sup1},
  pages = {S16-S26},
  issn = {2693-9169},
  doi = {10.1080/10691898.2020.1804497},
  urldate = {2021-06-24},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/N3CS6AP5/Çetinkaya-Rundel and Ellison - 2021 - A Fresh Look at Introductory Data Science.pdf}
}

@misc{cetinkaya-rundel2021a,
  title = {Data {{Science}} in a {{Box}}},
  author = {{\c C}etinkaya-Rundel, Mine},
  year = {2021},
  urldate = {2021-07-20},
  howpublished = {https://datasciencebox.org/},
  file = {/Users/amcnamara/Zotero/storage/SH3V8RQJ/datasciencebox.org.html}
}

@article{cetinkaya-rundeletal2022,
  title = {An Educator's Perspective of the \texttt{tidyverse}},
  author = {Cetinkaya-Rundel, Mine and Hardin, Johanna and Baumer, Benjamin S. and McNamara, Amelia and Horton, Nicholas J. and Rundel, Colin W.},
  year = {2022},
  journal = {Technology Innovations in Statistics Education},
  volume = {14},
  number = {1},
  doi = {http://dx.doi.org/10.5070/T514154352},
  abstract = {Computing makes up a large and growing component of data science and statistics courses. Many of those courses, especially when taught by faculty who are statisticians by training, teach R as the programming language. A number of instructors have opted to build much of their teaching around the use of the tidyverse. The tidyverse, in the words of its developers, "is a collection of R packages that share a high-level design philosophy and low-level grammar and data structures, so that learning one package makes it easier to learn the next" (Wickham et al. 2019). The shared principles have led to the widespread adoption of the tidyverse ecosystem. No small part of this usage is because the tidyverse tools have been intentionally designed to ease the learning process and cognitive load for users as they engage with each new piece of the larger ecosystem. Moreover, the functionality offered by the packages within the tidyverse spans the entire data science cycle, which includes data import, visualisation, wrangling, modeling, and communication. We believe the tidyverse provides an effective and efficient pathway to data science mastery for students at a variety of different levels of experience. In this paper, we introduce the tidyverse from an educator's perspective, touching on the what (a brief introduction to the tidyverse), the why (pedagogical benefits, opportunities, and challenges), the how (scoping and implementation options), and the where (details on courses, curricula, and student populations).},
  annotation = {MAG ID: 3192696361}
}

@manual{cetinkaya-rundeletal2022a,
  type = {Manual},
  title = {Openintro: {{Data}} Sets and Supplemental Functions from '{{OpenIntro}}' Textbooks and Labs},
  author = {{\c C}etinkaya-Rundel, Mine and Diez, David and Bray, Andrew and Kim, Albert Y. and Baumer, Ben and Ismay, Chester and Paterno, Nick and Barr, Christopher},
  year = {2022}
}

@misc{cetinkaya-rundeletal2023,
  title = {{{OpenIntro Statistics}}},
  author = {{\c C}etinkaya-Rundel, Mine and Diez, David M. and Barr, Christopher},
  year = {2023},
  urldate = {2023-01-11},
  howpublished = {https://www.openintro.org/book/os/},
  file = {/Users/amcnamara/Zotero/storage/SBKDWQXB/os.html}
}

@article{chan2012,
  title = {The Role of Information Technology in Developing Students' Statistical Reasoning},
  author = {Chan, Shiau Wei and Ismail, Zaleha},
  year = {2012},
  journal = {Priced - Social and Behavior Sciences},
  volume = {46}
}

@book{chance2006,
  title = {Using Simulation to Teach and Learn Statistics},
  author = {Chance, Beth and Rossman, Allan},
  year = {2006},
  annotation = {Published: ICOTS-7, http://www.ime.usp.br/ abe/ICOTS7/Proceedings/PDFs/InvitedPapers/7E1\_CHAN.pdf}
}

@article{chang2012,
  title = {Experimental Study for Dimethyl Ether Production from Biomass Gasification and Simulation on Dimethyl Ether Production},
  author = {Chang, Jie and Fu, Yan and Luo, Zhongyang},
  year = {2012},
  journal = {Biomass and Bioenergy},
  volume = {39},
  pages = {67--72}
}

@book{chang2015,
  title = {Shiny: {{Web}} Application Framework for {{R}}},
  author = {Chang, Winston and Cheng, Joe and Allaire, J. J. and Xie, Yihui and McPherson, Jonathan},
  year = {2015}
}

@book{charmaz2006,
  title = {Constructing {{Grounded Theory}}},
  author = {Charmaz, Kathy},
  year = {2006},
  publisher = {SAGE Publications Ltd},
  address = {London ; Thousand Oaks, Calif},
  abstract = {Kathy Charmaz presents the definitive guide to doing grounded theory from a constructivist perspective. This second edition of her groundbreaking text retains the accessibility and warmth of the first edition whilst introducing cutting edge examples and practical tips.  This expanded second edition:  - explores how to effectively focus on data collection  - demonstrates how to use data for theorizing  - adds two new chapters that guide you through conducting and analysing interviews in grounded theory   - adds a new chapter on symbolic interactionism and grounded theory  - considers recent epistemological debates about the place of prior theory  - discusses the legacy of Anselm Strauss for grounded theory. This is a seminal title for anyone serious about understanding and doing grounded theory research.},
  isbn = {978-0-85702-914-0},
  langid = {english}
}

@phdthesis{chaves2012,
  title = {A {{Framework}} for {{Participatory Sensing Systems}}},
  author = {Chaves, Diego Mendez},
  year = {2012},
  school = {University of South Florida},
  keywords = {to read}
}

@article{chen2009,
  title = {A Pattern Language for Screencasting},
  author = {Chen, Nicholas and Rabb, Maurice},
  year = {2009},
  journal = {PLoP '09: Proceedings of the 16th Conference on Pattern Languages of Programs},
  number = {6}
}

@article{chick2003,
  title = {Tools for Transnumeration: {{Early}} Stages in the Art of Data Representation},
  author = {Chick, Helen},
  year = {2003},
  journal = {Mathematics education research: Innovation, networking, opportunity},
  pages = {207--214},
  abstract = {This paper considers the skills needed for data representation. A framework of transnumerative techniques that facilitate data representation is proposed and applied to the responses of 73 year 7 students to two tasks involving association. Students' responses were classified according to their levels of success in representing the association, and the types of techniques used. It was found that while students had techniques for representing data, their choices of graph type were not always suitable, and they overlooked simple techniques such as ordering and grouping data that could have made their representations clearer. Implications for doing and teaching data analysis are discussed.},
  keywords = {to read}
}

@article{chickwatson2001,
  title = {Data Representation and Interpretation by Primary School Students Working in Groups},
  author = {Chick, Helen and Watson, Jane M.},
  year = {2001},
  journal = {Mathematics Education Research Journal},
  volume = {13},
  number = {2},
  pages = {91--111},
  keywords = {to read}
}

@misc{chin2021,
  title = {Students Who Grew up with Search Engines Might Change {{STEM}} Education Forever},
  author = {Chin, Monica},
  year = {2021},
  month = sep,
  journal = {The Verge},
  urldate = {2022-10-21},
  abstract = {Professors are struggling to teach Gen Z},
  howpublished = {https://www.theverge.com/22684730/students-file-folder-directory-structure-education-gen-z},
  langid = {american},
  file = {/Users/amcnamara/Zotero/storage/W3CQ2U78/students-file-folder-directory-structure-education-gen-z.html}
}

@inproceedings{christin2012,
  title = {{{IncogniSense}}: {{An}} Anonymity-Preserving Reputation Framework for Participatory Sensing Applications},
  booktitle = {2012 {{IEEE International Conference}} on {{Pervasive Computing}} and {{Communications}}},
  author = {Christin, Delphine and Ro{\ss}kopf, Christian and Hollick, Matthias and Martucci, Leonardo A. and Kanhere, Salil S.},
  year = {2012},
  abstract = {Reputation systems rate the contributions to participatory sensing campaigns from each user by associating a reputation score. The reputation scores are used to weed out incorrect sensor readings. However, an adversary can de- anonymize the users even when they use pseudonyms by linking the reputation scores associated with multiple contributions. Since the contributed readings are usually annotated with spatiotemporal information, this poses a serious breach of privacy for the users. In this paper, we address this privacy threat by proposing a framework called IncogniSense. Our system utilizes periodic pseudonyms generated using blind signature and relies on reputation transfer between these pseudonyms. The reputation transfer process has an inherent trade-off between anonymity protection and loss in reputation. We investigate by means of extensive simulations several reputation cloaking schemes that address this tradeoff in different ways. Our system is robust against reputation corruption and a prototype implementation demonstrates that the associated overheads are minimal.}
}

@inproceedings{chu2011,
  title = {Strategies for {{Crowdsourcing}} for {{Disaster Situation Information}}},
  booktitle = {{{WIT Transactions}} on the {{Built Environment}}},
  author = {Chu, Edward T.-H. and Chen, Y.-L. and Liu, J. W. S. and Zao, J. K.},
  editor = {{119}},
  year = {2011},
  abstract = {When existing surveillance sensors used by a disaster warning and response system cannot provide adequate data for situation assessment purposes, crowdsourcing information collection can be an effective solution: People armed with wireless devices and social network services can be used as mobile human sensors. Eye-witness reports from them can complement data from in-situ physical sensors and provide the system with more extensive and detailed sensor coverage. The crowdsourcing strategy used by the system can be random, relying solely on mobility of individuals for coverage of the threatened area; or crowd- driven, with the system providing situation updates as feedbacks to aid the crowd; or system-driven with individuals moving in response to directives from the system. The relative merits of the strategies clearly depend on the disaster scenario and the characteristics of the crowd. This paper presents a general crowd model for characterizing individuals within a crowd and the crowd as a whole and an abstract mobility model of crowd movements in the threatened area. The models can be specialized to characterize different disaster scenarios and crowds and used in simulation of the crowdsourcing strategies for evaluation purposes. Data on relative performance of different strategies for two types of disasters were thus obtained.}
}

@book{cleveland1985,
  title = {The Elements of Graphing Data},
  author = {Cleveland, William S.},
  year = {1985},
  publisher = {{Wadsworth Advanced Books and Software}},
  keywords = {to buy}
}

@article{cleveland2001,
  title = {Data {{Science}}: {{An}} Action Plan for Expanding the Technical Areas of the Field of Statistics},
  author = {Cleveland, William S.},
  year = {2001},
  journal = {ISI Review},
  volume = {69},
  pages = {21--26},
  abstract = {An action plan to enlarge the technical areas of statistics focuses on the data analyst. The plan sets out six technical areas of work for a university department and advocates a specific allocation of resources devoted to research in each area and to courses in each area. The value of technical work is judged by the extent to which it benefits the data analyst, either directly or indirectly. The plan is also applicable to government research labs and corporate research organizations.}
}

@article{cleveland2001a,
  title = {Data {{Science}}: An {{Action Plan}} for {{Expanding}} the {{Technical Areas}} of the {{Field}} of {{Statistics}}},
  shorttitle = {Data {{Science}}},
  author = {Cleveland, William S.},
  year = {2001},
  journal = {International Statistical Review},
  volume = {69},
  number = {1},
  pages = {21--26},
  issn = {1751-5823},
  doi = {10.1111/j.1751-5823.2001.tb00477.x},
  urldate = {2021-06-24},
  abstract = {An action plan to enlarge the technical areas of statistics focuses on the data analyst. The plan sets out six technical areas of work for a university department, and advocates a specific allocation of resources devoted to research in each area and to courses in each area. The value of technical work is judged by the extent to which it benefits the data analyst, either directly or indirectly. The plan is also applicable to government research labs and corporate research organizations.},
  langid = {english},
  keywords = {Applications,Computing,Future,Methods,Models,Theory},
  file = {/Users/amcnamara/Zotero/storage/PHGKRISX/j.1751-5823.2001.tb00477.html}
}

@article{cobb1993,
  title = {Reconsidering {{Statistics Education}}: {{A National Science Foundation Conference}}},
  author = {Cobb, George},
  year = {1993},
  journal = {Journal of Statistics Education},
  volume = {1},
  number = {1},
  keywords = {education,statistics}
}

@article{cobb1997,
  title = {Mathematics, {{Statistics}}, and {{Teaching}}},
  author = {Cobb, George and Moore, David S.},
  year = {1997},
  month = nov,
  journal = {The American Mathematical Monthly},
  volume = {104},
  number = {9},
  pages = {801--823},
  abstract = {How does statistical thinking differ from mathematical thinking? What is the role of mathematics in statistics? If you purge statistics of its mathematical content, what intellectual substance remains? In what follows, we offer some answers to these questions and relate them to a sequence of examples that provide an overview of current statistical practice. Along the way, and especially toward the end, we point to some implications for the teaching of statistics.}
}

@incollection{cobb2004,
  title = {The {{Challenge}} of {{Developing Statistical Literacy}}, {{Reasoning}} and {{Thinking}}},
  author = {Cobb, Paul and McClain, Kay},
  editor = {{Ben-Zvi}, Dani and Garfield, Joan},
  year = {2004},
  number = {16},
  publisher = {Kluwer Academic Publishers},
  chapter = {Principles of instructional design for supporting the development of students' statistical reasoning},
  keywords = {education,statistical literacy,statistics}
}

@article{cobb2007,
  title = {The Introductory Statistics Course: {{A}} Ptolemaic Curriculum?},
  author = {Cobb, George},
  year = {2007},
  journal = {Technology Innovations in Statistics Education},
  volume = {1},
  number = {1},
  keywords = {to read}
}

@misc{codap2021,
  title = {{{CODAP}} - {{Common Online Data Analysis Platform}}},
  author = {{The Concord Consortium}},
  year = {2020},
  urldate = {2021-06-24},
  howpublished = {https://codap.concord.org/},
  file = {/Users/amcnamara/Zotero/storage/8U3RNE7L/codap.concord.org.html}
}

@article{codd1972further,
  title = {Further Normalization of the Data Base Relational Model},
  author = {Codd, Edgar F},
  year = {1972},
  journal = {Data base systems},
  volume = {6},
  pages = {33--64},
  publisher = {Prentice-Hall Englewood Cliffs, NJ}
}

@book{cohen2001,
  title = {Numbers in the {{Newsroom}}: {{Using}} Math and Statistics in the News},
  author = {Cohen, Sarah},
  year = {2001},
  publisher = {{Investigative Reporters and Editors, Inc}}
}

@article{cohn2015,
  title = {Voting {{Case Has Potential}} to {{Put House Further Out}} of {{Reach}} for {{Democrats}}},
  author = {Cohn, Nate},
  year = {2015},
  month = jun,
  journal = {The New York Times}
}

@article{colavizzaetal2020,
  title = {The Citation Advantage of Linking Publications to Research Data},
  author = {Colavizza, Giovanni and Hrynaszkiewicz, Iain and Staden, Isla and Whitaker, Kirstie and McGillivray, Barbara},
  year = {2020},
  month = apr,
  journal = {PLOS ONE},
  volume = {15},
  number = {4},
  pages = {e0230416},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0230416},
  urldate = {2022-07-15},
  abstract = {Efforts to make research results open and reproducible are increasingly reflected by journal policies encouraging or mandating authors to provide data availability statements. As a consequence of this, there has been a strong uptake of data availability statements in recent literature. Nevertheless, it is still unclear what proportion of these statements actually contain well-formed links to data, for example via a URL or permanent identifier, and if there is an added value in providing such links. We consider 531, 889 journal articles published by PLOS and BMC, develop an automatic system for labelling their data availability statements according to four categories based on their content and the type of data availability they display, and finally analyze the citation advantage of different statement categories via regression. We find that, following mandated publisher policies, data availability statements become very common. In 2018 93.7\% of 21,793 PLOS articles and 88.2\% of 31,956 BMC articles had data availability statements. Data availability statements containing a link to data in a repository---rather than being available on request or included as supporting information files---are a fraction of the total. In 2017 and 2018, 20.8\% of PLOS publications and 12.2\% of BMC publications provided DAS containing a link to data in a repository. We also find an association between articles that include statements that link to data in a repository and up to 25.36\% ({\textpm} 1.07\%) higher citation impact on average, using a citation prediction model. We discuss the potential implications of these results for authors (researchers) and journal publishers who make the effort of sharing their data in repositories. All our data and code are made available in order to reproduce and extend our results.},
  langid = {english},
  keywords = {Bibliometrics,Citation analysis,Data management,Open access publishing,Reproducibility,Science policy,Scientific publishing,Support vector machines},
  file = {/Users/amcnamara/Zotero/storage/KXGAUUKS/Colavizza et al. - 2020 - The citation advantage of linking publications to .pdf;/Users/amcnamara/Zotero/storage/BZDNDPK9/article.html}
}

@techreport{connell1997,
  title = {The {{Principles}} of {{Universal Design}}},
  author = {Connell, Bettye Rose and Jones, Mike and Mace, Ron and Mueller, Jim and Mullick, Abir and Ostroff, Elaine and Sanford, Jon and Steinfeld, Ed and Story, Molly and Vanderheiden, Gregg},
  year = {1997},
  institution = {The Center for Universal Design}
}

@incollection{conner2006,
  title = {Thinking and {{Reasoning}} with {{Data}} and {{Chance}}},
  author = {Conner, Doreen and Davies, Neville and Holmes, Peter},
  year = {2006},
  publisher = {National Council of Teachers of Mathematics},
  chapter = {Using real data and technology to develop statistical thinking}
}

@article{cook1995,
  title = {Grand {{Tour}} and {{Projection Pursuit}}},
  author = {Cook, Dianne and Buja, Andreas and Cabrera, Javier and Hurley, Catherine},
  year = {1995},
  journal = {Journal of Computational and Graphical Statistics},
  volume = {4},
  number = {3},
  pages = {155--172},
  keywords = {computer science,data projections,data visualization,exploratory data analysis,interactive dynamic graphics,statistics,technology}
}

@article{cooper2010,
  title = {Teaching Computer Science in Context},
  author = {Cooper, Steve and Cunningham, Steve},
  year = {2010},
  journal = {ACM Inroads},
  volume = {1},
  number = {1},
  pages = {5--8},
  keywords = {computer science,education}
}

@article{coopershore2008,
  title = {Students' {{Misconceptions}} in {{Interpreting Center}} and {{Variability}} of {{Data Represented}} via {{Histograms}} and {{Stem-and-Leaf Plots}}},
  author = {Cooper, Linda L. and Shore, Felice S.},
  year = {2008},
  month = jul,
  journal = {Journal of Statistics Education},
  volume = {16},
  number = {2},
  pages = {null},
  publisher = {Taylor \& Francis},
  issn = {null},
  doi = {10.1080/10691898.2008.11889559},
  urldate = {2023-06-08},
  abstract = {This paper identifies and discusses misconceptions that students have in making judgments of center and variability when data are presented graphically. An assessment addressing interpreting center and variability in histograms and stem-and-leaf plots was administered to, and follow-up interviews were conducted with, undergraduates enrolled in introductory statistics courses. Assessment items focused upon comparing the variability of two data sets of common range represented by bell-shaped histograms on a common scale, computing measures of center from data extracted from graphs, and in comparing the relative location of the mean and median on a histogram from skewed data. Students' misconceptions often stemmed from their difficulty in maintaining understanding of the data that are being represented graphically.},
  keywords = {Descriptive statistics,Mean,Median,Undergraduate statistics,Variation},
  file = {/Users/amcnamara/Zotero/storage/ADYAV592/Cooper and Shore - 2008 - Students' Misconceptions in Interpreting Center an.pdf}
}

@incollection{cordell2016,
  title = {How Not to Teach Digital Humanities},
  booktitle = {Debates in the {{Digital Humanities}} 2016},
  author = {Cordell, Ryan},
  editor = {Gold, Matthew K. and Klein, Lauren F.},
  year = {2016},
  month = may,
  eprint = {10.5749/j.ctt1cn6thb},
  eprinttype = {jstor},
  publisher = {University of Minnesota Press},
  doi = {10.5749/j.ctt1cn6thb},
  urldate = {2024-01-15},
  isbn = {978-1-4529-5148-5 978-0-8166-9954-4},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/T8Q762YV/Gold and Klein - 2016 - Debates in the Digital Humanities 2016.pdf}
}

@book{cornelissen2014,
  title = {{{DataCamp}}},
  author = {Cornelissen, Jonathan and Theuwissen, Martijn and Mesmaeker, Dieter De and Schouwenaars, Filip},
  year = {2014},
  annotation = {Published: www.datacamp.com}
}

@inproceedings{correa2009,
  title = {A {{Framework}} for Uncertainty-Aware Visual Analytics},
  booktitle = {Visual {{Analytics Science}} and {{Technology}}},
  author = {Correa, Carlos D. and Chan, Yu-Hsuan and Ma, Kwan-Liu},
  year = {2009},
  abstract = {Visual analytics has become an important tool for gaining insight on large and complex collections of data. Numerous statistical tools and data transformations, such as projections, binning and cluster- ing, have been coupled with visualization to help analysts under- stand data better and faster. However, data is inherently uncertain, due to error, noise or unreliable sources. When making decisions based on uncertain data, it is important to quantify and present to the analyst both the aggregated uncertainty of the results and the impact of the sources of that uncertainty. In this paper, we present a new framework to support uncertainty in the visual analytics process, through statistic methods such as uncertainty modeling, propagation and aggregation. We show that data transformations, such as regression, principal component analysis and k-means clustering, can be adapted to account for uncertainty. This framework leads to better visualizations that improve the decision-making process and help analysts gain insight on the analytic process itself.}
}

@article{cowan2000,
  title = {The Magical Number 4 in Short-Term Memory: {{A}} Reconsideration of Mental Storage Capacity},
  author = {Cowan, Nelson},
  year = {2000},
  journal = {Behavioral and Brain Sciences},
  volume = {24},
  pages = {87--185},
  keywords = {to read}
}

@book{cramerotti2009,
  title = {Aesthetic {{Journalism}}},
  author = {Cramerotti, Alfredo},
  year = {2009},
  publisher = {Intellect}
}

@techreport{crawford2013,
  type = {White Paper},
  title = {Big {{Data}}, {{Communities}} and {{Ethical Resilience}}: {{A Framework}} for {{Action}}},
  author = {Crawford, Kate and Faleiros, Gustavo and Luers, Amy and Meier, Patrick and Perlich, Claudia and Thorp, Jer},
  year = {2013},
  institution = {Bellagio/PopTech Fellows}
}

@book{crockford2008,
  title = {{{JavaScript}}: {{The Good Parts}}},
  author = {Crockford, Douglas},
  year = {2008},
  publisher = {O'Reilly}
}

@article{curcio1989,
  title = {Comprehension of Mathematical Relationships Expressed in Graphs},
  author = {Curcio, Frances R.},
  year = {1989},
  journal = {Journal for Research in Mathematics Education},
  volume = {18},
  number = {5},
  pages = {382--393},
  abstract = {In this study, the schema-theoretic perspective of understanding general discourse was extended to include graph comprehension. Fourth graders (n = 204) and seventh graders (n = 185) were given a prior-knowledge inventory, a graph test, and the SRA Reading and Mathematics Achievement Tests during four testing sessions. The unique predictors of graph comprehension for Grade 4 included reading achievement, mathematics achievement, and prior knowledge of the topic, mathematical content, and form of the graph. The unique predictors for Grade 7 were the same except that prior knowledge of topic and graphical form were not included. The results suggest that children should be involved in graphing activities to build and expand relevant schemata needed for comprehension.},
  keywords = {to read}
}

@article{curry1995,
  title = {Rethinking {{Rights}} and {{Responsibilities}} in {{Geographic Information Systems}}: {{Beyond}} the {{Power}} of the {{Image}}},
  author = {Curry, Michael R.},
  year = {1995},
  journal = {Cartography and Geographic Information Systems},
  volume = {22},
  number = {1},
  pages = {58--69},
  abstract = {With respect to the practice of geography and of science more generally, geographic information systems raise fundamental questions concerning rights and responsibilities. On the one hand, the systems appear both to support and to appeal to a traditional view of image of science. On the other hand, the size and complexity of the systems undercuts traditional ways of thinking about those issues, and appears to require new ways of thinking. Yet there are certain features of the systems that make this rethinking more difficult. The article analyses this traditional image of science, points to suggested alternatives, and characterizes those features of science and geographic information systems that will make more difficult the development of an alternative.}
}

@article{curry1997,
  title = {The Digital Individual and the Private Realm},
  author = {Curry, Michael R.},
  year = {1997},
  journal = {Annals of the Association of American Geographers},
  volume = {87},
  number = {4},
  pages = {681--699},
  abstract = {Geographic information systems and the technological family associated with them---global positioning systems, geodemographics, and remote surveillance systems---raise important questions with respect to the issue of privacy. Of most immediate import, the systems store and represent data in ways that render ineffective the most popular safeguards against privacy abuse. But the systems are associated with more fundamental changes in the right to privacy and even, some would say, with challenges to the possibility of privacy itself. They make reasonable and acceptable the view that technological change is inevitable and autonomous, and therefore, too, are the development of increasingly comprehensive dossiers on individuals and households and the use of increasingly powerful means for the technological enhancements of vision. And their use in the creation of data profiles supports a wide-ranging reconceptualization of community, place, and individual. Nonetheless, in the ways they create and use digital profiles, the systems do offer suggestions for a partial remedy to the problems that they have created.},
  keywords = {to read}
}

@misc{dataurban2019,
  title = {Building an {{R}} Community at the {{Urban Institute}}},
  author = {Data@Urban},
  year = {2019},
  month = feb,
  journal = {Medium},
  urldate = {2022-03-30},
  abstract = {R is one of the premier software environments for data science and one of the fastest-growing programming languages. Created by{\dots}},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/RR6EG6B5/building-an-r-community-at-the-urban-institute-b66739aaaaa7.html}
}

@article{deek1998,
  title = {A Survey and Critical Analysis of Tools for Learning Programming},
  author = {Deek, Fadi P. and McHugh, James A.},
  year = {1998},
  journal = {Computer Science Education},
  volume = {8},
  number = {2},
  pages = {130--178},
  abstract = {Systems and methodologies have been developed to improve the learning and practice of programming. We examine the kinds of support tools that have been developed to date, and we discuss their role in meeting the needs of beginning students. We begin with a literature review to summarize the actual difficulties involved in learning the tasks of program development. A comprehensive survey of environments developed to support the learning of problem solving and programming follows, covering programming environments, debugging aids, intelligent tutoring systems, and intelligent programming environments. A careful analysis of these systems uncovers the limitations that have prevented them from accomplishing their goals.},
  keywords = {computer science,education}
}

@book{deiteldietal2020,
  title = {Intro to {{Python}} for the Computer and Data Sciences: Learning to Program with {{AI}}, Big Data and the Cloud},
  shorttitle = {Intro to {{Python}} for the Computer and Data Sciences},
  author = {Deitel, Paul J. and Dietal, Harvey},
  year = {2020},
  edition = {1st edition},
  publisher = {Pearson Education, Inc},
  address = {New York},
  isbn = {978-0-13-540467-6},
  lccn = {QA76.73.P98 D45 2020},
  keywords = {Python (Computer program language)}
}

@inproceedings{delcourtetal2024,
  title = {What's in a {{Social Computing Course}}: {{Analyzing Computer}} and {{Information Science Syllabi}}},
  shorttitle = {What's in a {{Social Computing Course}}},
  booktitle = {Proceedings of the 6th {{Annual Symposium}} on {{HCI Education}}},
  author = {Delcourt, Catherine and Venkatagiri, Sukrit and Chandrasekharan, Eshwar},
  year = {2024},
  month = jun,
  series = {{{EduCHI}} '24},
  pages = {1--8},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3658619.3658623},
  urldate = {2024-06-14},
  abstract = {Social computing systems---such as social media and e-commerce platforms as well as search engines and collaboration software--- not only drive vast economic value and societal impact, but are also becoming prominent topics in policy discourse. Although social technology companies heavily recruit students from Computer and Information Science (CS and IS) programs, and social computing is a well-established scholarly field within human-computer interaction (HCI) focused on the social interactions between people mediated through computational systems, little is known about social computing education. Consequently, in this paper we analyzed 25 undergraduate and graduate level courses titled ``social computing.'' First, as a fast-paced discipline that follows developments in computing as well as related societal implications, we highlight foundational and emergent topics. Second, we map these topics onto the life cycle of social computing systems to highlight gaps in coverage. Third, we map social computing topics to the 2023 ACM CS Curricula Body of Knowledge to provide a framework for introducing social computing concepts into CS and IS curricula. We find that social computing courses require diverse skill sets both within HCI and CS, as well as inter-disciplinary concepts from Sociology, Economics, among others. We conclude with guidelines for designing new social computing courses and discuss ways to critically examine the role of---and the power held by---system builders.},
  isbn = {9798400716591},
  keywords = {computer science education,CSEd,curriculum,HCI education,social computing,syllabi},
  file = {/Users/amcnamara/Zotero/storage/T3GHTEZR/Delcourt et al. - 2024 - What's in a Social Computing Course Analyzing Com.pdf}
}

@incollection{delmasetal2014,
  title = {{Using TinkerPlots™ to develop tertiary students' statistical thinking in a modeling-based introductory statistics class}},
  booktitle = {{Mit Werkzeugen Mathematik und Stochastik lernen -- Using Tools for Learning Mathematics and Statistics}},
  author = {{delMas}, Robert and Garfield, Joan and Zieffler, Andrew},
  editor = {Wassong, Thomas and Frischemeier, Daniel and Fischer, Pascal R. and Hochmuth, Reinhard and Bender, Peter},
  year = {2014},
  pages = {405--420},
  publisher = {Springer Fachmedien Wiesbaden},
  address = {Wiesbaden},
  doi = {10.1007/978-3-658-03104-6_29},
  urldate = {2021-10-18},
  isbn = {978-3-658-03103-9 978-3-658-03104-6},
  langid = {ngerman}
}

@article{demiralp2014,
  title = {Learning {{Perceptual Kernels}} for {{Visualization Design}}},
  author = {Demiralp, {\c C}a{\v g}atay and Bernstein, Michael S. and Heer, Jeffrey},
  year = {2014},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {20},
  number = {12},
  abstract = {Visualization design can benefit from careful consideration of perception, as different assignments of visual encoding variables such as color, shape and size affect how viewers interpret data. In this work, we introduce perceptual kernels: distance matrices derived from aggregate perceptual judgments. Perceptual kernels represent perceptual differences between and within visual variables in a reusable form that is directly applicable to visualization evaluation and automated design. We report results from crowd- sourced experiments to estimate kernels for color, shape, size and combinations thereof. We analyze kernels estimated using five different judgment types --- including Likert ratings among pairs, ordinal triplet comparisons, and manual spatial arrangement --- and compare them to existing perceptual models. We derive recommendations for collecting perceptual similarities, and then demonstrate how the resulting kernels can be applied to automate visualization design decisions.},
  keywords = {experimental,IEEEviz}
}

@misc{deneroetal2022,
  title = {\texttt{datascience} 0.17.5},
  author = {DeNero, John and Culler, David and Wan, Allen and Lau, Sam},
  year = {2022},
  month = jun,
  urldate = {2021-07-19},
  file = {/Users/amcnamara/Zotero/storage/QX9SNYEN/datascience.html}
}

@article{denning1989,
  title = {A Debate on Teaching Computing Science},
  editor = {Denning, Peter J.},
  year = {1989},
  month = dec,
  journal = {Communications of the ACM},
  volume = {32},
  number = {12},
  pages = {1397--1414},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/76380.76381},
  urldate = {2021-06-11},
  langid = {english}
}

@article{desousagomes2018,
  title = {Teaching Statistics Using {{R}} at a College or a University Level: It Can Be Possible?},
  shorttitle = {Teaching Statistics Using {{R}} at a College or a University Level},
  author = {{de Sousa}, Bruno and Gomes, Dulce},
  year = {2018},
  month = jul,
  publisher = {Proceedings of the 10th International Conference on Teaching Statistics (ICOTS10)},
  doi = {10/?topic=9},
  urldate = {2021-07-27},
  abstract = {Although the advantages of R are well-known (free, open source, continually updated by experts), it is not the first choice among college students, especially those not majoring in mathematics or statistics. A problem that appears when teaching R is that once the great potential of the software is understood, the temptation is to focus immediately on more advanced analysis, which adds frustration for beginning learners of R. Our online module begins with basic visualizations and summaries of data, followed by how mathematical functions can be used to model data, and concludes with some inferential statistics, such as linear regression. This approach will not only help R beginners, but can also be used by school teachers in a global plan to bring R to the youngest learners.},
  langid = {english},
  annotation = {Accepted: 2020-02-11T11:05:24Z},
  file = {/Users/amcnamara/Zotero/storage/FA4KZMWB/26890.html}
}

@article{deveauxetal2017,
  title = {Curriculum {{Guidelines}} for {{Undergraduate Programs}} in {{Data Science}}},
  author = {De Veaux, Richard D. and Agarwal, Mahesh and Averett, Maia and Baumer, Benjamin S. and Bray, Andrew and Bressoud, Thomas C. and Bryant, Lance and Cheng, Lei Z. and Francis, Amanda and Gould, Robert and Kim, Albert Y. and Kretchmar, Matt and Lu, Qin and Moskol, Ann and Nolan, Deborah and Pelayo, Roberto and Raleigh, Sean and Sethi, Ricky J. and Sondjaja, Mutiara and Tiruviluamala, Neelesh and Uhlig, Paul X. and Washington, Talitha M. and Wesley, Curtis L. and White, David and Ye, Ping},
  year = {2017},
  month = mar,
  journal = {Annual Review of Statistics and Its Application},
  volume = {4},
  number = {1},
  pages = {15--30},
  issn = {2326-8298, 2326-831X},
  doi = {10.1146/annurev-statistics-060116-053930},
  urldate = {2023-07-07},
  abstract = {The Park City Math Institute 2016 Summer Undergraduate Faculty Program met for the purpose of composing guidelines for undergraduate programs in data science. The group consisted of 25 undergraduate faculty from a variety of institutions in the United States, primarily from the disciplines of mathematics, statistics, and computer science. These guidelines are meant to provide some structure for institutions planning for or revising a major in data science.},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/PA77NY6T/De Veaux et al. - 2017 - Curriculum Guidelines for Undergraduate Programs i.pdf}
}

@techreport{diakopoulos2014,
  title = {Algorithmic Accountability Reporting: {{On}} the Investigation of Black Boxes},
  author = {Diakopoulos, Nick},
  year = {2014},
  institution = {Tow Center for Digital Journalism}
}

@phdthesis{diatchki2007,
  title = {High-Level Abstractions for Low-Level Programming},
  author = {Diatchki, Iavor Sotirov},
  year = {2007},
  month = may,
  school = {Oregon Health and Science University},
  keywords = {computer science,dissertation}
}

@book{diez2014,
  title = {Introductory {{Statistics}} with {{Randomization}} and {{Simulation}}},
  author = {Diez, David M. and Barr, Christopher D. and {\c C}etinkaya-Rundel, Mine},
  year = {2014},
  publisher = {OpenIntro},
  annotation = {Published: www.openintro.org}
}

@book{dignazioklein2020,
  title = {Data Feminism},
  author = {D'Ignazio, Catherine and Klein, Lauren F.},
  year = {2020},
  series = {Strong Ideas Series},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts},
  abstract = {"We have seen through many examples that data science and artificial intelligence can reinforce structural inequalities like sexism and racism. Data is power, and that power is distributed unequally. This book offers a vision for a feminist data science that can challenge power and work towards justice. This book takes a stand against a world that benefits some (including the authors, two white women) at the expense of others. It seeks to provide concrete steps for data scientists seeking to learn how feminism can help them work towards justice, and for feminists seeking to learn how their own work can carry over to the growing field of data science. It is addressed to professionals in all fields where data-driven decisions are being made, as well as to communities that want to better understand the data that surrounds them. It is written for everyone who seeks to better understand the charts and statistics that they encounter in their day-to-day lives, and for everyone who seeks to better communicate the significance of such charts and statistics to others. This is an example-driven book written with a broad audience of scholars, students, and practitioners in mind. It offers a way of thinking about data, both their uses and their limits, that is informed by direct experience, by a commitment to action, and by the ideas associated with intersectional feminist thought"--},
  isbn = {978-0-262-04400-4},
  lccn = {HQ1190 .D574 2020},
  keywords = {Big data,Feminism,Feminism and science,Methodology Social aspects,Power (Social sciences),Quantitative research,Social aspects}
}

@misc{dijkstra1989,
  title = {On the Cruelty of Really Teaching Computing Science},
  author = {Dijkstra, Edsger W.},
  year = {1989},
  journal = {Communications of the ACM},
  howpublished = {https://www.cs.utexas.edu/users/EWD/transcriptions/EWD10xx/EWD1036.html}
}

@phdthesis{doan2002,
  title = {Learning to {{Map}} between Structured Representations of Data},
  author = {Doan, AnHai},
  year = {2002},
  school = {University of Washington},
  keywords = {education,statistics}
}

@inproceedings{donoho2015,
  title = {50 Years of {{Data Science}}},
  booktitle = {Princeton {{NJ}}, {{Tukey Centennial Workshop}}},
  author = {Donoho, David},
  year = {2015},
  month = sep,
  abstract = {More than 50 years ago, John Tukey called for a reformation of academic statistics. In `The Future of Data Analysis', he pointed to the existence of an as-yet unrecognized science, whose subject of interest was learning from data, or `data analysis'. Ten to twenty years ago, John Chambers, Bill Cleveland and Leo Breiman independently once again urged academic statistics to expand its boundaries beyond the classical domain of theoretical statistics; Chambers called for more emphasis on data preparation and presentation rather than statistical modeling; and Breiman called for emphasis on prediction rather than inference. Cleveland even suggested the catchy name ``Data Science'' for his envisioned field. A recent and growing phenomenon is the emergence of ``Data Science'' programs at major universities, including UC Berkeley, NYU, MIT, and most recently the Univ. of Michigan, which on September 8, 2015 announced a \$100M ``Data Science Initiative'' that will hire 35 new faculty. Teaching in these new programs has significant overlap in curricular subject matter with traditional statistics courses; in general, though, the new initiatives steer away from close involvement with academic statistics departments. This paper reviews some ingredients of the current ``Data Science moment'', including recent commentary about data science in the popular media, and about how/whether Data Science is really different from Statistics. The now-contemplated field of Data Science amounts to a superset of the fields of statistics and machine learning which adds some technology for `scaling up' to `big data'. This chosen superset is motivated by commercial rather than intellectual developments. Choosing in this way is likely to miss out on the really important intellectual event of the next fifty years. Because all of science itself will soon become data that can be mined, the imminent revolution in Data Science is not about mere `scaling up', but instead the emergence of scientific studies of data analysis science-wide. In the future, we will be able to predict how a proposal to change data analysis workflows would impact the validity of data analysis across all of science, even predicting the impacts field-by-field. Drawing on work by Tukey, Cleveland, Chambers and Breiman, I present a vision of data science based on the activities of people who are `learning from data', and I describe an academic field dedicated to improving that activity in an evidence-based manner. This new field is a better academic enlargement of statistics and machine learning than today's Data Science Initiatives, while being able to accommodate the same short-term goals.}
}

@book{downey2002,
  title = {How to Think like a Computer Scientist: Learning with Python},
  author = {Downey, Allen and Elkner, Jeffrey and Meyers, Chris},
  year = {2002},
  publisher = {Green Tea Press}
}

@inproceedings{downs2010,
  title = {Are Your Participants Gaming the System? {{Screening Mechanical Turk}} Workers},
  booktitle = {{{CHI}} 2010},
  author = {Downs, Julie S. and Holbrook, Mandy B. and Sheng, Steve and Cranor, Lorrie Faith},
  year = {2010},
  abstract = {In this paper we discuss a screening process used in conjunction with a survey administered via Amazon.com's Mechanical Turk. We sought an easily implementable method to disqualify those people who participate but don't take the study tasks seriously. By using two previously pilot tested screening questions, we identified 764 of 1,962 people who did not answer conscientiously. Young men seem to be most likely to fail the qualification task. Those that are professionals, students, and non-workers seem to be more likely to take the task seriously than financial workers, hourly workers, and other workers. Men over 30 and women were more likely to answer seriously.},
  keywords = {data,mTurk,psychology}
}

@article{drill2013,
  title = {Mobile Applications for Participatory Sensing},
  author = {Drill, Sabrina L.},
  year = {2013},
  journal = {Journal of Extension},
  volume = {51},
  number = {1}
}

@article{duesberyetal2017,
  title = {Thinking Critically about Data Displays},
  author = {Duesbery, Luke and {Braun-Monegan}, Jenelle and Liu, Kimy and McCoy, Jan},
  year = {2017},
  month = jan,
  journal = {Journal of Visual Literacy},
  volume = {36},
  number = {1},
  pages = {41--54},
  publisher = {Routledge},
  issn = {1051-144X},
  doi = {10.1080/1051144X.2017.1331681},
  urldate = {2023-06-14},
  abstract = {The quality of a data display can have an impact on the interpretation of those data. A survey of the literature indicates that data displays can vary in quality of accuracy, clarity, and efficacy. In this study we develop and apply an evaluative rubric to graphs in a sample of six education journals: three research and three practitioner. Results indicate that graph quality is typically high in educational journals, however, in practitioner oriented journals issues around graph clarity and efficacy should be addressed. Common error patterns are pinpointed, and four recommendations are made to authors and editors: focus on meaningful labels, increase amount of data displayed, portray multiple relationships, and elaborate with supporting text.},
  keywords = {data displays,Data graphics,graphs,information science,research to practice,research use,specifiers,visual literacy},
  file = {/Users/amcnamara/Zotero/storage/T9RXXQI7/Duesbery et al. - 2017 - Thinking critically about data displays.pdf}
}

@article{dumbill2012,
  title = {What Is Big Data? {{An}} Introduction to the Big Data Landscape},
  author = {Dumbill, Edd},
  year = {2012},
  month = jan,
  journal = {O'Reilly}
}

@incollection{dunham2008,
  title = {Equity and Use of Educational Technology in Mathematics},
  booktitle = {Research on {{Technology}} and the {{Teaching}} and {{Learning}} of {{Mathematics}}},
  author = {Dunham, Penelope and Henessy, Sara},
  editor = {Heid, M. Kathleen and Blume, Glendon W.},
  year = {2008},
  volume = {1},
  publisher = {National Council of Teachers of Mathematics},
  address = {Reston, VA},
  keywords = {education,equity,mathematics,technology}
}

@article{dupuis2012,
  title = {A Multi-Institutional Study of the Relationship between High School Mathematics Achievement and Performance in Introductory College Statistics},
  author = {Dupuis, Danielle N. and Medhanie, Amanuel and Harwell, Michael and Lebeau, Brandon and Monson, Debra and Post, Thomas R.},
  year = {2012},
  journal = {Statistics Education Research Journal},
  volume = {11},
  number = {1}
}

@incollection{easterbrooketal2008,
  title = {Selecting {{Empirical Methods}} for {{Software Engineering Research}}},
  booktitle = {Guide to {{Advanced Empirical Software Engineering}}},
  author = {Easterbrook, Steve and Singer, Janice and Storey, Margaret-Anne and Damian, Daniela},
  editor = {Shull, Forrest and Singer, Janice and Sj{\o}berg, Dag I. K.},
  year = {2008},
  pages = {285--311},
  publisher = {Springer London},
  address = {London},
  doi = {10.1007/978-1-84800-044-5_11},
  urldate = {2021-06-15},
  isbn = {978-1-84800-043-8 978-1-84800-044-5},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/XZRHFC3I/Easterbrook et al. - 2008 - Selecting Empirical Methods for Software Engineeri.pdf}
}

@book{eckstrom2012,
  title = {Jakob {{Bernoulli}}'s {{Theory}} of {{Inference}}},
  author = {Eckstr{\"o}m, Joakim},
  year = {2012},
  abstract = {This review of Ars Conjectandi, written on the eve of its 300th anniversary, discusses an aspect of Bernoulli's magnum opus which hitherto has not received the attention it merits. Bernoulli envisioned a theory for the advancement of science based on the idea of pairing empirical evidence with the then-novel concept of probability. This theory of inference, which he termed ``ars conjectandi'', was intended to complement the predominant axiomatic-deductive method where the latter could not be applied successfully. In the 300 years since its publication, Bernoulli's idea went through ups and downs, but eventually ended up as the defining characteristic of statistical science and a cornerstone of modern science. This review discusses the historical context from which Bernoulli's idea was conceived, his sources of inspiration, and provides a detailed account of his theory of inference.}
}

@techreport{education1983,
  title = {A {{Nation}} at {{Risk}}: {{The Imperative}} for {{Educational Reform}}},
  author = {on Education, The National Commission},
  year = {1983},
  institution = {United States Department of Education},
  keywords = {education}
}

@article{efron1986,
  title = {Bootstrap Methods for Standard Errors, Confidence Intervals, and Other Measures of Statistical Accuracy},
  author = {Efron, B. and Tibshirani, R.},
  year = {1986},
  journal = {Statistical Science},
  volume = {1},
  number = {1},
  pages = {54--77},
  abstract = {This is a review of bootstrap methods, concentrating on basic ideas and applications rather than theoretical considerations. It begins with an exposition of the bootstrap estimate of standard error for one-sample situations. Several examples, some involving quite complicated statistical procedures, are given. The bootstrap is then extended to other measures of statistical accuracy such as bias and prediction error, and to complicated data structures such as time series, censored data, and regression models. Several more examples are presented illustrating these ideas. The last third of the paper deals mainly with bootstrap confidence intervals.},
  keywords = {statistics}
}

@article{eicher2001,
  title = {Dasymetric Mapping and Areal Interpolation: {{Implementation}} and Evaluation},
  author = {Eicher, Cory L. and Brewer, Cynthia A.},
  year = {2001},
  journal = {Cartography and Geographic Information Systems},
  volume = {28},
  number = {2},
  pages = {125--138},
  abstract = {Dasymetric maps display statistical data in meaningful spatial zones. Such maps can be preferable to choropleth maps that show data by enumeration zones, because dasymetric zones more accurately represent underlying data distributions. Though dasymetric mapping has existed for well over a century, the methods for producing these maps have not been thoroughly examined. In contrast, research on areal interpolation has been more thorough and has examined methods of transferring data from one set of map zones to another, an issue that is applicable to dasymetric mapping. Inspired by this work, we tested five dasymetric mapping methods, including methods derived from work on areal interpolation. Dasymetric maps of six socio-economic variables were produced fm a study area of 159 counties in the eastern U.S. using county choropleth data and ancillary land-use data. Both polygonal (vector) and grid (raster) dasymetric methods were tested. We evaluated map accuracy using both statistical analyses and visual presentations of error. A repeated-measures analysis of variance showed that the traditional limiting variable method had significantly lower error than the other four methods. In addition, polygon methods had lower error than their grid-based counterparts, though the difference was not statistically significant. Error maps largely supported the conclusions from the statistical analysis, while also presenting patterns of error that were not obvious from the statistics.},
  keywords = {to read}
}

@article{eickwills1995,
  title = {High Interaction Graphics},
  author = {Eick, Stephen G. and Wills, Graham J.},
  year = {1995},
  month = mar,
  journal = {European Journal of Operational Research},
  volume = {81},
  number = {3},
  pages = {445--459},
  issn = {0377-2217},
  doi = {10.1016/0377-2217(94)00188-I},
  urldate = {2023-06-14},
  abstract = {Examining data using graphical tools, such as histograms, quantile plots, scatterplots and the like, is a necessary part of any serious analysis effort. With the advent of inexpensive graphics-capable desktop computing, such tools are generally available. But the use of computers enables more than simply reproducing static plots on a display; it allows users to interact with plots, changing parameters, querying, zooming and linking plots together so that interesting features of one plot can be seen in the light of the others. In this paper we discuss the core features of interactive graphics, investigate how familiar plots can be made interactive and show examples of interactive graphics for general and specific data analysis.},
  langid = {english},
  keywords = {Computing,Exploratory data analysis,Interactive graphics},
  file = {/Users/amcnamara/Zotero/storage/8QPKFWMD/Eick and Wills - 1995 - High interaction graphics.pdf}
}

@article{ellior2007,
  title = {Color and Psychological Functioning: {{The}} Effect of Red on Performance Attainment},
  author = {Ellior, Andrew J. and Maier, Markus A. and Moller, Arlen C. and Friedman, Ron and Meinhardt, J{\"o}rg},
  year = {2007},
  journal = {Journal of Experimental Psychology},
  volume = {136},
  number = {1},
  pages = {154--168}
}

@article{emerson2013,
  title = {The Generalized Pairs Plot},
  author = {Emerson, John W. and Green, Walton A. and Schloerke, Barret and Crowley, Jason and Cook, Dianne and Hofmann, Heike and Wickham, Hadley},
  year = {2013},
  journal = {Journal of Computational and Graphical Statistics},
  volume = {22},
  number = {1},
  pages = {79--91},
  abstract = {This article develops a generalization of the scatterplot matrix based on the recognition that most datasets include both categorical and quantitative information. Traditional grids of scatterplots often obscure important features of the data when one or more variables are categorical but coded as numerical. The generalized pairs plot offers a range of displays of paired combinations of categorical and quantitative variables. A mosaic plot, fluctuation diagram, or faceted bar chart may be used to display two categorical variables. A side-by-side boxplot, stripplot, faceted histogram, or density plot helps visualize a categorical and a quantitative variable. A traditional scatterplot is suitable for displaying a pair of numerical variables, but options also support density contours or annotating summary statistics such as the correlation and number of missing values, for example. By combining these, the generalized pairs plot may help to reveal structure in multivariate data that otherwise might go unnoticed in the process of exploratory data analysis. Two different R packages provide implementations of the generalized pairs plot, gpairs and GGally. Supplementary materials for this article are available online on the journal web site.}
}

@article{ericksonetal2019,
  title = {Data {{Moves}}},
  author = {Erickson, Tim and Wilkerson, Michelle and Finzer, William and Reichsman, Frieda},
  year = {2019},
  journal = {Technology Innovations in Statistics Education},
  volume = {12},
  number = {1},
  doi = {10.5070/T5121038001},
  urldate = {2023-01-10},
  abstract = {When experienced analysts explore data in a rich environment, they often transform the dataset. For example, they may choose to group or filter data, calculate new variables and summary measures, or reorganize a dataset by changing its structure or merging it with other information. Such actions background, highlight, or even fundamentally change particular features of the data, allowing different types of questions to be explored. We call these actions data moves. In this paper, we argue that paying explicit attention to data moves, as well as their purposes and consequences, is necessary for educators to support student learning about data. This is especially needed in an era when students are expected to develop critical literacy around data and engage in purposeful, self-directed exploration of large and often complex datasets.},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/8ESAXI2B/Erickson et al. - 2019 - Data Moves.pdf}
}

@article{ericssonsimon1998,
  title = {How to {{Study Thinking}} in {{Everyday Life}}: {{Contrasting Think-Aloud Protocols With Descriptions}} and {{Explanations}} of {{Thinking}}},
  shorttitle = {How to {{Study Thinking}} in {{Everyday Life}}},
  author = {Ericsson, K. Anders and Simon, Herbert A.},
  year = {1998},
  month = jul,
  journal = {Mind, Culture, and Activity},
  volume = {5},
  number = {3},
  pages = {178--186},
  publisher = {Routledge},
  issn = {1074-9039},
  doi = {10.1207/s15327884mca0503_3},
  urldate = {2023-06-07}
}

@article{ernst2004,
  title = {Permutation Methods: {{A}} Basis for Exact Inference},
  author = {Ernst, Michael D.},
  year = {2004},
  month = nov,
  journal = {Statistical Science},
  volume = {19},
  number = {4},
  pages = {676--685},
  abstract = {The use of permutation methods for exact inference dates back to Fisher in 1935. Since then, the practicality of such methods has increased steadily with computing power. They can now easily be employed in many situations without concern for computing difficulties. We discuss the reasoning behind these methods and describe situations when they are exact and distribution-free. We illustrate their use in several examples.}
}

@book{ervin2015,
  title = {{{MAUP}}: {{An Introduction}} to the {{Modifiable Areal Unit Problem}}},
  author = {Ervin, Daniel},
  year = {2015}
}

@book{ESRME2005,
  title = {Working {{Group}} 5: {{Stochastic Thinking}}},
  year = {2005},
  publisher = {European Society for Research in Mathematics Education},
  keywords = {education,statistics}
}

@article{everson2008,
  title = {Implementing New Reform Guidelines in Teaching Introductory College Statistics Courses},
  author = {Everson, Michelle and Zieffler, Andrew and Garfield, Joan},
  year = {2008},
  journal = {Teaching Statistics},
  volume = {30},
  number = {3},
  keywords = {to read}
}

@inproceedings{exel2010,
  title = {The {{Impact}} of Crowdsourcing on Spatial Data Quality Indicators},
  booktitle = {{{GIScience}} 2010},
  author = {van Exel, M. and Dias, E. and Fruijtier, S.},
  year = {2010},
  keywords = {crowdsourcing,GIS,spatial statistics}
}

@book{falguerolles1996,
  title = {A Tribute to {{J Bertin}}'s {{Graphical Data Analysis}}},
  author = {de Falguerolles, Antoine and Friedrich, Felix and Sawitzki, G{\"u}nther},
  year = {1996},
  month = nov
}

@book{fayyad2002,
  title = {Information {{Visualization}} in {{Data Mining}} and {{Knowledge Discovery}}},
  editor = {Fayyad, Usama and Grinstein, Georges G. and Wierse, Andreas},
  year = {2002},
  publisher = {Morgan Kaufmann Publishers}
}

@inproceedings{feinberg2017,
  title = {A {{Design Perspective}} on {{Data}}},
  booktitle = {Proceedings of the 2017 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Feinberg, Melanie},
  year = {2017},
  month = may,
  series = {{{CHI}} '17},
  pages = {2952--2963},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3025453.3025837},
  urldate = {2023-07-10},
  abstract = {Empirical studies invariably show that data generation is situationally contingent and interpretively flexible, even when data is collected automatically. This essay situates data generation within a design perspective, demonstrating how data creation can be understood as a multilayered set of interlocking design activities. By showing how data is infused with design, this paper argues that any "use" of data represents a continuation of its design. We are always designers of data, never its mere appropriators.},
  isbn = {978-1-4503-4655-9},
  keywords = {data,design,infrastructure,materiality,metadata},
  file = {/Users/amcnamara/Zotero/storage/KEE2BJ47/Feinberg - 2017 - A Design Perspective on Data.pdf}
}

@article{feinerer2008,
  title = {Text Mining Infrastructure in {{R}}},
  author = {Feinerer, Ingo and Hornik, Kurt},
  year = {2008},
  journal = {Journal of Statistical Software},
  volume = {25},
  number = {5},
  pages = {1--54}
}

@book{feinerer2014,
  title = {Tm: {{Text}} Mining Package},
  author = {Feinerer, Ingo and Hornik, Kurt},
  year = {2014}
}

@article{fellows2012,
  title = {Deducer: {{A Data Analysis GUI}} for {{R}}},
  author = {Fellows, Ian},
  year = {2012},
  journal = {Journal of Statistical Software},
  volume = {49},
  number = {8}
}

@book{felton2015,
  title = {Geometric {{Choropleths}} 1895 vs 1978},
  author = {Felton, Nicholas},
  year = {2015}
}

@article{fergussonpfannkuch2021,
  title = {Introducing Teachers Who Use {{GUI-driven}} Tools for the Randomization Test to Code-Driven Tools},
  author = {Fergusson, Anna and Pfannkuch, Maxine},
  year = {2021},
  month = jun,
  journal = {Mathematical Thinking and Learning},
  pages = {1--21},
  issn = {1098-6065, 1532-7833},
  doi = {10.1080/10986065.2021.1922856},
  urldate = {2021-07-21},
  langid = {english}
}

@techreport{few2008,
  title = {Dual-Scaled Axes in Graphs: {{Are}} They Ever the Best Solution?},
  author = {Few, Stephen},
  year = {2008},
  institution = {Perceptual Edge}
}

@techreport{few2010,
  title = {Coordinated {{Highlighting}} in {{Context}}: {{Bringing Multidimensional Connections}} to {{Light}}},
  author = {Few, Stephen},
  year = {2010},
  institution = {Perceptual Edge},
  abstract = {A promising visual analysis technique was first proposed back in the late 1970s, which has since been researched fairly well, but has seldom been integrated into commercial software and never to its full potential. I'm referring to the technique called brushing and linking. Chances are, you've never heard of it---at least not by this name. If this is the case, I'd like to introduce you to a powerful way to spot and examine otherwise elusive meanings in quantitative data.}
}

@book{field2015,
  title = {When Is a Heat Map Not a Heat Map},
  author = {Field, Kenneth},
  year = {2015},
  month = feb
}

@book{fincher2004,
  title = {Computer {{Science Education Research}}},
  editor = {Fincher, Sally and Petre, Marian},
  year = {2004},
  publisher = {RoutledgeFalmer},
  keywords = {computer science,education}
}

@inproceedings{fincher2010,
  title = {Comparing {{Alice}}, {{Greenfoot}} \& {{Scratch}}},
  booktitle = {{{SIGCSE}}'10},
  author = {Fincher, Sally and Cooper, Stephen and K{\"o}lling, Michael and Maloney, John},
  year = {2010},
  pages = {192--193},
  publisher = {ACM},
  keywords = {computer science,education}
}

@article{fincher2010a,
  title = {Machines for Thinking},
  author = {Fincher, Sally and Utting, Ian},
  year = {2010},
  journal = {ACM Transactions on Computing Education},
  volume = {10},
  number = {4},
  keywords = {computer science,education}
}

@misc{finzer2002,
  title = {Fathom: {{Dynamic Data Software}} (Version 2.1)},
  author = {Finzer, William},
  year = {2002},
  address = {Emeryville, CA},
  howpublished = {Key Curriculum Press}
}

@inproceedings{finzer2002a,
  title = {The {{Fathom Experience}}: {{Is Research-Based Development}} of a {{Commercial Statistics Learning Environment Possible}}?},
  booktitle = {{{ICOTS-6}}},
  author = {Finzer, William},
  year = {2002},
  abstract = {Research on learning and commercial software development competes strongly for a project's scarce resources, and yet they have widely overlapping goals. If they could be made to coexist, their synergy could improve both processes. On the research side, to use software to help understand how students perceive and learn statistical concepts requires a software platform that is stable, easy for students to use, and flexible enough to allow different models to be tried; that is, the research benefits from a smoothly functioning development process. On the development side, there is great need for insight into the learning process to inform the software design, and need for research methods to test whether any given design works with students and improves their statistical understanding.},
  keywords = {to read}
}

@article{finzer2013,
  title = {The {{Data Science Education Dilemma}}},
  author = {Finzer, William},
  year = {2013},
  journal = {Technology Innovations in Statistics Education},
  volume = {7},
  number = {2}
}

@book{finzer2014,
  title = {Hierarchical Data Visualization as a Tool for Developing Student Understanding of Variation of Data Generated in Simulations},
  author = {Finzer, William},
  year = {2014},
  journal = {ICOTS9},
  abstract = {Data Games is a data exploration environment in which data generated by playing simple games is used by students to build models with which they can improve their game-playing strategies. The data is structured hierarchically, with games at the upper level containing game events at the lower level. Visualization of these two levels, typically with multiple graphs linked by dynamic selection, reveals patterns and variation among the games and illuminates the role that chance plays in the games. We explore the parallels of Data Games to repeated sampling simulations and in a NetLogo forest fire simulation. We look at uses of hierarchical data visualization techniques in each of these three situations.},
  keywords = {to read},
  annotation = {Published: ICOTS9, http://iase-web.org/icots/9/proceedings/pdfs/ICOTS9\_9B1\_FINZER.pdf}
}

@book{fisher1970,
  title = {Statistical {{Methods}} for {{Research Workers}}},
  author = {Fisher, Ronald A.},
  year = {1970},
  edition = {14},
  publisher = {Macmillan Publishing Co},
  keywords = {statistics}
}

@phdthesis{fitzallen2012,
  title = {Reasoning about Covariation with {{TinkerPlots}}},
  author = {Fitzallen, Noleine},
  year = {2012},
  school = {University of Tasmania}
}

@article{fitzallen2013,
  title = {Characterizing {{Students}}' {{Interaction}} with {{TinkerPlots}}},
  author = {Fitzallen, Noleine},
  year = {2013},
  journal = {Technology Innovations in Statistics Education},
  volume = {7},
  number = {1}
}

@techreport{fitzjohn2014,
  title = {Reproducible Research Is Still a Challenge},
  author = {FitzJohn, Rich and Pennell, Matt and Zanne, Amy and Cornwell, Will},
  year = {2014},
  institution = {rOpenSci}
}

@article{fleischeretal2022,
  title = {Teaching and {{Learning Data-Driven Machien Learning}} with {{Educationally}} Designed {{Jupyter Notebooks}}},
  author = {Fleischer, Yannik and Biehler, Rolf and Schulte, Carsten},
  year = {2022},
  month = jul,
  journal = {Statistics Education Research Journal},
  volume = {21},
  number = {2},
  pages = {7--7},
  issn = {1570-1824},
  doi = {10.52041/serj.v21i2.61},
  urldate = {2022-07-12},
  abstract = {This study examines modelling with machine learning. In the context of a yearlong data science course, the study explores how upper secondary students apply machine learning with Jupyter Notebooks and document the modelling process as a computational essay incorporating the different steps of the CRISP-DM cycle. The students' work is based on a teaching module about decision trees in machine learning and a worked example of such a modelling process. The study outlines the students' performance in carrying out the machine learning technically and reasoning about bias in the data, different data preparation steps, the application context, and the resulting decision model. Furthermore, the context of the study and the theoretical backgrounds are presented.},
  copyright = {Copyright (c) 2022 STATISTICS EDUCATION RESEARCH JOURNAL},
  langid = {english},
  keywords = {Jupyter Notebook},
  file = {/Users/amcnamara/Zotero/storage/KKLJXMLG/Fleischer et al. - 2022 - TEACHING AND LEARNING DATA-DRIVEN MACHINE LEARNING.pdf}
}

@inproceedings{flowers2016,
  title = {{{FiveThirtyEight}}'s Data Journalism Workflow with {{R}}},
  shorttitle = {User2016},
  booktitle = {\{\vphantom\}{{useR}}\vphantom\{\} 2016},
  author = {Flowers, Andrew},
  year = {2016},
  urldate = {2022-03-30},
  abstract = {View more about this event at user2016},
  file = {/Users/amcnamara/Zotero/storage/UUW3VPJI/fivethirtyeights-data-journalism-workflow-with-r.html}
}

@inproceedings{fogg2009,
  title = {A Behavior Model for Persuasive Design},
  booktitle = {Persuasive'09},
  author = {Fogg, B. J.},
  year = {2009},
  abstract = {This paper presents a new model for understanding human behavior. In this model (FBM), behavior is a product of three factors: motivation, ability, and triggers, each of which has subcomponents. The FBM asserts that for a person to perform a target behavior, he or she must (1) be sufficiently motivated, (2) have the ability to perform the behavior, and (3) be triggered to perform the behavior. These three factors must occur at the same moment, else the behavior will not happen. The FBM is useful in analysis and design of persuasive technologies. The FBM also helps teams work together efficiently because this model gives people a shared way of thinking about behavior change.}
}

@book{fordp.2016,
  title = {@ftrain: ``{{There}}'s a Lot to Process Right Now but Don't Think We Won't Be Coming Back to Those {{NYT}} Election Probability Gauges with Random Jitter.''},
  author = {{Ford, P.}},
  year = {2016}
}

@book{forstag2023,
  title = {Foundations of {{Data Science}} for {{Students}} in {{Grades K-12}}: {{Proceedings}} of a {{Workshop}}},
  shorttitle = {Foundations of {{Data Science}} for {{Students}} in {{Grades K-12}}},
  editor = {Forstag, Erin Hammers},
  year = {2023},
  month = apr,
  publisher = {National Academies Press},
  address = {Washington, D.C.},
  doi = {10.17226/26852},
  urldate = {2024-06-27},
  collaborator = {{Board on Science Education} and {Division of Behavioral and Social Sciences and Education} and {National Academies of Sciences, Engineering, and Medicine}},
  isbn = {978-0-309-69815-3},
  keywords = {{Education--Policy, Reviews and Evaluations},Education--K-12 Education,Education--Math and Science Education}
}

@book{fowler2011,
  title = {Domain-Specific Languages},
  author = {Fowler, Martin},
  year = {2011},
  publisher = {Addison-Wesley Publishing Company},
  keywords = {to buy}
}

@inproceedings{fox2004,
  title = {Getting Started with the {{R Commander}}: {{A}} Basic-Statistics Graphical User Interface to {{R}}},
  booktitle = {{{useR}}! {{Conference}}},
  author = {Fox, John},
  year = {2004},
  abstract = {Unlike S-PLUS, R does not incorporate a statistical graphical user interface (GUI), but it does include tools for building GUIs. Based on the tcltk package (which furnishes an interface to the Tcl/Tk GUI builder) the Rcmdr package provides a basic-statistics graphical user interface to R called the ``R Commander.'' The design objectives of the R Commander were as follows: to support, through an easy-to-use, extensible, cross-platform GUI, the statistical functionality required for a basic-statistics course (though its current functionality has grown to include support for linear and generalized-linear models); to make it relatively difficult to do unreasonable things; and to render visible the relationship between choices made in the GUI and the R commands that they generate. The R Commander uses a simple and familiar menu/dialog-box interface. Top-level menus include File, Edit, Data, Statistics, Graphs, Models, Distributions, and Help, with the complete menu tree given in the paper. Each dialog box includes a Help button, which leads to a relevant help page. Menu and dialog-box selections generate R commands, which are recorded in a log/script window and are echoed, along with output, to an output window. The log/script window also provides the ability to edit, enter, and re-execute commands. Data sets in the R Commander are simply R data frames, and can be read from attached packages or imported from files. Although several data frames may reside in memory, only one is ``active'' at any given time. The purpose of this paper is to introduce and describe the basic use of the R Commander GUI and the manner in which it can be extended. Most of the paper can serve as an introductory guide for students who will use the R Commander},
  keywords = {to read}
}

@book{franklin2005,
  title = {Guidelines for Assessment and Instruction in Statistics Education Report: {{K-12}}},
  author = {Franklin, Christine and Kader, Gary and Mewborn, Denise and Moreno, Jerry and Peck, Roxy and Perry, Mike and Schaeffer, Richard},
  year = {2005},
  publisher = {American Statistical Association},
  keywords = {assessment,education,statistics}
}

@article{freedman1983,
  title = {A Nonstochastic Interpretation of Reported Significance Levels},
  author = {Freedman, David and Lane, David},
  year = {1983},
  journal = {Journal of business \& economic statistics},
  volume = {1},
  number = {4},
  pages = {292--298},
  abstract = {Tests of significance are often made in situations where the standard assumptions underlying the probability calculations do not hold. As a result, the reported significance levels become difficult to interpret. This article sketches an alternative interpretation of a reported significance level, valid in considerable generality. This level locates the given data set within the spectrum of other data sets derived from the given one by an appropriate class of transformations. If the null hypothesis being tested holds, the derived data sets should be equivalent to the original one. Thus, a small reported significance level indicates an unusual data set. This development parallels that of randomization tests, but there is a crucial technical difference: our approach involves permuting observed residuals; the classical randomization approach involves permuting unobservable, or perhaps nonexistent, stochastic disturbance terms.},
  keywords = {statistics}
}

@inproceedings{friedman2008,
  title = {Laying the {{Foundations}} for {{Public Participation}} and {{Value Advocacy}}: {{Interaction Design}} for a {{Large Scale Urban Simulation}}},
  booktitle = {Proceedings of the 2008 International Conference on {{Digital}} Government Research},
  author = {Friedman, Batya and Borning, Alan and Davis, Janet L. and Gill, Brian T. and Peter H Kahn, Jr and Kriplean, Travis and Lin, Peyina},
  year = {2008},
  publisher = {Digital Government Society of North America},
  abstract = {Supporting public participation is often a key goal in the design of digital government systems. However, years of work may be required before a complex system, such as the UrbanSim urban simulation system, is deployed and ready for such participation. In this paper, we investigate laying the foundations for public participation in advance of wide-scale public deployment, with the goal of having interaction designs ready when the system is put into such use. Moreover, in a highly politicized domain such as this one, value advocacy as well as factual information plays a central role. Using the theory and methods of Value Sensitive Design, we address three design goals toward public participation and value advocacy, and provide evidence that each of them was achieved: (1) enabling indirect stakeholders to become direct stakeholders (i.e. enabling more people to interact directly with UrbanSim in useful ways); (2) developing a participatory process by which these stakeholders can help guide the development of the system itself; and (3) enabling participating organizations to engage in value advocacy while at the same time enhancing overall system legitimation.}
}

@incollection{friel2008,
  title = {The {{Research Frontier}}: {{Where Technology Interacts}} with the {{Teaching}} and {{Learning}} of {{Data Analysis}} and {{Statistics}}.},
  booktitle = {Research on Technology and the Teaching and Learning of Mathematics},
  author = {Friel, Susan N.},
  editor = {Heid, M. Kathleen and Blume, Glendon W.},
  year = {2008},
  volume = {2},
  publisher = {National Council of Teachers of Mathematics},
  keywords = {education,statistics,technology}
}

@book{friendly,
  title = {Milestones in the History of Thematic Cartography, Statistical Graphics, and Data Visualization},
  author = {Friendly, Michael}
}

@article{friendly1995,
  title = {Conceptual and Visual Models for Categorical Data},
  author = {Friendly, Michael},
  year = {1995},
  journal = {The American Statistician},
  volume = {49},
  number = {2},
  pages = {153--160},
  abstract = {A dynamic conceptual model for categorical data is described that likens observations to gas molecules in a pressure chamber. In this physical model frequency corresponds to pressure, and fitting a statistical model by maximum likelihood corresponds to minimizing energy or balancing of forces. The model provides neat explanations of many results for categorical data, extends readily to multiway tables, and provides a rationale for the graphic representation of counts by area or visual density.}
}

@article{friendly2001,
  title = {A {{Brief History}} of the {{Mosaic Display}}},
  author = {Friendly, Michael},
  year = {2001},
  journal = {Journal of Computational and Graphical Statistics}
}

@article{frischemeieretal2021,
  title = {A First Introduction to Data Science Education in Secondary Schools: {{Teaching}} and Learning about Data Exploration with {{{\textsc{CODAP}}}} Using Survey Data},
  shorttitle = {A First Introduction to Data Science Education in Secondary Schools},
  author = {Frischemeier, Daniel and Biehler, Rolf and Podworny, Susanne and Budde, Lea},
  year = {2021},
  month = jul,
  journal = {Teaching Statistics},
  volume = {43},
  number = {S1},
  issn = {0141-982X, 1467-9639},
  doi = {10.1111/test.12283},
  urldate = {2021-07-27},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/5FM2EM36/Frischemeier et al. - 2021 - A first introduction to data science education in .pdf}
}

@phdthesis{fry2000,
  title = {Organic {{Information Design}}},
  author = {Fry, Benjamin},
  year = {2000},
  month = may,
  abstract = {Design techniques for static information are well understood, their descriptions and discourse thorough and well-evolved. But these techniques fail when dynamic information is considered. There is a space of highly complex systems for which we lack deep understanding because few techniques exist for visualization of data whose structure and content are continually changing. To approach these problems, this thesis introduces a visualization process titled Organic Information Design. The resulting systems employ simulated organic properties in an interactive, visually refined environment to glean qualitative facts from large bodies of quantitative data generated by dynamic information sources.},
  school = {Massachusetts Institute of Technology}
}

@phdthesis{fry2004,
  title = {Computational {{Information Design}}},
  author = {Fry, Benjamin},
  year = {2004},
  month = apr,
  school = {Massachusetts Institute of Technology},
  keywords = {computer science,data visualization,dissertation}
}

@book{fry2008,
  title = {Visualizing {{Data}}},
  author = {Fry, Ben},
  year = {2008},
  publisher = {O'Reilly}
}

@article{gajos2010,
  title = {Automatically Generating Personalized User Interfaces with {{SUPPLE}}},
  author = {Gajos, Krzysztof Z. and Weld, Daniel S. and Wobbrock, Jacob O.},
  year = {2010},
  journal = {Artificial Intelligence},
  volume = {174},
  pages = {910--950}
}

@article{gal-ezer1999,
  title = {Curriculum and {{Course Syllabi}} for a {{High-School Program}} in {{Computer Science}}},
  author = {{Gal-ezer}, Judith and Harel, David},
  year = {1999},
  journal = {Computer Science Education},
  abstract = {The authors served on a committee that designed a high-school curriculum in computer science and has been supervising the preparation of a comprehensive study program based on it. The new program is intended for the Israeli high-school system, and has been formally approved by the Ministry of Education, and is expected to fully replace the old one in the near future. The program emphasizes the foundations of algorithmics, and teaches programming as a way to get the computer to carry out an algorithm. The purpose of this paper is to describe the program's curriculum and syllabi in detail.},
  keywords = {algorithms,computer science,curriculum,education,high-school}
}

@incollection{gal1997,
  title = {The {{Assessment Challenge}} in {{Statistics Education}}},
  author = {Gal, Iddo and Ginsburg, Lynda and Schau, Candace},
  year = {1997},
  publisher = {IOS Press},
  chapter = {Monitoring Attitudes and Beliefs in Statistics Education},
  keywords = {assessment,education,statistics}
}

@article{gallianetal2000,
  title = {Project {{NExT}}},
  author = {Gallian, Joseph A. and Higgins, Aparna and Hudelson, Matt and Jacobsen, Jon and Lefcourt, Tammy and Stevens, T. Christine},
  year = {2000},
  month = feb,
  journal = {Notices of the AMS},
  publisher = {AMS Customer Service, P},
  urldate = {2022-07-21},
  abstract = {This paper describes Project NExT (New Experiences in Teaching), a professional development program for new or recent Ph.D.s in the mathematical sciences. The program consists of workshops and short courses on teaching, establishing a research program, involving undergraduates in research, grant writing, and balancing teaching and research. An introduction to the program and the testimonies of three Project NExT fellows writing about their experiences as new faculty members at research institutes are featured. (MM)},
  langid = {english},
  keywords = {Higher Education,Mathematicians,Mathematics Instruction,Mathematics Teachers,Professional Development,Research Universities},
  file = {/Users/amcnamara/Zotero/storage/4QV7E75J/Gallian et al. - 2000 - Project NExT.pdf;/Users/amcnamara/Zotero/storage/BVL7MR2S/eric-ed-gov.ezproxy.stthomas.edu.html}
}

@misc{gansetal2021,
  title = {{{TidyBlocks}}},
  author = {Gans, Maya and Singh, Justin and Wilson, Greg},
  year = {2021},
  urldate = {2021-07-23},
  file = {/Users/amcnamara/Zotero/storage/JKJKRJVD/tidyblocks.tech.html}
}

@article{gardneretal2021,
  title = {Biology {{Undergraduate Students}}' {{Graphing Practice}} in {{Digital Versus Pen}} and {{Paper Graphing Environments}}},
  author = {Gardner, Stephanie M. and {Suazo-Flores}, Elizabeth and Maruca, Susan and Abraham, Joel K. and Karippadath, Anupriya and Meir, Eli},
  year = {2021},
  month = jun,
  journal = {Journal of Science Education and Technology},
  volume = {30},
  number = {3},
  pages = {431--446},
  issn = {1573-1839},
  doi = {10.1007/s10956-020-09886-w},
  urldate = {2023-02-22},
  abstract = {Graphing is an important practice for scientists and in K-16 science curricula. Graphs can be constructed using an array of software packages as well as by hand, with pen-and-paper. However, we have an incomplete understanding of how students' graphing practice vary by graphing environment; differences could affect how best to teach and assess graphing. Here we explore the role of two graphing environments in students' graphing practice. We studied 43 undergraduate biology students' graphing practice using either pen-and-paper (PP) (n~=~21 students) or a digital graphing tool GraphSmarts (GS) (n~=~22 students). Participants' graphs and verbal justifications were analyzed to identify features such as the variables plotted, number of graphs created, raw data versus summarized data plotted, and graph types (e.g., scatter plot, line graph, or bar graph) as well as participants' reasoning for their graphing choices. Several aspects of participant graphs were similar regardless of graphing environment, including plotting raw vs. summarized data, graph type, and overall graph quality, while GS participants were more likely to plot the most relevant variables. In GS, participants could easily make more graphs than in PP and this may have helped some participants show latent features of their graphing practice. Those students using PP tended to focus more on ease of constructing the graph than GS. This study illuminates how the different characteristics of the graphing environment have implications for instruction and interpretation of assessments of student graphing practices.},
  langid = {english},
  keywords = {Assessment,Improving classroom teaching,Pedagogical issues,Post-secondary education,Teaching/learning strategies},
  file = {/Users/amcnamara/Zotero/storage/GF7SCBSD/Gardner et al. - 2021 - Biology Undergraduate Students’ Graphing Practice .pdf}
}

@article{garfield1997,
  title = {Teaching {{Data Analysis}} to {{Primary Teachers}}},
  author = {Garfield, Joan},
  year = {1997},
  journal = {The Statistics Teacher Network},
  number = {45},
  pages = {1--2},
  keywords = {data,education,statistics}
}

@incollection{garfield2002,
  title = {The {{Teaching}} and {{Learning}} of {{Mathematics}} at the {{University Level}}},
  author = {Garfield, Joan and Chance, Beth and Snell, J. Laurie},
  year = {2002},
  publisher = {Springer},
  chapter = {Technology in college statistics courses}
}

@article{garfield2007,
  title = {How Students Learn Statistics Revisited: A Current Review of Research on Teaching and Learning Statistics},
  author = {Garfield, Joan and {Ben-Zvi}, Dani},
  year = {2007},
  journal = {International Statistical Review},
  volume = {75},
  number = {3},
  pages = {372--396},
  keywords = {education,statistics}
}

@incollection{garfield2007a,
  title = {Using {{Students}}' Informal Notions of Variability to Develop an Understanding of Formal Measures of Variability},
  booktitle = {Thinking with {{Data}}},
  author = {Garfield, Joan and {delMas}, Robert C. and Chance, Beth},
  editor = {Lovett, Marsha C. and Shah, Priti},
  year = {2007},
  publisher = {Lawrence Erlbaum Associates},
  keywords = {education,statistics}
}

@inproceedings{garfieldben-zvi2008,
  title = {Preparing {{School Teachers}} to {{Develop Students}}' {{Statistical Reasoning}}},
  booktitle = {Joint {{ICMI}}/{{IASE Study}}: {{Teaching Statistics}} in {{School Mathematics}}. {{Challenges}} for {{Teaching}} and {{Teacher Education}}. {{Proceedings}} of the {{ICMI Study}} 18 and 2008 {{IASE Round Table Conference}}},
  author = {Garfield, Joan and {Ben-Zvi}, Dani},
  year = {2008},
  publisher = {ICMI/IASE},
  keywords = {education,statistics}
}

@book{garfieldben-zvi2008a,
  title = {Developing Students' Statistical Reasoning: Connecting Research and Teaching Practice},
  shorttitle = {Developing Students' Statistical Reasoning},
  author = {Garfield, J. B. and {Ben-Zvi}, Dani},
  year = {2008},
  publisher = {Springer},
  address = {New York},
  isbn = {978-1-4020-8382-2 978-1-4020-8383-9},
  lccn = {QA276.18 .G37 2008},
  keywords = {Mathematical statistics,Study and teaching},
  annotation = {OCLC: ocn222158919}
}

@article{garfieldetal2015,
  title = {Developing Students' Reasoning about Samples and Sampling Variability as a Path to Expert Statistical Thinking},
  author = {Garfield, Joan and Le, Laura and Zieffler, Andrew and {Ben-Zvi}, Dani},
  year = {2015},
  month = mar,
  journal = {Educational Studies in Mathematics},
  volume = {88},
  number = {3},
  pages = {327--342},
  issn = {0013-1954, 1573-0816},
  doi = {10.1007/s10649-014-9541-7},
  urldate = {2021-10-18},
  langid = {english}
}

@article{garfunkel2011,
  title = {How to Fix Our Math Education},
  author = {Garfunkel, Sol and Mumford, David},
  year = {2011},
  month = aug,
  journal = {The New York Times},
  keywords = {education,mathematics,statistics}
}

@article{garijo2013,
  title = {Quantifying Reproducibility in Computational Biology: {{The}} Case of the {{Tuberculosis}} Drugome},
  author = {Garijo, Daniel and Kinnings, Sarah and Xie, Li and Xie, Lei and Zhang, Yinliang and Bourne, Philip E. and Gil, Yolanda},
  year = {2013},
  journal = {PLOS ONE},
  volume = {8},
  number = {11},
  pages = {e80278}
}

@article{gaudart2015,
  title = {{{SPODT}}: {{An R}} Package to Perform Spatial Partitioning},
  author = {Gaudart, Jean and Graffeo, Nathalie and Coulibaly, Drissa and Barbet, Guillaume and Rebaudet, Stanilas and Dessay, Nadine and Doumbo, Ogobara K. and Giorgi, Roch},
  year = {2015},
  journal = {Journal of Statistical Software},
  volume = {63},
  number = {16},
  keywords = {to read}
}

@article{gehrkeetal2021,
  title = {Statistics Education from a Data-centric Perspective},
  author = {Gehrke, Matthias and Kistler, Tanja and L{\"u}bke, Karsten and Markgraf, Norman and Krol, Bianca and Sauer, Sebastian},
  year = {2021},
  month = jul,
  journal = {Teaching Statistics},
  volume = {43},
  number = {S1},
  issn = {0141-982X, 1467-9639},
  doi = {10.1111/test.12264},
  urldate = {2021-07-27},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/R5RHT7JM/Gehrke et al. - 2021 - Statistics education from a data‐centric perspecti.pdf}
}

@article{gehrkeetal2021a,
  title = {Statistics Education from a Data-Centric Perspective},
  author = {Gehrke, Matthias and Kistler, Tanja and L{\"u}bke, Karsten and Markgraf, Norman and Krol, Bianca and Sauer, Sebastian},
  year = {2021},
  journal = {Teaching Statistics},
  volume = {43},
  number = {S1},
  pages = {S201-S215},
  issn = {1467-9639},
  doi = {10.1111/test.12264},
  urldate = {2023-07-07},
  abstract = {The ubiquitous acquisition and generation of data require a reworking of curricula in introductory statistics in tertiary education. We present a renewed curriculum that focuses on scientific thinking, modeling, and simulation-based inference, utilizing R and various R tools such as shiny and learnr apps. We teach statistics from a data-centric perspective, enabling the students to become data literate. Initial feedback of students and educators at our university of applied sciences shows that students' conceptual understanding improved and they understood the practical applicability of statistics better.},
  copyright = {{\copyright} 2021 The Authors. Teaching Statistics published by John Wiley \& Sons Ltd on behalf of Teaching Statistics Trust.},
  langid = {english},
  keywords = {data literacy,data science,learnr,modeling,Shiny apps,simulation-based inference,teaching,teaching statistics},
  file = {/Users/amcnamara/Zotero/storage/77XQXSQY/Gehrke et al. - 2021 - Statistics education from a data-centric perspecti.pdf;/Users/amcnamara/Zotero/storage/9TPMH5RL/test.html}
}

@article{gelman1999,
  title = {All Maps of Parameter Estimates Are Misleading},
  author = {Gelman, Andrew and Price, Phillip N.},
  year = {1999},
  journal = {Statistics in Medicine},
  volume = {18},
  pages = {3221--3234},
  abstract = {Maps are frequently used to display spatial distributions of parameters of interest, such as cancer rates or average pollutant concentrations by county. It is well known that plotting observed rates can have serious drawbacks when sample sizes vary by area, since very high (and low) observed rates are found disproportionately in poorly-sampled areas. Unfortunately, adjusting the observed rates to account for the effects of small-sample noise can introduce an opposite effect, in which the highest adjusted rates tend to be found disproportionately in well-sampled areas. In either case, the maps can be difficult to interpret because the display of spatial variation in the underlying parameters of interest is confounded with spatial variation in sample sizes. As a result, spatial patterns occur in adjusted rates even if there is no spatial structure in the underlying parameters of interest, and adjusted rates tend to look too uniform in areas with little data. We introduce two models (normal and Poisson) in which parameters of interest have no spatial patterns, and demonstrate the existence of spatial artifacts in inference from these models. We also discuss spatial models and the extent to which they are subject to the same artifacts. We present examples from Bayesian modeling, but, as we explain, the artifacts occur generally.}
}

@article{gelman2004,
  title = {Exploratory {{Data Analysis}} for {{Complex Models}}},
  author = {Gelman, Andrew},
  year = {2004},
  journal = {Journal of Computational and Graphical Statistics},
  volume = {13},
  number = {4},
  pages = {755--779},
  abstract = {``Exploratory'' and ``confirmatory'' data analysis can both be viewed as methods for comparing observed data to what would be obtained under an implicit or explicit statistical model. For example, many of Tukey's methods can be interpreted as checks against hypothetical linear models and Poisson distributions. In more complex situations, Bayesian methods can be useful for constructing reference distributions for various plots that are useful in exploratory data analysis. This article proposes an approach to unify exploratory data analysis with more formal statistical methods based on probability models. These ideas are developed in the context of examples from fields including psychology, medicine, and social science. Key Words: Bayesian inference; Bootstrap; Graphs; Multiple imputation; Posterior predictive checks.}
}

@article{gelman2012,
  title = {Philosophy and the Practice of {{Bayesian}} Statistics},
  author = {Gelman, Andrew and Shalizi, Cosma Rohilla},
  year = {2012},
  journal = {British Journal of Mathematical and Statistical Psychology}
}

@article{gelman2012a,
  title = {Statisticians: {{When We Teach}}, {{We Don}}'t {{Practice What We Preach}}},
  author = {Gelman, Andrew and Loken, Eric},
  year = {2012},
  journal = {Chance},
  volume = {25}
}

@book{gelman2013,
  title = {When Do Stories Work? {{Evidence}} and Illustration in the Social Sciences},
  author = {Gelman, Andrew and Basboll, Thomas},
  year = {2013},
  abstract = {Storytelling has long been recognized as central to human cognition and communication. Here we explore a more active role of stories in social science research, not merely to illustrate concepts but also to develop new ideas and evaluate hypotheses, for example in deciding that a research method is effective. We see stories as central to engagement with the development and evaluation of theories, and we argue that for a story to be useful in this way, it should be anomalous (representing aspects of life that are not well explained by existing models) and immutable (with details that are well- enough established that they have the potential to indicate problems with a new model). We develop these ideas through considering two well-known examples from the work of Karl Weick and Robert Axelrod, and we discuss why transparent sourcing (in the case of Axelrod) makes a story a more effective research tool, whereas plagiarism (in the case of Weick) interferes with the key useful roles of stories in the scientific process.}
}

@article{gentleman2004,
  title = {Some Perspectives on Statistical Computing},
  author = {Gentleman, Robert},
  year = {2004},
  journal = {The Canadian Journal of Statistics},
  volume = {32},
  number = {3},
  pages = {209--226},
  abstract = {The author argues for an increased emphasis on computing in the training of statisticians and in their professional practice. He describes some of the current technological challenges and demonstrates the importance for statisticians of becoming more active in computational aspects of their work and specifically in producing software for carrying out statistical procedures. Such a reorientation will require substantial changes in thinking, pedagogy and infrastructure; the author mentions some of the conditions required to achieve these goals.},
  keywords = {to read}
}

@article{gentleman2007,
  title = {Statistical Analyses and Reproducible Research},
  author = {Gentleman, Robert and Lang, Duncan Temple},
  year = {2007},
  journal = {Journal of Computational and Graphical Statistics},
  volume = {16},
  number = {1},
  pages = {1--23},
  abstract = {It is important, if not essential, to integrate the computations and code used in data analyses, methodological descriptions, simulations, and so on with the documents that describe and rely on them. This integration allows readers to both verify and adapt the claims in the documents. Authors can easily reproduce the results in the future, and they can present the document's contents in a different medium, for example, with interactive controls. This article describes a software framework for both authoring and distributing these integrated, dynamic documents that contain text, code, data, and any auxiliary content needed to recreate the computations. The documents are dynamic in that the contents---including figures, tables, and so on---can be recalculated each time a view of the document is generated. Our model treats a dynamic document as a master or ``source'' document from which one can generate different views in the form of traditional, derived documents for different audiences. We introduce the concept of a compendium as a container for one or more dynamic documents and the different elements needed when processing them, such as code and data. The compendium serves as a means for distributing, managing, and updating the collection. The step from disseminating analyses via a compendium to reproducible research is a small one. By reproducible research, we mean research papers with accompanying software tools that allow the reader to directly reproduce the results and employ the computational methods that are presented in the research paper. Some of the issues involved in paradigms for the production, distribution, and use of such reproducible research are discussed.},
  keywords = {to read}
}

@book{GEOLtips,
  title = {{{GEOL}} 388 {{Excel Tips}}}
}

@book{gerlach1970,
  title = {The National Atlas of the {{United States}} of {{America}}},
  editor = {Gerlach, Arch C.},
  year = {1970},
  publisher = {US Geological Survey}
}

@article{gigerenzerhoffrage1995,
  title = {How to Improve {{Bayesian}} Reasoning without Instruction: Frequency Formats},
  author = {Gigerenzer, Gerd and Hoffrage, Ulrich},
  year = {1995},
  journal = {Psychological review},
  volume = {102},
  number = {4},
  pages = {684--704},
  abstract = {Is the mind, by design, predisposed against performing Bayesian inference? Previous research on base rate neglect suggests that the mind lacks the appropriate cognitive algorithms. However, any claim against the existence of an algorithm, Bayesian or otherwise, is impossible to evaluate unless one specifies the information format in which it is designed to operate. The authors show that Bayesian algorithms are computationally simpler in frequency formats than in the probability formats used in previous research. Frequency formats correspond to the sequential way information is acquired in natural sampling, from animal foraging to neural networks. By analyzing several thousand solutions to Bayesian problems, the authors found that when information was presented in frequency formats, statistically naive participants derived up to 50\% of all inferences by Bayesian algorithms. Non-Bayesian algorithms included simple versions of Fisherian and Neyman-Pearsonian inference.},
  keywords = {statistical thinking,statistics}
}

@article{gilgibbs2017,
  title = {{{PROMOTING MODELING AND COVARIATIONAL REASONING AMONG SECONDARY SCHOOL STUDENTS IN THE CONTEXT OF BIG DATA}}},
  author = {Gil, Einat and Gibbs, Alison L.},
  year = {2017},
  journal = {STATISTICS EDUCATION RESEARCH JOURNAL},
  volume = {16},
  number = {2},
  pages = {163--190},
  issn = {1570-1824},
  doi = {10.52041/serj.v16i2.189},
  urldate = {2023-07-07},
  abstract = {In this study, we follow students' modeling and covariational reasoning in the context of learning about big data. A three-week unit was designed to allow 12th grade students in a mathematics course to explore big and mid-size data using concepts such as trend and scatter to describe the relationships between variables in multivariate settings. Students' emergent ideas were followed along a varied learning trajectory that included computer-supported collaborative and inquiry-based approaches, using visualization tools and statistical software to explore data and fit a suitable trend, and student presentations of investigations. Findings show progress in some components of students' reasoning and modeling of covariation, and indicate which features of the unit design might contribute to it. First published November 2017 at Statistics Education Research Journal Archives},
  copyright = {Copyright (c) 2021 STATISTICS EDUCATION RESEARCH JOURNAL},
  langid = {english},
  keywords = {Representational gestures},
  file = {/Users/amcnamara/Zotero/storage/PTBMCSJG/Gil and Gibbs - 2017 - PROMOTING MODELING AND COVARIATIONAL REASONING AMO.pdf}
}

@article{gillet2012,
  title = {Comparing Volunteer and Professionally Collected Monitoring Data from the Rocky Sub Tidal Reefs of {{Southern California}}, {{USA}}},
  author = {Gillet, David J. and II, Daniel J. Pondella and Freiwald, Jan and Schiff, Kenneth C. and Caselle, Jennifer E. and Shuman, Craig and Weisberg, Stephen B.},
  year = {2012},
  journal = {Environmental Monitoring and Assessment},
  abstract = {Volunteer-based citizen monitoring has increasingly become part of the natural resources monitoring framework, but it is often unclear whether the data quality from these programs is sufficient for integration with traditional efforts conducted by professional scientists. At present, the biological and physical characteristics of California's rocky reef kelp forests are concurrently monitored by two such groups, using similar methodologies---underwater visual census (UVC) of fish, benthic invertebrates, and reef habitat,though the volunteer group limits their sampling to transects close to the reef surface and they use a more constrained list of species for enumeration and measurement. Here, we compared the data collected from 13 reefs that were sampled by both programs in 2008. These groups described relatively similar fish communities, total fish abundance and abundance of the dominant fish species but there were some differences in the measured size distributions of the dominant fish species. Descriptions of the benthic invertebrate community were also similar, though there were some differences in relative abundance that may have resulted from the less detailed subsampling protocols used by the volunteers. The biggest difference was in characterization of the physical habitat of the reefs, which appeared to result from selection bias of transect path by the volunteer program towards more complex structured sections of a reef. Changes to address these differences are relatively simple to implement and if so, offer the promise of better integration of the trained volunteer monitoring with that of professional monitoring groups.}
}

@inproceedings{gjekmarkajetal2017,
  title = {Exploring {{MAUP}} with {{Flint Water Data}}},
  booktitle = {Hudson {{River Undergraduate Math Conference}}},
  author = {Gjekmarkaj, Eva and Liu, Junzhou and Niyonzima, Yvonne and Stephen, Carolyn and Li, Zixian},
  year = {2017}
}

@article{glantzetal2023,
  title = {Students' {{Experience}} and {{Perspective}} of a {{Data Science Program}} in a {{Two-Year College}}},
  author = {Glantz, Mary and Johnson, Jennifer and Macy, Marilyn and Nunez, Juan J. and Saidi, Rachel and Velez, Camilo},
  year = {2023},
  month = apr,
  journal = {Journal of Statistics and Data Science Education},
  volume = {0},
  number = {0},
  pages = {1--10},
  publisher = {Taylor \& Francis},
  issn = {null},
  doi = {10.1080/26939169.2023.2208185},
  urldate = {2023-07-14},
  abstract = {Two-year colleges provide the opportunity for students of all ages to try new subjects, change careers, upskill, or begin exploring higher education, at affordable rates. Many might begin their exploration by taking a course at a local two-year college. Currently, not many of these institutions in the U.S. offer data science courses. This article introduces the perspective lens of students who have gone through the Montgomery College Data Science Certificate Program. We found that, contrary to many other educational fields at the College, data science students tend to come from diverse backgrounds and career paths. A common theme emerged that all students learned valuable skills and applications such as coding in various programming languages and approaches to machine learning. Other meaningful themes included an appreciation of course accessibility, especially catered toward busy professionals who might only be able to take evening courses. Students appreciated learning that data science and ethics are intertwined. Finally, it was evident that going through the data science program positively impacted the lives and careers of these students. The implications of the themes of these student experiences are discussed as they relate to data science education. Supplementary materials for this article are available online.},
  keywords = {Certificate,Data science,Diversity,Montgomery college,Student experience,Two-year community college},
  file = {/Users/amcnamara/Zotero/storage/BY5LPSTL/Glantz et al. - 2023 - Students’ Experience and Perspective of a Data Sci.pdf}
}

@article{glassetal1972,
  title = {Consequences of {{Failure}} to {{Meet Assumptions Underlying}} the {{Fixed Effects Analyses}} of {{Variance}} and {{Covariance}}},
  author = {Glass, Gene V and Peckham, Percy D. and Sanders, James R.},
  year = {1972},
  month = sep,
  journal = {Review of Educational Research},
  volume = {42},
  number = {3},
  pages = {237--288},
  publisher = {American Educational Research Association},
  issn = {0034-6543},
  doi = {10.3102/00346543042003237},
  urldate = {2022-01-06},
  langid = {english}
}

@article{godfrey2013,
  title = {Statistical Software from a Blind Person's Perspective},
  author = {Godfrey, A. Jonathan R.},
  year = {2013},
  journal = {The R Journal},
  volume = {5},
  number = {1},
  pages = {73--79},
  abstract = {Abstract Blind people have experienced access issues to many software applications since the advent of the Windows operating system; statistical software has proven to follow the rule and not be an exception. The ability to use R within minutes of download with next to no adaptation has opened doors for accessible production of statistical analyses for this author (himself blind) and blind students around the world. This article shows how little is required to make R the most accessible statistical software available today. There is any number of ramifications that this opportunity creates for blind students, especially in terms of their future research and employment prospects. There is potential for making R even better for blind users. The extensibility of R makes this possible through added functionality being made available in an add-on package called BrailleR. Functions in this package are intended to make graphical information available in text form.}
}

@book{godfrey2019,
  title = {Statistical {{Software}} and {{Blind Users}}},
  author = {Godfrey, Jonathan},
  year = {2019},
  month = sep
}

@book{golub1996,
  title = {Matrix {{Computations}}},
  author = {Golub, Gene H. and Loan, Charles F. Van},
  year = {1996},
  publisher = {The Johns Hopkins University Press},
  keywords = {to buy}
}

@article{goode2007,
  title = {If You Build Teachers, Will Students Come? {{The}} Role of Teachers in Broadening Computer Science Learning for Urban Youth},
  author = {Goode, Joanna},
  year = {2007},
  journal = {Journal of Educational Computing Research},
  volume = {36},
  number = {1},
  pages = {65--88},
  abstract = {Despite the digital saturation of today's youth across demographic groups, students of color and females remain severely underrepresented in computer science. Reporting on a sequential mixed methods study, this article explores the ways that high school computer science teachers can act as change agents to broaden the participation in computing for historically underrepresented students. Three high school case studies reveal a critical need for professional development and support to do this work. The subsequent part of the study focuses on the impact of a district-university intervention which trained 25 urban teachers to teach Advanced Placement computer science in their schools. The swift success of this intervention was evident from the following years' dramatic increase in course offerings and enrollment of females, Latinos, and African Americans.},
  keywords = {to read}
}

@techreport{goode2013,
  title = {Exploring {{Computer Science Curriculum}}},
  author = {Goode, Joanna and Chapman, Gail},
  year = {2013},
  institution = {Exploring Computer Science}
}

@incollection{goodman2009,
  title = {The {{Forth Paradigm}}: {{Data-Intensive Scientific Discovery}}},
  author = {Goodman, Alyssa A. and Wong, Curtis G.},
  year = {2009},
  publisher = {Microsoft},
  chapter = {Bringing the Night Sky Closer: Discoveries in the Data Deluge}
}

@book{google2011,
  title = {Exploring Computational Thinking},
  author = {{Google}},
  year = {2011},
  keywords = {computational thinking,computer science,education}
}

@book{goovaerts,
  title = {Geostatistics in Practice},
  author = {Goovaerts, Pierre},
  keywords = {to read}
}

@article{gould2004,
  title = {Preparing Secondary Mathematics Educators to Teach Statistics},
  author = {Gould, Robert and Peck, Roxy},
  year = {2004},
  journal = {Curricular development of statistics education},
  pages = {244--255},
  abstract = {In this paper, we address two Roundtable topics: distance education and developing teachers' statistical knowledge. We describe a new professional development program for secondary school mathematics teachers who are preparing to teach statistics. We also discuss what we have learned in our efforts to design a course that has a significant online component and that is relevant and useful from a teacher's perspective. We describe the ways in which our online environment incorporates group work, self-study, concept exploration, and assessments. We also discuss the challenges associated with delivering the necessary content while simultaneously accommodating the practical time constraints of adult students who are, themselves, teaching full-time.},
  keywords = {education,mathematics,statistics}
}

@article{gould2010,
  title = {Statistics and the {{Modern Student}}},
  author = {Gould, Robert},
  year = {2010},
  journal = {International Statistical Review},
  volume = {78},
  number = {2},
  pages = {297--315},
  abstract = {The introductory statistics course has traditionally targeted consumers of statistics with the intent of producing a citizenry capable of a critical analysis of basic published statistics. More recently, statistics educators have attempted to centre the intro course on real data, in part to motivate students and in part to create a more relevant course. The success of this approach is predicated on providing data that the students see as real and relevant. Modern students, however, have a different view of data than did students of 10 or even 5 years ago. Modern statistics courses must adjust to the fact that students' first exposure to data occurs outside the academy.}
}

@unpublished{gould2015a,
  title = {Modeling with ``{{Big Data}}''},
  author = {Gould, Robert and Johnson, Terri and {Moncada-Machado}, Suyen and Molyneux, James},
  year = {2015}
}

@article{gould2016,
  title = {Data Literacy Is Statistical Literacy},
  author = {Gould, Robert},
  year = {2016},
  journal = {Statistics Education Research Journal},
  volume = {16},
  number = {1},
  pages = {22--25}
}

@inproceedings{gould2016a,
  title = {Teaching Data Science to Secondary Students: {{The Mobilize Introduction}} to {{Data Science Curriculum}}},
  booktitle = {Proceedings of the {{Roundtable Conference}} of the {{International Association}} of {{Statistics Education}}},
  author = {Gould, Robert and {Moncada-Machado}, Suyen and Ong, Christine and Johnson, Terri and Molyneux, James and Nolen, Steve and Tangmunarunkit, Hongsuda and Trusela, LeeAnn and Zanontian, Linda},
  editor = {Engel, J.},
  year = {2016}
}

@article{gould2021,
  title = {Toward Data-scientific Thinking},
  shorttitle = {Toward},
  author = {Gould, Robert},
  year = {2021},
  month = jul,
  journal = {Teaching Statistics},
  volume = {43},
  number = {S1},
  issn = {0141-982X, 1467-9639},
  doi = {10.1111/test.12267},
  urldate = {2021-07-20},
  langid = {english}
}

@book{gouldetal2015,
  title = {Introduction to {{Data Science}}},
  author = {Gould, Robert and Johnson, Terri and McNamara, Amelia and Molyneux, James and {Moncada-Machado}, Suyen},
  year = {2015},
  publisher = {{Mobilize: Mobilizing for Innovative Computer Science Teaching and Learning}}
}

@mastersthesis{graf2014,
  title = {Direct Manipulation of Turtle Graphics},
  author = {Graf, Matthias},
  year = {2014},
  month = sep,
  abstract = {This thesis is centered around the question of how dynamic pictures can be created and manipulated directly, analogous to drawing images, in an attempt to overcome traditional abstract textual program representations and interfaces (coding). To explore new ideas, Vogo1 is presented, an experimental, spatially-oriented, direct manipulation, live programming environment for Logo Turtle Graphics. It allows complex abstract shapes to be created entirely on a canvas. The interplay of several interface design principles is demonstrated to encourage exploration, curiosity and serendipitous discoveries. By reaching out to new programmers, this thesis seeks to question established programming paradigms and expand the view of what programming is.},
  school = {Universit{\"a}t Magdeburg},
  keywords = {to read}
}

@article{gramazioetal2017,
  title = {Colorgorical: {{Creating Discriminable}} and {{Preferable Color Palettes}} for {{Information Visualization}}},
  shorttitle = {Colorgorical},
  author = {Gramazio, Connor C. and Laidlaw, David H. and Schloss, Karen B.},
  year = {2017},
  month = jan,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {23},
  number = {1},
  pages = {521--530},
  issn = {1077-2626},
  doi = {10.1109/TVCG.2016.2598918},
  urldate = {2021-10-27}
}

@phdthesis{grammel2012,
  title = {User {{Interfaces Supporting Information Visualization Novices}} in {{Visualization Construction}}},
  author = {Grammel, Lars},
  year = {2012},
  abstract = {The amount of data that is available to us is ever increasing, and thus is the potential to extract information from it. Information visualization, which leverages our perceptual system to enable us to perceive patterns, outliers, trends and anomalies in large amounts of data, is an important technique for exploratory data analysis. As part of a flexible visual data analysis process, the user needs to construct and parametrize visualizations, which is challenging for novice users. In this thesis, I explore how information visualization novices can be supported in visualization construction. First, I identify existing visualization construction approaches in a systematic literature survey and examine their use cases. Second, I conduct a laboratory study to learn about the process and the characteristics of how information visualization novices construct visualization during data analysis. Third, I identify natural language visualization queries as a promising alternative specification approach that I study by analyzing the queries from the laboratory experiment and by conducting an online survey study. Based on my findings, I propose a descriptive model of natural language visualization queries. Fourth, I derive guidelines for visualization construction tools from my studies and from related work. Finally, I show how these guidelines can be applied to existing visualization tools using the example of the Choosel visualization framework.},
  school = {University of Victoria}
}

@inproceedings{greenberg2008,
  title = {Usability {{Evaluation Considered Harmful}} (Some of the Time)},
  booktitle = {{{CHI}} 2008 {{Proceedings}}},
  author = {Greenberg, Saul and Buxton, Bill},
  year = {2008},
  abstract = {Current practice in Human Computer Interaction as encouraged by educational institutes, academic review processes, and institutions with usability groups advocate usability evaluation as a critical part of every design process. This is for good reason: usability evaluation has a significant role to play when conditions warrant it. Yet evaluation can be ineffective and even harmful if naively done `by rule' rather than `by thought'. If done during early stage design, it can mute creative ideas that do not conform to current interface norms. If done to test radical innovations, the many interface issues that would likely arise from an immature technology can quash what could have been an inspired vision. If done to validate an academic prototype, it may incorrectly suggest a design's scientific worthiness rather than offer a meaningful critique of how it would be adopted and used in everyday practice. If done without regard to how cultures adopt technology over time, then today's reluctant reactions by users will forestall tomorrow's eager acceptance. The choice of evaluation methodology -- if any -- must arise from and be appropriate for the actual problem or research question under consideration.}
}

@article{grim1936,
  title = {A {{Technique}} for the {{Measurement}} of {{Attitudes}} in the {{Social Studies}}},
  author = {Grim, Paul R.},
  year = {1936},
  journal = {Educational Research Bulletin},
  volume = {15},
  number = {4},
  eprint = {1471067},
  eprinttype = {jstor},
  pages = {95--104},
  publisher = {Taylor \& Francis, Ltd.},
  issn = {1555-4023},
  urldate = {2021-10-01}
}

@book{groeger2014,
  title = {Wee {{Things}}},
  author = {Groeger, Lena},
  year = {2014}
}

@article{grolemund2011,
  title = {Dates and Times Made Easy with Lubridate},
  author = {Grolemund, Garrett and Wickham, Hadley},
  year = {2011},
  journal = {Journal of Statistical Software},
  volume = {40},
  number = {3}
}

@techreport{grolemund2013,
  title = {Visualizing Complex Data with Embedded Plots},
  author = {Grolemund, Garrett and Wickham, Hadley},
  year = {2013},
  institution = {Department of Statistics, Rice University},
  abstract = {This paper describes a class of graphs, embedded plots, that are particularly useful for analyzing large and complex data sets. Embedded plots organize a collection of graphs into a larger graphic. This arrangement allows for more complex relationships to be visualized within a static graphs than would otherwise be possible. Embedded plots provide additional axes, prevent overplotting, provide multiple levels of summarization, and facilitate understanding. Complex data overwhelms the human cognitive system, which prevents comprehension. Embedded plots preprocess complex data into a form more suitable for the human cognitive system through visualization, isolation, and automation. We illustrate the usefulness of embedded plots with a case study, discuss the practical and cognitive advantages of embedded plots, and demonstrate how to implement embedded plots as a general class within visualization software, something currently unavailable.},
  keywords = {to read}
}

@article{guo2014,
  title = {Python Is {{Now}} the {{Most Popular Introductory Teaching Language}} at {{Top U}}.{{S}}. {{Universities}}},
  author = {Guo, Philip},
  year = {2014},
  journal = {Blog@ACM}
}

@inproceedings{guoetal2014,
  title = {How {{Video Production Affects Student Engagement}}: {{An Empirical Study}} of {{MOOC Videos}}},
  shorttitle = {How {{Video Production Affects Student Engagement}}},
  booktitle = {Proceedings of the First {{ACM}} Conference on {{Learning}} @ Scale Conference},
  author = {Guo, Philip J. and Kim, Juho and Rubin, Rob},
  year = {2014},
  month = mar,
  pages = {41--50},
  publisher = {ACM},
  address = {Atlanta Georgia USA},
  doi = {10.1145/2556325.2566239},
  urldate = {2021-11-05},
  isbn = {978-1-4503-2669-8},
  langid = {english}
}

@incollection{guzdial2004,
  title = {{{CS Education Research}}},
  author = {Guzdial, Mark},
  editor = {Fincher, Sally and Petre, Marian},
  year = {2004},
  pages = {127--155},
  publisher = {Taylor \& Francis},
  chapter = {Programming Environments for Novices},
  keywords = {computer science,education}
}

@article{guzdialadams2021,
  title = {Disputing {{Dijkstra}}, and Birthdays in Base 2},
  author = {Guzdial, Mark and Adams, Joel C.},
  year = {2021},
  month = mar,
  journal = {Communications of the ACM},
  volume = {64},
  number = {3},
  pages = {12--13},
  issn = {0001-0782, 1557-7317},
  doi = {10.1145/3446806},
  urldate = {2022-02-14},
  abstract = {The               Communications               Web site, http://cacm.acm.org, features more than a dozen bloggers in the BLOG@CACM community. In each issue of               Communications               , we'll publish selected posts or excerpts.                                         twitter               Follow us on Twitter at http://twitter.com/blogCACM                          http://cacm.acm.org/blogs/blog-cacm             Mark Guzdial takes issue with Dijkstra's metaphors, while Joel C. Adams considers how birthdays might differ if based on binary numbers.},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/BPKJBYPH/Guzdial and Adams - 2021 - Disputing Dijkstra, and birthdays in base 2.pdf}
}

@book{haghish2014,
  title = {{{MarkDoc}}: {{Literate}} Programming in {{Stata}}},
  author = {Haghish, E. F.},
  year = {2014}
}

@inproceedings{hall2008,
  title = {Using {{Census}} at {{School}} and {{Tinkerplots}} to Support {{Ontario}} Elementary Teachers' Statistics Teaching and Learning},
  booktitle = {Joint {{ICMI}}/{{IASE Study}}: {{Teaching Statistics}} in {{School Mathematics}}. {{Challenges}} for {{Teaching}} and {{Teacher Education}}. {{Proceedings}} of the {{ICMI Study}} 18 and 2008 {{IASE Round Table Conference}}},
  author = {Hall, Jennifer},
  year = {2008},
  publisher = {ICMI/IASE},
  keywords = {computer science,education,statistics,technology}
}

@incollection{hall2011,
  title = {Teaching {{Statistics}} in {{School Mathematics- Challenges}} for {{Teaching}} and {{Teacher Education}}},
  author = {Hall, Jennifer},
  year = {2011},
  publisher = {Springer Science + Business Media},
  abstract = {Using real data in statistics education provides significant benefits for both teachers and students. In this chapter, considerations faced by teachers when using real data are explored with regard to student engagement and learning, as well as potential pedagogical issues. To address these issues, suggestions for obtaining and using real data are offered, as is the description of a successful example of a primary data collection project -- Census at School. The chapter concludes by considering the use of real data in the training of teachers, and extends the example of Census at School to explore Canadian professional development workshops for elementary teachers.},
  chapter = {Engaging Teachers and Students with Real Data: Benefits and Challenges}
}

@article{halmos1970,
  title = {How to Write Mathematics},
  author = {Halmos, P. R.},
  year = {1970},
  journal = {L'Enseignement Math{\'e}matique},
  volume = {16}
}

@inproceedings{hammerman2006,
  title = {Saying the Same (or a Different) Thing: {{How}} Shape Affects Ideas about Distribution in a Software Exploration Environment},
  booktitle = {{{ICOTS-7}}},
  author = {Hammerman, James K. and Rubin, Andee},
  year = {2006},
  abstract = {Educational software for statistics and data analysis provides a variety of tools for seeing and expressing ideas about data distributions. However, the ideas that learners find important to express often depend on an interaction between software and the shape of the distributions themselves. In this interview study of teachers participating in the VISOR professional development program, we investigate how distributional shape (symmetric or skewed) and choice of software tool (TinkerPlot or Fathom) affect how teachers discuss data distributions when comparing groups. We find teachers' confidence is increased when different measures or ways of viewing data ``say the same thing,'' which more often holds true with symmetric distributions. When these seem to conflict, typically with skew distributions, teachers work to understand the measures themselves, and introduce new ways of characterizing data, so that they can make coherent sense of the distributions. The paper introduces a distinction between rule-driven and value-driven measures which we find important in understanding teachers' analytic methods.},
  keywords = {to read}
}

@inproceedings{hammermanrubin2003,
  title = {Reasoning in the Presence of Variability},
  booktitle = {The Third International Research Forum on Statistical Reasoning, Thinking and Literacy},
  author = {Hammerman, James K. and Rubin, Andee},
  editor = {Lee, Carl},
  year = {2003},
  abstract = {This paper describes several ways that learners use new software tools to reduce cognitive complexity in analyzing data, and suggests the beginnings of a framework for understanding these techniques. The approaches we address include normalizing differences in group size by using proportions; reducing the detail of data variability by grouping it using numerical bins or cut points; and attending to general trends in data while ignoring variability around those trends. This framework is built on our observations of middle- and high-school teachers in a professional development seminar, as well as of students in these teachers' classrooms and in a 13-week sixth grade teaching experiment.},
  keywords = {statistical literacy,statistical thinking,statistics}
}

@article{hammermanrubin2004,
  title = {Strategies for {{Managing Statistical Complexity}} with {{New Software Tools}}},
  author = {Hammerman, James K. and Rubin, Andee},
  year = {2004},
  month = nov,
  journal = {Statistics Education Research Journal},
  volume = {3},
  number = {2},
  pages = {12--41},
  keywords = {binning,covariation,education,group comparison,proportional reasoning,representations,software tools,statistics,technology,variability}
}

@incollection{hancock1995,
  title = {Computers and Exploratory Learning},
  author = {Hancock, Chris},
  year = {1995},
  publisher = {Springer},
  chapter = {The medium and the curriculum: Reflections on transparent tools and tacit mathematics},
  keywords = {to read}
}

@article{hancocketal1992,
  title = {Authentic {{Inquiry}} with {{Data}}: {{Critical Barriers}} to {{Classroom Implementation}}},
  author = {Hancock, Chris and Kaput, James J. and Goldsmith, Lynn T.},
  year = {1992},
  journal = {Educational Psychologist},
  volume = {27},
  number = {3},
  pages = {337--364}
}

@inproceedings{hanenberg2010,
  title = {Faith, {{Hope}}, and {{Love}}: {{An Essay}} on {{Software Science}}'s {{Neglect}} of {{Human Factors}}},
  shorttitle = {Faith, Hope, and Love},
  booktitle = {Proceedings of the {{ACM}} International Conference on {{Object}} Oriented Programming Systems Languages and Applications - {{OOPSLA}} '10},
  author = {Hanenberg, Stefan},
  year = {2010},
  pages = {933},
  publisher = {ACM Press},
  address = {Reno/Tahoe, Nevada, USA},
  doi = {10.1145/1869459.1869536},
  urldate = {2021-06-15},
  isbn = {978-1-4503-0203-6},
  langid = {english}
}

@article{hanna-attisha2016,
  title = {Elevated {{Blood Lead Levels}} in {{Children Associated With}} the {{Flint Drinking Water Crisis}}: {{A Spatial Analysis}} of {{Risk}} and {{Public Health Response}}},
  author = {{Hanna-Attisha}, Mona and LaChance, Jenny and Sadler, Richard Casey and Schnepp, Allison Champney},
  year = {2016},
  journal = {American Journal of Public Health},
  volume = {106},
  number = {2},
  pages = {283--290}
}

@techreport{hansen,
  title = {Data {{IGERT}}},
  author = {Hansen, Mark and Blanchette, Jean-Fran{\c c}ois and Estrin, Deborah and Gillespie, Tomas},
  institution = {University of California, Los Angeles}
}

@article{hardinetal2015,
  title = {Data {{Science}} in {{Statistics Curricula}}: {{Preparing Students}} to ``{{Think}} with {{Data}}''},
  shorttitle = {Data {{Science}} in {{Statistics Curricula}}},
  author = {Hardin, J. and Hoerl, R. and Horton, Nicholas J. and Nolan, D. and Baumer, B. and {Hall-Holt}, O. and Murrell, P. and Peng, R. and Roback, P. and Temple Lang, D. and Ward, M. D.},
  year = {2015},
  month = oct,
  journal = {The American Statistician},
  volume = {69},
  number = {4},
  pages = {343--353},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2015.1077729},
  urldate = {2023-01-10},
  abstract = {A growing number of students are completing undergraduate degrees in statistics and entering the workforce as data analysts. In these positions, they are expected to understand how to use databases and other data warehouses, scrape data from Internet sources, program solutions to complex problems in multiple languages, and think algorithmically as well as statistically. These data science topics have not traditionally been a major component of undergraduate programs in statistics. Consequently, a curricular shift is needed to address additional learning outcomes. The goal of this article is to motivate the importance of data science proficiency and to provide examples and resources for instructors to implement data science in their own statistics curricula. We provide case studies from seven institutions. These varied approaches to teaching data science demonstrate curricular innovations to address new needs. Also included here are examples of assignments designed for courses that foster engagement of undergraduates with data and data science.[Received November 2014. Revised July 2015.]},
  keywords = {Big data; Computational statistics; Statistical practice; Statistics education},
  file = {/Users/amcnamara/Zotero/storage/PTTPB4WX/Hardin et al. - 2015 - Data Science in Statistics Curricula Preparing St.pdf}
}

@article{hardinetal2021,
  title = {Computing in the {{Statistics Curricula}}: {{A}} 10-{{Year Retrospective}}},
  shorttitle = {Computing in the {{Statistics Curricula}}},
  author = {Hardin, Johanna and Horton, Nicholas J. and Nolan, Deborah and Lang, Duncan Temple},
  year = {2021},
  month = jan,
  journal = {Journal of Statistics and Data Science Education},
  volume = {29},
  number = {sup1},
  pages = {S4-S6},
  publisher = {Taylor \& Francis},
  issn = {null},
  doi = {10.1080/10691898.2020.1862609},
  urldate = {2022-06-07},
  abstract = {The Journal of Statistics and Data Science Education special issue on ``Computing in the Statistics and Data Science Curriculum'' features a set of papers that provide a mosaic of curricular innovations and approaches that embrace computing. As we reviewed the papers we felt that this collection would benefit from the perspective of the authors of the landmark ``Computing in the Statistics Curricula'' (TAS 2010) paper. We asked Deb and Duncan to take this opportunity to reflect on the landscape when they wrote the paper, to comment on the current situation, and to speculate on the future.Johanna Hardin and Nicholas J. Horton},
  file = {/Users/amcnamara/Zotero/storage/JXTWE7HC/Hardin et al. - 2021 - Computing in the Statistics Curricula A 10-Year R.pdf;/Users/amcnamara/Zotero/storage/Q6CHKH3B/10691898.2020.html}
}

@article{hardyetal2020,
  title = {From {{Data Collectors}} to {{Data Producers}}: {{Shifting Students}}' {{Relationship}} to {{Data}}},
  shorttitle = {From {{Data Collectors}} to {{Data Producers}}},
  author = {Hardy, Lisa and Dixon, Colin and Hsi, Sherry},
  year = {2020},
  month = jan,
  journal = {Journal of the Learning Sciences},
  volume = {29},
  number = {1},
  pages = {104--126},
  publisher = {Routledge},
  issn = {1050-8406},
  doi = {10.1080/10508406.2019.1678164},
  urldate = {2023-07-17},
  abstract = {This paper contributes a theoretical framework informed by historical, philosophical and ethnographic studies of science practice to argue that data should be considered to be actively produced, rather than passively collected. We further argue that traditional school science laboratory investigations misconstrue the nature of data and overly constrain student agency in their production. We use our ``Data Production'' framework to analyze activity of and interviews with high school students who created data using sensors and software in a ninth-grade integrated science class. To understand the opportunities for students to develop act with and perceive agency in data production, we analyze in detail the case of one student as she came to use unfamiliar technologies to produce data for her own personally relevant purposes. We find that her purposes for producing data emerged as she worked, and that resistances to her purposes were opportunities to act with and perceive her own agency, and to see data in new ways. We discuss implications for designing science learning experiences in which students act as agents in producing and using data.}
}

@article{harrison2014,
  title = {Ranking Visualizations of Correlation Using {{Weber}}'s Law},
  author = {Harrison, Lane and Yang, Fuming and Franconeri, Steven and Chang, Remco},
  year = {2014},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {20},
  number = {12},
  pages = {1943--1952},
  abstract = {Despite years of research yielding systems and guidelines to aid visualization design, practitioners still face the challenge of identifying the best visualization for a given dataset and task. One promising approach to circumvent this problem is to leverage perceptual laws to quantitatively evaluate the effectiveness of a visualization design. Following previously established methodologies, we conduct a large scale (n=1687) crowdsourced experiment to investigate whether the perception of correlation in nine commonly used visualizations can be modeled using Weber's law. The results of this experiment contribute to our understanding of information visualization by establishing that: 1) for all tested visualizations, the precision of correlation judgment could be modeled by Weber's law, 2) correlation judgment precision showed striking variation between negatively and positively correlated data, and 3) Weber models provide a concise means to quantify, compare, and rank the perceptual precision afforded by a visualization.},
  keywords = {to read}
}

@techreport{harrispoll2014,
  title = {2014 {{Wells Fargo Millennial Study}}},
  author = {{Harris Poll}},
  year = {2014},
  institution = {Wells Fargo}
}

@article{harrowerbrewer2003,
  title = {{{ColorBrewer}}.Org: {{An Online Tool}} for {{Selecting Colour Schemes}} for {{Maps}}},
  shorttitle = {{{ColorBrewer}}.Org},
  author = {Harrower, Mark and Brewer, Cynthia A.},
  year = {2003},
  month = jun,
  journal = {The Cartographic Journal},
  volume = {40},
  number = {1},
  pages = {27--37},
  publisher = {Taylor \& Francis},
  issn = {0008-7041},
  doi = {10.1179/000870403235002042},
  urldate = {2021-10-27},
  abstract = {Choosing effective colour schemes for thematic maps is surprisingly difficult. ColorBrewer is an online tool designed to take some of the guesswork out of this process by helping users select appropriate colour schemes for their specific mapping needs by considering: the number of data classes; the nature of their data (matched with sequential, diverging and qualitative schemes); and the end-use environment for the map (e.g., CRT, LCD, printed, projected, photocopied). ColorBrewer contains 'learn more' tutorials to help guide users, prompts them to test-drive colour schemes as both map and legend, and provides output in five colour specification systems.},
  file = {/Users/amcnamara/Zotero/storage/4FW3CTEC/000870403235002042.html}
}

@inproceedings{hazzan2008,
  title = {A {{Model}} for {{High School Computer Science Education}}: {{The Four Key Elements}} That {{Make It}}!},
  booktitle = {{{SIGCSE}}'08},
  author = {Hazzan, Orit and {Gal-ezer}, Judith and Blum, Lenore},
  year = {2008},
  abstract = {This paper presents a model program for high school computer science education. It is based on an analysis of the structure of the Israeli high school computer science curriculum considered to be one of the leading curricula worldwide. The model consists of four key elements as well as interconnections between these elements. It is proposed that such a model be considered and/or adapted when a country wishes to implement a nation-wide program for high school computer science education.}
}

@article{head2015,
  title = {The Extent and Consequences of P-Hacking in Science},
  author = {Head, Megan L. and Holman, Luke and Lancer, Rob and Kahn, Andrew T. and Jennions, Michael D.},
  year = {2015},
  journal = {PLoS Biology},
  volume = {13},
  number = {3}
}

@article{heer2008,
  title = {Graphical {{Histories}} for {{Visualization}}: {{Supporting Analysis}}, {{Communication}} and {{Evaluation}}},
  author = {Heer, Jeffrey and Mackinlay, Jock D. and Stolte, Chris and Agrawala, Maneesh},
  year = {2008},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {14},
  number = {6},
  pages = {1189--1196},
  abstract = {Interactive history tools, ranging from basic undo and redo to branching timelines of user actions, facilitate iterative forms of interaction. In this paper, we investigate the design of history mechanisms for information visualization. We present a design space analysis of both architectural and interface issues, identifying design decisions and associated trade-offs. Based on this analysis, we contribute a design study of graphical history tools for Tableau, a database visualization system. These tools record and visualize interaction histories, support data analysis and communication of findings, and contribute novel mechanisms for presenting, managing, and exporting histories. Furthermore, we have analyzed aggregated collections of history sessions to evaluate Tableau usage. We describe additional tools for analyzing users history logs and how they have been applied to study usage patterns in Tableau.},
  keywords = {to read}
}

@incollection{heer2008a,
  title = {Information {{Visualization}}},
  author = {Heer, Jeffrey and van Ham, Frank and Carpendale, Sheelagh and Weaver, Chris and Isenberg, Petra},
  editor = {Kerren, A. and others},
  year = {2008},
  publisher = {Springer-Verlag},
  chapter = {Creation and Collaboration: Engaging new audiences for information visualization},
  keywords = {to read}
}

@article{heer2010,
  title = {Crowdsourcing Graphical Perception: Using Mechanical Turk to Assess Visualization Design},
  author = {Heer, Jeffrey and Bostock, Michael},
  year = {2010},
  journal = {ACM Human Factors in Computing Systems (CHI)},
  pages = {2013--212},
  abstract = {ABSTRACT Understanding perception is critical to effective visualization design. With its low cost and scalability, crowdsourcing presents an attractive option for evaluating the large design space of visualizations; however, it first requires validation. In this paper, we assess the viability of Amazon's Mechanical Turk as a platform for graphical perception experiments. We replicate previous studies of spatial encoding and luminance contrast and compare our results. We also conduct new experiments on rectangular area perception (as in treemaps or cartograms) and on chart size and gridline spacing. Our results demonstrate that crowdsourced perception experiments are viable and contribute new insights for visualization de- sign. Lastly, we report cost and performance data from our experiments and distill recommendations for the design of crowdsourced studies. ACM Classification: H5.2 [Information interfaces and presentation]: User Interfaces---Evaluation/Methodology General Terms: Experimentation, Human Factors. Keywords: Information visualization, graphical perception, user study, evaluation, Mechanical Turk, crowdsourcing.}
}

@article{heer2012,
  title = {Interactive Dynamics for Visual Analysis},
  author = {Heer, Jeffrey and Shneiderman, Ben},
  year = {2012},
  journal = {ACM Queue},
  volume = {10},
  number = {2},
  keywords = {to read}
}

@book{heer2014,
  title = {Vega},
  author = {Heer, Jeffrey},
  year = {2014}
}

@article{heiberger2014,
  title = {Design of Diverging Stacked Bar Charts for {{Likert}} Scales and Other Applications},
  author = {Heiberger, Richard M. and Robbins, Naomi B.},
  year = {2014},
  journal = {Journal of Statistical Software},
  volume = {57},
  number = {5}
}

@book{heller2014,
  title = {Infographic {{Designers}}' {{Sketchbooks}}},
  author = {Heller, Steven and Landers, Rick},
  year = {2014},
  publisher = {Princeton Architectural Press}
}

@article{hemmenginger2007,
  title = {The {{ACM}} and {{IEEE-CS}} Guidelines for Undergraduate {{CS}} Education},
  author = {Hemmenginger, David},
  year = {2007},
  journal = {Communications of the ACM},
  volume = {50},
  number = {5}
}

@inproceedings{hermans2015,
  title = {Enron's Spreadsheets and Related Emails: {{A}} Dataset and Analysis},
  booktitle = {{{ICSE}}},
  author = {Hermans, Felienne and {Murphy-Hill}, Emerson},
  year = {2015}
}

@book{hermans2016,
  title = {Functional Programming in {{Excel}}},
  author = {Hermans, Felienne},
  year = {2016},
  month = jan
}

@misc{hermans2019,
  title = {Explicit {{Direct Instruction}} in {{Programming Education}}},
  author = {Hermans, Filienne},
  year = {2019},
  month = jan
}

@book{hermans2021,
  title = {The {{Programmer}}'s {{Brain}}},
  author = {Hermans, Felienne},
  year = {2021},
  publisher = {Manning},
  urldate = {2021-07-21},
  abstract = {Your brain responds in a predictable way when it encounters new or difficult tasks. This unique book teaches you concrete techniques rooted in cognitive science that will improve the way you learn and think about code.{$<$}br/{$><$}br/{$>$} In The Programmer's Brain: What every programmer needs to know about cognition you will learn:{$<$}br/{$><$}br/{$>$} Fast and effective ways to master new programming languages{$<$}/li{$>$} Speed reading skills to quickly comprehend new code{$<$}/li{$>$} Techniques to unravel the meaning of complex code{$<$}/li{$>$} Ways to learn new syntax and keep it memorized{$<$}/li{$>$} Writing code that is easy for others to read{$<$}/li{$>$} Picking the right names for your variables{$<$}/li{$>$} Making your codebase more understandable to newcomers{$<$}/li{$>$} Onboarding new developers to your team{$<$}/li{$>$} {$<$}/ul{$>$} {$<$}br/{$>$} Learn how to optimize your brain's natural cognitive processes to read code more easily, write code faster, and pick up new languages in much less time. This book will help you through the confusion you feel when faced with strange and complex code, and explain a codebase in ways that can make a new team member productive in days!},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/66B834PE/the-programmers-brain.html}
}

@incollection{hermansetal2009,
  title = {Domain-{{Specific Languages}} in {{Practice}}: {{A User Study}} on the {{Success Factors}}},
  shorttitle = {Domain-{{Specific Languages}} in {{Practice}}},
  booktitle = {Model {{Driven Engineering Languages}} and {{Systems}}},
  author = {Hermans, Felienne and Pinzger, Martin and {van Deursen}, Arie},
  editor = {Sch{\"u}rr, Andy and Selic, Bran},
  year = {2009},
  volume = {5795},
  pages = {423--437},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-04425-0_33},
  urldate = {2021-07-27},
  isbn = {978-3-642-04424-3 978-3-642-04425-0},
  file = {/Users/amcnamara/Zotero/storage/SCU8D4ZE/Hermans et al. - 2009 - Domain-Specific Languages in Practice A User Stud.pdf}
}

@inproceedings{hermansetal2018,
  title = {Code {{Phonology}}: {{An Exploration}} into the {{Vocalization}} of {{Code}}},
  booktitle = {2018 {{ACM}}/{{IEEE}} 26th {{International Conference}} on {{Program Comprehension}}},
  author = {Hermans, Felienne and Swidan, Alaaeddin and Aivaloglou, Efthimia},
  year = {2018}
}

@article{herndon2013,
  title = {Does {{High Public Debt Consistently Stifle Economic Growth}}? {{A Critique}} of {{Reinhart}} and {{Rogoff}}},
  author = {Herndon, Thomas and Ash, Michael and Pollin, Robert},
  year = {2013},
  journal = {Cambridge Journal of Economics},
  volume = {38},
  number = {2}
}

@article{hesterberg2015,
  title = {What Teachers Should Know about the Bootstrap: {{Resampling}} in the Undergraduate Statistics Curriculum},
  author = {Hesterberg, Tim},
  year = {2015},
  journal = {The American Statistician},
  volume = {69},
  number = {4},
  pages = {371--386}
}

@article{hicksirizarry2018,
  title = {A {{Guide}} to {{Teaching Data Science}}},
  author = {Hicks, Stephanie C. and Irizarry, Rafael A.},
  year = {2018},
  month = oct,
  journal = {The American Statistician},
  volume = {72},
  number = {4},
  pages = {382--391},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2017.1356747},
  urldate = {2021-06-24},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/8EL5EEIV/Hicks and Irizarry - 2018 - A Guide to Teaching Data Science.pdf}
}

@article{hicksirizarry2018a,
  title = {A {{Guide}} to {{Teaching Data Science}}},
  author = {Hicks, Stephanie C. and Irizarry, Rafael A.},
  year = {2018},
  month = oct,
  journal = {The American Statistician},
  volume = {72},
  number = {4},
  pages = {382--391},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2017.1356747},
  urldate = {2021-07-20},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/FWEISKMD/Hicks and Irizarry - 2018 - A Guide to Teaching Data Science.pdf}
}

@article{higginsetal,
  title = {Students' Approaches to Exploring Relationships between Categorical Variables},
  author = {Higgins, Traci and Mokros, Jan and Rubin, Andee and Sagrans, Jacob},
  journal = {Teaching Statistics},
  volume = {n/a},
  number = {n/a},
  issn = {1467-9639},
  doi = {10.1111/test.12331},
  urldate = {2023-07-07},
  abstract = {In the context of an afterschool program in which students explore relatively large authentic datasets, we investigated how 11- to 14-year old students worked with categorical variables. During the program, students learned to use the Common Online Data Analysis Platform (CODAP), a statistical analysis platform specifically designed for middle and high school students, to create and interpret graphs. Following the program, we conducted individual clinical interviews, during which students used CODAP to answer questions about relationships between variables. Here, we describe how students engaged in exploratory data analysis that involved looking at relationships between two categorical variables. Students worked from data in table form and created ``contingency graphs,'' a variant of contingency tables, which they used to analyze and draw insights from the data. Our research identified four strategies that students used to examine the data in order to explore patterns, make comparisons, and answer questions with the data.},
  copyright = {{\copyright} 2023 The Authors. Teaching Statistics published by John Wiley \& Sons Ltd on behalf of Teaching Statistics Trust.},
  langid = {english},
  keywords = {categorical variables,contingency graph,data science,graphing,questioning data,teaching},
  file = {/Users/amcnamara/Zotero/storage/F4TDN6EQ/Higgins et al. - Students' approaches to exploring relationships be.pdf;/Users/amcnamara/Zotero/storage/R78MP2VC/test.html}
}

@article{hochachka2007,
  title = {Data-Mining Discovery of Pattern and Process in Ecological Systems},
  author = {Hochachka, Wesley M. and Caruana, Rich and Fink, Daniel and Munson, Art and Riedewald, Mirek and Sorokina, Daria and Kelling, Steve},
  year = {2007},
  journal = {Journal of Wildlife Management},
  volume = {71},
  number = {7},
  pages = {2427--2437},
  abstract = {Most ecologists use statistical methods as their main analytical tools when analyzing data to identify relationships between a response and a set of predictors; thus, they treat all analyses as hypothesis tests or exercises in parameter estimation. However, little or no prior knowledge about a system can lead to creation of a statistical model or models that do not accurately describe major sources of variation in the response variable. We suggest that under such circumstances data mining is more appropriate for analysis. In this paper we 1) present the distinctions between data-mining (usually exploratory) analyses and parametric statistical (confirmatory) analyses, 2) illustrate 3 strengths of data-mining tools for generating hypotheses from data, and 3) suggest useful ways in which data mining and statistical analyses can be integrated into a thorough analysis of data to facilitate rapid creation of accurate models and to guide further research.},
  keywords = {ecology,statistics}
}

@inproceedings{hochachka2010,
  title = {Checklist Programs as a Source of Data for Bird Monitoring: Designing Analyses and Model Validations to Account for Unequal Spatial and Temporal Sampling Effort},
  booktitle = {Proceedings of the 18th {{Conference}} of the {{European Bird Census Council}}},
  author = {Hochachka, Wesley M. and Fink, Daniel and Kelling, Steve},
  year = {2010},
  volume = {23},
  pages = {9--20},
  publisher = {European Bird Census Council},
  abstract = {Data on bird distribution and abundance are being collected in an ever- increasing number of countries using internet-based data collection schemes. We work with data from one well-established checklist schemes in North America, called eBird. Our experience with constructing species distribution models from these data has shown that an important challenge in analysis is dealing with data that are not evenly distributed across the continent or among times of year. Here we summarize recent work in developing a novel method for creating and validating species distribution models from bird checklist data, producing predictions of distributions throughout the year.},
  keywords = {participatory sensing,statistics}
}

@article{hofmann2012,
  title = {Graphical Tests for Power Comparison of Competing Designs},
  author = {Hofmann, Heike and Follett, Lendie and Majumder, Mahbubul and Cook, Dianne},
  year = {2012},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {18},
  number = {12},
  abstract = {Lineups [4, 28] have been established as tools for visual testing similar to standard statistical inference tests, allowing us to evaluate the validity of graphical findings in an objective manner. In simulation studies [12] lineups have been shown as being efficient: the power of visual tests is comparable to classical tests while being much less stringent in terms of distributional assumptions made. This makes lineups versatile, yet powerful, tools in situations where conditions for regular statistical tests are not or cannot be met. In this paper we introduce lineups as a tool for evaluating the power of competing graphical designs. We highlight some of the theoretical properties and then show results from two studies evaluating competing designs: both studies are designed to go to the limits of our perceptual abilities to highlight differences between designs. We use both accuracy and speed of evaluation as measures of a successful design. The first study compares the choice of coordinate system: polar versus cartesian coordinates. The results show strong support in favor of cartesian coordinates in finding fast and accurate answers to spotting patterns. The second study is aimed at finding shift differences between distributions. Both studies are motivated by data problems that we have recently encountered, and explore using simulated data to evaluate the plot designs under controlled conditions. Amazon Mechanical Turk (MTurk) is used to conduct the studies. The lineups provide an effective mechanism for objectively evaluating plot designs.},
  keywords = {to read}
}

@inproceedings{holmboe2000,
  title = {A Framework for Knowledge: {{Analysing}} High School Students' Understanding of Data Modeling},
  booktitle = {12th {{Workshop}} of the {{Psychology}} of {{Programming Interest Group}}},
  author = {Holmboe, Christian},
  editor = {Blackwell, A. F. and Bilotta, E.},
  year = {2000},
  pages = {267--279},
  publisher = {Psychology of Programming Interest Group},
  keywords = {education,statistics}
}

@inproceedings{holmboe2001,
  title = {Research {{Agenda}} for {{Computer Science Education}}},
  booktitle = {13th Workshop of the Psychology of Programming Interest Group},
  author = {Holmboe, Christian and McIver, Linda and George, Carlisle},
  year = {2001},
  pages = {207--223},
  publisher = {Psychology of Programming Interest Group},
  keywords = {computer science,education}
}

@article{hoogeveenetal2021,
  title = {Many {{Analysts Religion Project}}: {{Data}} on {{Religiosity}} and {{Well-being}}},
  shorttitle = {Many {{Analysts Religion Project}}},
  author = {Hoogeveen, Suzanne and Sarafoglou, Alexandra and van Elk, Michiel and Wagenmakers, Eric-Jan},
  year = {2021},
  month = may,
  publisher = {OSF},
  urldate = {2021-09-27},
  abstract = {This repository includes the dataset, documentation, original survey, and background information.      Hosted on the Open Science Framework},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/NUCBC85P/qbdce.html}
}

@manual{horstetal2020,
  type = {manual},
  title = {\texttt{palmerpenguins}: Palmer Achipelago (Antarctica) Penguin Data. R Package Version 0.1.0},
  shorttitle = {allisonhorst/palmerpenguins},
  author = {Horst, Allison M and Hill, Alison Presmanes and Gorman, Kristen B},
  year = {2020},
  month = jul,
  doi = {10.5281/ZENODO.3960218},
  urldate = {2021-10-27},
  abstract = {CRAN release of palmerpenguins v0.1.0 R package by Horst, Hill and Gorman (July 2020).},
  copyright = {Open Access}
}

@book{horton2014,
  title = {A Compendium of Commands to Teach Statistics with {{R}}},
  author = {Horton, Nicholas J. and Pruim, Randall and Kaplan, Daniel},
  year = {2014},
  publisher = {Project MOSAIC}
}

@inproceedings{horton2014a,
  title = {Teaching Precursors to Data Science in Introductory and Second Courses in Statistics},
  booktitle = {{{ICOTS-9}}},
  author = {Horton, Nicholas J. and Baumer, Ben and Wickham, Hadley},
  year = {2014}
}

@article{horton2015,
  title = {Challenges and {{Opportunities}} for {{Statistics}} and {{Statistical Education}}: {{Looking Back}}, {{Looking Forward}}},
  shorttitle = {Challenges and {{Opportunities}} for {{Statistics}} and {{Statistical Education}}},
  author = {Horton, Nicholas J.},
  year = {2015},
  month = apr,
  journal = {The American Statistician},
  volume = {69},
  number = {2},
  pages = {138--145},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.2015.1032435},
  urldate = {2022-01-24},
  abstract = {The 175th anniversary of the ASA provides an opportunity to look back into the past and peer into the future. What led our forebears to found the association? What commonalities do we still see? What insights might we glean from their experiences and observations? I will use the anniversary as a chance to reflect on where we are now and where we are headed in terms of statistical education amidst the growth of data science. Statistics is the science of learning from data. By fostering more multivariable thinking, building data-related skills, and developing simulation-based problem solving, we can help to ensure that statisticians are fully engaged in data science and the analysis of the abundance of data now available to us.},
  keywords = {American Statistical Association,Data science,Empirical problem solving,History,Lemuel Shattuck,Simulation studies,Statistical computing,Statistical education.},
  file = {/Users/amcnamara/Zotero/storage/V79AHRL7/Horton - 2015 - Challenges and Opportunities for Statistics and St.pdf;/Users/amcnamara/Zotero/storage/QFG4G8DV/00031305.2015.html}
}

@article{hortonetal2015,
  title = {Taking a {{Chance}} in the {{Classroom}}: {{Setting}} the {{Stage}} for {{Data Science}}: {{Integration}} of {{Data Management Skills}} in {{Introductory}} and {{Second Courses}} in {{Statistics}}},
  shorttitle = {Taking a {{Chance}} in the {{Classroom}}},
  author = {Horton, Nicholas J. and Baumer, Benjamin S. and Wickham, Hadley},
  year = {2015},
  month = apr,
  journal = {CHANCE},
  volume = {28},
  number = {2},
  pages = {40--50},
  publisher = {Taylor \& Francis},
  issn = {0933-2480},
  doi = {10.1080/09332480.2015.1042739},
  urldate = {2022-01-24},
  file = {/Users/amcnamara/Zotero/storage/BZQCBKWH/Horton et al. - 2015 - Taking a Chance in the Classroom Setting the Stag.pdf;/Users/amcnamara/Zotero/storage/HR3B93BL/09332480.2015.html}
}

@article{hortonhardin2021,
  title = {Integrating {{Computing}} in the {{Statistics}} and {{Data Science Curriculum}}: {{Creative Structures}}, {{Novel Skills}} and {{Habits}}, and {{Ways}} to {{Teach Computational Thinking}}},
  shorttitle = {Integrating {{Computing}} in the {{Statistics}} and {{Data Science Curriculum}}},
  author = {Horton, Nicholas J. and Hardin, Johanna S.},
  year = {2021},
  month = jan,
  journal = {Journal of Statistics and Data Science Education},
  volume = {29},
  number = {sup1},
  pages = {S1-S3},
  issn = {2693-9169},
  doi = {10.1080/10691898.2020.1870416},
  urldate = {2021-07-20},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/BPZPN2FA/hortonhardin2021.pdf;/Users/amcnamara/Zotero/storage/N3RTPVYR/Horton and Hardin - 2021 - Integrating Computing in the Statistics and Data S.pdf}
}

@inproceedings{howe2019,
  title = {The {{Next Million R}} Users},
  booktitle = {Rstudio::Conf},
  author = {Howe, Carl},
  year = {2019}
}

@book{hsia2005,
  title = {Taming {{Java}} for the Classroom},
  author = {Hsia, James I. and Simpson, Elspeth and Smith, Daniel and Cartwright, Robert},
  year = {2005},
  abstract = {Java is the canonical language for teaching introductory programming, but its complex syntax and abundance of con- structs are difficult for beginners to learn. This paper shows how object-oriented programming in Java can be made more accessible to beginners through the use of ``language levels'', a hierarchy of progressively richer subsets of Java. This hierarchy is implemented as an extension of the DrJava pedagogic programming environment.},
  keywords = {DrJava,language levels,object-oriented programming},
  annotation = {Published: SIGCSE'05, https://www.cs.rice.edu/ javaplt/drjava/papers/drjava-language-levels.pdf}
}

@inproceedings{hsueh,
  title = {Data Quality from Crowdsourcing: {{A}} Study of Annotation Selection Criteria},
  booktitle = {Proceedings of the {{NAACL HLT Workshop}} on Active Learning for Natural Language Processing},
  author = {Hsueh, Pei-Yun and Melville, Prem and Sindhwani, Vikas},
  keywords = {crowdsourcing,data,statistics}
}

@inproceedings{huang2010,
  title = {Are You Contributing Trustworthy Data? {{The}} Case for a Reputation System in Participatory Sensing},
  booktitle = {{{MSWiM}}'10},
  author = {Huang, Kuan Lun and Kanhere, Salil S. and Hu, Wen},
  year = {2010},
  abstract = {Participatory sensing is a revolutionary new paradigm in which volunteers collect and share information from their local environment using mobile phones. The inherent open- ness of this platform makes it easy to contribute corrupted data. This paper proposes a novel reputation system that employs the Gompertz function for computing device reputation score as a reflection of the trustworthiness of the contributed data. We implement this system in the con- text of a participatory noise monitoring application and con- duct extensive real-world experiments using Apple iPhones. Experimental results demonstrate that our scheme achieves three-fold improvement in comparison with the state-of-the- art Beta reputation scheme.}
}

@unpublished{huchette,
  title = {A Constraint-Based Layout Approach to Data Visualization},
  author = {Huchette, Joey and Wickham, Hadley},
  abstract = {This work explores the connection between statistical data visualization problems and simulated physical systems. Apply- ing techniques for constraint-based layout, data points are treated as physical objects under the influence of forces such as gravity and electrostatic repulsion. Many common statistical data visualization problems can be viewed through this lens; we consider scatterplot overplotting and the Dorling cartogram as our motivating problems. The reference implementation is straightforward and capable of producing instructive visualizations. Additionally, we present a suite of tools useful for tuning the system parameters that determine the quality of the produced visualization.}
}

@book{huff1954,
  title = {How to Lie with Statistics},
  author = {Huff, Darrell},
  year = {1954},
  publisher = {W W Norton \& Company}
}

@article{hullman2015,
  title = {Hypothetical {{Outcome Plots Outperform Error Bars}} and {{Violin Plots}} for {{Inferences About Reliability}} of {{Variable Ordering}}},
  author = {Hullman, Jessica and Resnick, Paul and Adar, Eytan},
  year = {2015},
  journal = {PLOS ONE},
  volume = {10},
  number = {11},
  pages = {e0142444}
}

@book{hullman2016,
  title = {Hypothetical {{Outcome Plots}}: {{Experiencing}} the {{Uncertain}}},
  author = {Hullman, Jessica},
  year = {2016},
  month = jan
}

@book{huynh,
  title = {Google Refine},
  author = {Huynh, David and Mazzocchi, Stefano}
}

@book{hyndman2021,
  title = {Forecasting: {{Principles}} and {{Practice}}},
  shorttitle = {Forecasting},
  author = {Hyndman, Rob J. and Athanasopoulos, George},
  year = {2021},
  month = may,
  edition = {3rd ed. edition},
  publisher = {Otexts},
  abstract = {Forecasting is required in many situations. Deciding whether to build another power generation plant in the next five years requires forecasts of future demand. Scheduling staff in a call centre next week requires forecasts of call volumes. Stocking an inventory requires forecasts of stock requirements. Telecommunication routing requires traffic forecasts a few minutes ahead. Whatever the circumstances or time horizons involved, forecasting is an important aid in effective and efficient planning. This textbook provides a comprehensive introduction to forecasting methods and presents enough information about each method for readers to use them sensibly. Examples use R with many data sets taken from the authors' own consulting experience. In this third edition, all chapters have been updated to cover the latest research and forecasting methods. One new chapter has been added on time series features. The latest version of the book is freely available online at http: //OTexts.com/fpp3.},
  isbn = {978-0-9875071-3-6},
  langid = {english}
}

@book{ibmcorp2013,
  title = {{{SPSS Statistics}} for {{Windows}}, {{Version}} 22.0},
  author = {{IBM Corp}},
  year = {2013},
  annotation = {Published: Armonk, NY: IBM Corp}
}

@article{ihaka,
  title = {R: {{A Language}} for Data Analysis and Graphics},
  author = {Ihaka, Ross and Gentleman, Robert}
}

@article{ihaka1996,
  title = {R: {{A Language}} for Data Analysis and Graphics},
  author = {Ihaka, Ross and Gentleman, Robert},
  year = {1996},
  journal = {Journal of Computational and Graphical Statistics},
  volume = {5},
  number = {3},
  pages = {299--314},
  abstract = {In this article we discuss our experience designing and implementing a statistical computing language. In developing this new language, we sought to combine what we felt were useful features from two existing computer languages. We feel that the new language provides advantages in the areas of portability, computational efficiency, memory management, and scoping.}
}

@book{ihaka2010,
  title = {Simply Start over and Build Something Better},
  author = {Ihaka, Ross},
  year = {2010},
  month = sep
}

@book{inc2015,
  title = {Collaborative {{Data Science}}},
  author = {Inc, Polly Technologies},
  year = {2015}
}

@article{ince2012,
  title = {The Case for Open Computer Programs},
  author = {Ince, Darrel C. and Hatton, Leslie and {Graham-Cumming}, John},
  year = {2012},
  month = feb,
  journal = {Nature},
  volume = {482},
  pages = {485--488},
  keywords = {computer science,open source}
}

@book{ingalls2013,
  title = {Lively {{Web}}},
  author = {Ingalls, Dan and Krahn, Robert and others},
  year = {2013}
}

@article{ingraham2015,
  title = {This Is the Best Explanation of Gerrymandering You Will Ever See},
  author = {Ingraham, Christopher},
  year = {2015},
  month = mar,
  journal = {The Washington Post}
}

@techreport{initiative2010,
  title = {Common {{Core State Standards}} for {{English Language Arts}} \& {{Literacy}} in {{History}}/{{Social Studies}}, {{Science}}, and {{Technical Subjects}}},
  author = {Initiative, Common Core State Standards},
  year = {2010},
  institution = {Common Core State Standards Initiative}
}

@inproceedings{ioannidou2011,
  title = {Computational {{Thinking Patterns}}},
  booktitle = {American {{Educational Research Association}}},
  author = {Ioannidou, Andri and Bennett, Vicki and Repenning, Alexander and Koh, Kyu Han and Basawapatna, Ashok},
  year = {2011},
  month = apr,
  keywords = {computational thinking,education}
}

@article{ipeirotis2010,
  title = {Analyzing the {{Amazon Mechanical Turk}} Marketplace},
  author = {Ipeirotis, Panagiotis G.},
  year = {2010},
  journal = {XRDS: Crossroads, The ACM Magazine for Students},
  volume = {17},
  number = {2},
  pages = {16--21},
  keywords = {data,mTurk}
}

@inproceedings{ipeirotis2010a,
  title = {Quality {{Management}} on {{Amazon Mechanical Turk}}},
  booktitle = {Proceedings of the {{ACM SIGKDD Workshop}} on {{Human Computation}}},
  author = {Ipeirotis, Panagiotis G. and Provost, Foster and Wang, Jing},
  year = {2010},
  abstract = {Crowdsourcing services, such as Amazon Mechanical Turk, allow for easy distribution of small tasks to a large number of workers. Unfortunately, since manually verifying the quality of the submitted results is hard, malicious workers often take advantage of the verification difficulty and submit answers of low quality. Currently, most requesters rely on redundancy to identify the correct answers. However, redundancy is not a panacea. Massive redundancy is expensive, increasing significantly the cost of crowdsourced solutions. Therefore, we need techniques that will accurately estimate the quality of the workers, allowing for the rejection and blocking of the low-performing workers and spammers. However, existing techniques cannot separate the true (un- recoverable) error rate from the (recoverable) biases that some workers exhibit. This lack of separation leads to incorrect assessments of a worker's quality. We present algorithms that improve the existing state-of-the-art techniques, enabling the separation of bias and error. Our algorithm generates a scalar score representing the inherent quality of each worker. We illustrate how to incorporate cost-sensitive classification errors in the overall framework and how to seamlessly integrate unsupervised and supervised techniques for inferring the quality of the workers. We present experimental results demonstrating the performance of the proposed algorithm under a variety of settings.},
  keywords = {data,mTurk}
}

@book{irizarry2016,
  title = {Not All Forecasters Got It Wrong: {{Nate Silver}} Does It Again (Again)},
  author = {Irizarry, Rafa},
  year = {2016},
  month = nov
}

@article{irizarry2020,
  title = {The {{Role}} of {{Academia}} in {{Data Science Education}}},
  author = {Irizarry, Rafael A.},
  year = {2020},
  month = jan,
  journal = {Harvard Data Science Review},
  doi = {10.1162/99608f92.dd363929},
  urldate = {2021-07-20},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/Q792RTFV/Irizarry - 2020 - The Role of Academia in Data Science Education.pdf}
}

@article{isenberg2011,
  title = {A {{Study}} on {{Dual-Scale Data Charts}}},
  author = {Isenberg, Petra and Bezerianos, Anastasia and Dragicevic, Pierre and Fekete, Jean-Daniel},
  year = {2011},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {17},
  number = {12},
  abstract = {Abstract---We present the results of a user study that compares different ways of representing Dual-Scale data charts. Dual- Scale charts incorporate two different data resolutions into one chart in order to emphasize data in regions of interest or to enable the comparison of data from distant regions. While some design guidelines exist for these types of charts, there is currently little empirical evidence on which to base their design. We fill this gap by discussing the design space of Dual-Scale cartesian-coordinate charts and by experimentally comparing the performance of different chart types with respect to elementary graphical perception tasks such as comparing lengths and distances. Our study suggests that cut-out charts which include collocated full context and focus are the best alternative, and that superimposed charts in which focus and context overlap on top of each other should be avoided. Index Terms---Focus+Context, Quantitative Experiment, Dual-Scale Charts.}
}

@article{jackson1992,
  title = {Computer-{{Assisted Thinking Tools}}: {{Problem Solving}} in {{Graphical Data Analysis}}},
  author = {Jackson, David F. and Berger, Carl F. and Edwards, Billie Jean},
  year = {1992},
  journal = {Journal of Educational Computing Research},
  volume = {8},
  number = {1},
  pages = {43--67},
  abstract = {In an effort to infuse problem-solving activities requiring higher-order think- ing into inner-city classrooms, we investigated the use of microcomputers with a graphing application program to teach principles of the design and interpretation of graphs to a population of students with little or no prior knowledge of graphs or data analysis. This is an exploratory, naturalistic research study of students' thoughts and behavior in this context. It differs from most earlier work in the field of computer-assisted graphing in three major ways: first, we focus broadly on a wide variety of types of graphs, used in several different subject matter contexts; second, we conduct research into problem solving processes in an unmodified whole-classroom environment; third, we supplement participant observations on student cognition with ex- tensive behavioral sequence data gathered automatically and unobtrusively by the computer. We detail our attempts to develop, adapt, and apply quantitative methods of analysis of behavioral sequences which are potentially applicable to such data gathered for a large number of students.},
  keywords = {to read}
}

@book{james2015,
  title = {An Introduction to Statistical Learning},
  author = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
  year = {2015},
  publisher = {Springer}
}

@article{jamieson2004,
  title = {Likert Scales: How to (Ab)Use Them},
  shorttitle = {Likert Scales},
  author = {Jamieson, Susan},
  year = {2004},
  month = dec,
  journal = {Medical Education},
  volume = {38},
  number = {12},
  pages = {1217--1218},
  issn = {0308-0110, 1365-2923},
  doi = {10.1111/j.1365-2929.2004.02012.x},
  urldate = {2021-10-01},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/LQDZC6CJ/Jamieson - 2004 - Likert scales how to (ab)use them.pdf}
}

@article{jenaetal2021,
  title = {The {{Next Billion Users}} of {{Visualization}}},
  author = {Jena, Amit and Butler, Matthew and Dwyer, Tim and Ellis, Kirsten and Engelke, Ulrich and Kirkham, Reuben and Marriott, Kim and Paris, Cecile and Rajamanickam, Venkatesh},
  year = {2021},
  month = mar,
  journal = {IEEE Computer Graphics and Applications},
  volume = {41},
  number = {2},
  pages = {8--16},
  issn = {1558-1756},
  doi = {10.1109/MCG.2020.3044071},
  abstract = {We argue that visualization research has overwhelmingly focused on users from the economically developed world. However, billions of people around the world are rapidly emerging as new users of information technology. Most of the next billion users of visualization technologies will come from parts of the world that are extremely populous but historically ignored by the visualization research community. Their needs may be different to the types of users that researchers have targeted in the past, but, at the same time, they may have even more to gain in terms of access to data potentially affecting their quality of life. We propose a call to action for the visualization community to identify opportunities and use cases where users can benefit from visualization; develop universal design principles; extend evaluations by including the general population; and engage with a wider global population.},
  keywords = {Artificial intelligence,Data visualization,Globalization,Human computer interaction,Information technology,Sociology,Statistics},
  file = {/Users/amcnamara/Zotero/storage/BL2GJV4C/Jena et al. - 2021 - The Next Billion Users of Visualization.pdf;/Users/amcnamara/Zotero/storage/QXB3LLME/9380869.html}
}

@book{jenkins2009,
  title = {Confronting the {{Challenges}} of {{Participatory Culture}}: {{Media Education}} for the 21st {{Century}}},
  author = {Jenkins, Henry and Clinton, Katie and Purushotma, Ravi and Robison, Alice J. and Weigel, Margaret},
  year = {2009},
  publisher = {The MIT Press},
  keywords = {digital humanities,education,participatory sensing}
}

@article{jing2015,
  title = {{{geoCount}}: {{An R}} Package for the Analysis of Geostatistical Count Data},
  author = {Jing, Liang and Oliveira, Victor De},
  year = {2015},
  journal = {Journal of Statistical Software},
  volume = {63},
  number = {11},
  keywords = {to read}
}

@article{johnsonberenson2019,
  title = {Choosing {{Among Computational Software Tools}} to {{Enhance Learning}} in {{Introductory Business Statistics}}},
  author = {Johnson, Marina E. and Berenson, Mark L.},
  year = {2019},
  month = jul,
  journal = {Decision Sciences Journal of Innovative Education},
  volume = {17},
  number = {3},
  pages = {214--238},
  issn = {1540-4595, 1540-4609},
  doi = {10.1111/dsji.12186},
  urldate = {2021-06-24},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/EY9JYCT3/Johnson and Berenson - 2019 - Choosing Among Computational Software Tools to Enh.pdf}
}

@book{jones2015,
  title = {Avoiding {{Data Pitfalls}}, {{Part}} 2: {{Fooled}} by {{Small Samples}}},
  author = {Jones, Ben},
  year = {2015},
  month = jan
}

@book{jumpingrivers2023,
  title = {R {{User Groups}} {\textbar} {{A}} List of {{R}} Conferences and Meetings},
  author = {{Jumping Rivers}},
  year = {2023},
  urldate = {2023-01-11},
  abstract = {R User Groups {\textbar} A list of R conferences and meetings.},
  file = {/Users/amcnamara/Zotero/storage/MPT8PGX7/r-user-groups.html}
}

@article{kader2008,
  title = {Statistics in the Middle Grades: {{Understanding}} Center and Spread},
  author = {Kader, Gary and Mamer, Jim},
  year = {2008},
  month = aug,
  journal = {Mathematics Teaching in the Middle School},
  volume = {14},
  number = {1},
  pages = {38--43}
}

@misc{kaggle2020,
  title = {State of {{Machine Learning}} and {{Data Science}} 2020},
  author = {Kaggle},
  year = {2020}
}

@article{kammounetal2022,
  title = {Generative {{Adversarial Networks}} for Face Generation: {{A}} Survey},
  shorttitle = {Generative {{Adversarial Networks}} for Face Generation},
  author = {Kammoun, Amina and Slama, Rim and Tabia, Hedi and Ouni, Tarek and Abid, Mohmed},
  year = {2022},
  month = mar,
  journal = {ACM Computing Surveys},
  pages = {1122445.1122456},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/1122445.1122456},
  urldate = {2022-07-12},
  abstract = {Recently, Generative Adversarial Networks (GANs) have received enormous progress, which makes them able to learn complex data distributions in particular faces. More and more efficient GAN architectures have been designed and proposed to learn the different variations of faces, such as cross pose, age, expression and style. These GAN based approaches need to be reviewed, discussed, and categorized in terms of architectures, applications, and metrics. Several reviews that focus on the use and advances of GAN in general have been proposed. However, the GAN models applied to the face, that we call facial GANs, have never been addressed. In this article, we review facial GANs and their different applications. We mainly focus on architectures, problems and performance evaluation with respect to each application and used datasets. More precisely, we reviewed the progress of architectures and we discussed the contributions and limits of each. Then, we exposed the encountered problems of facial GANs and proposed solutions to handle them. Additionally, as GANs evaluation has become a notable current defiance, we investigate the state of the art quantitative and qualitative evaluation metrics and their applications. We concluded the article with a discussion on the face generation challenges and proposed open research issues.},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/P7YUMP5A/Kammoun et al. - 2022 - Generative Adversarial Networks for face generatio.pdf}
}

@article{kandel2011,
  title = {Research Directions in Data Wrangling: {{Visualizations}} and Transformations for Usable and Credible Data},
  author = {Kandel, Sean and Heer, Jeffrey and Pleasant, Catherine and Kennedy, Jessie and van Ham, Frank and Riche, Nathalie Henry and Weaver, Chris and Lee, Bongshin and Brodbeck, Dominique and Buono, Paolo},
  year = {2011},
  journal = {Information Visualization},
  volume = {10},
  number = {4},
  pages = {271--288},
  abstract = {In spite of advances in technologies for working with data, analysts still spend an inordinate amount of time diagnosing data quality issues and manipulating data into a usable form. This process of `data wrangling' often constitutes the most tedious and time-consuming aspect of analysis. Though data cleaning and integration are longstanding issues in the database community, relatively little research has explored how interactive visualization can advance the state of the art. In this article, we review the challenges and opportunities associated with addressing data quality issues. We argue that analysts might more effectively wrangle data through new interactive systems that integrate data verification, transformation, and visualization. We identify a number of outstanding research questions, including how appropriate visual encodings can facilitate apprehension of missing data, discrepant values, and uncertainty; how interactive visualizations might facilitate data transform specification; and how recorded provenance and social interaction might enable wider reuse, verification, and modification of data transformations.}
}

@inproceedings{kandel2011a,
  title = {Wrangler: {{Interactive}} Visual Specification of Data Transformation Scripts},
  booktitle = {{{CHI}} 2011},
  author = {Kandel, Sean and Paepcke, Andreas and Hellerstein, Joseph and Heer, Jeffrey},
  year = {2011}
}

@article{kandeletal2012,
  title = {Enterprise {{Data Analysis}} and {{Visualization}}: {{An Interview Study}}},
  shorttitle = {Enterprise {{Data Analysis}} and {{Visualization}}},
  author = {Kandel, Sean and Paepcke, Andreas and Hellerstein, Joseph M. and Heer, Jeffrey},
  year = {2012},
  month = dec,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {18},
  number = {12},
  pages = {2917--2926},
  issn = {1077-2626},
  doi = {10.1109/TVCG.2012.219},
  urldate = {2021-07-18},
  file = {/Users/amcnamara/Zotero/storage/ET2H8BQX/Kandel et al. - 2012 - Enterprise Data Analysis and Visualization An Int.pdf}
}

@inproceedings{kanhere2011,
  title = {Participatory Sensing: {{Crowdsourcing}} Data from Mobile Smartphones in Urban Spaces},
  booktitle = {2011 12th {{IEEE International Conference}} on {{Mobile Data Management}}},
  author = {Kanhere, Salil S.},
  year = {2011},
  abstract = {Abstract---The recent wave of sensor-rich, Internet-enabled, smart mobile devices such as the Apple iPhone has opened the door for a novel paradigm for monitoring the urban landscape known as participatory sensing. Using this paradigm, ordinary citizens can collect multi-modal data streams from the surround- ing environment using their mobile devices and share the same using existing communication infrastructure (e.g., 3G service or WiFi access points). The data contributed from multiple participants can be combined to build a spatiotemporal view of the phenomenon of interest and also to extract important community statistics. Given the ubiquity of mobile phones and the high density of people in metropolitan areas, participatory sensing can achieve an unprecedented level of coverage in both space and time for observing events of interest in urban spaces. Several exciting participatory sensing applications have emerged in recent years. For example, GPS traces uploaded by drivers and passengers can be used to generate realtime traffic statistics. Similarly, street-level audio samples collected by pedestrians can be aggregated to create a citywide noise map. In this advanced seminar, we will provide a comprehensive overview of this new and exciting paradigm and outline the major research challenges.},
  keywords = {crowdsourcing,participatory sensing}
}

@incollection{kanter2006,
  title = {New {{Directions}} for {{Teaching}} and {{Learning}}},
  author = {Kanter, David E. and Schreck, Melissa},
  year = {2006},
  publisher = {Wiley InterScience},
  abstract = {This chapter explores the extent to which project-based science (PBS) curricula designed with supports for students' inquiry into complex scientific data can help urban students make sense of such data and promote their deep understanding of standards-based content. We review qualitative and quantitative data from classroom enactments of a PBS high school biology curriculum called Disease Detectives.},
  chapter = {Learning content using complex data in project-based science: An example from high school biology in urban classroom}
}

@article{kanter2010,
  title = {The Impact of a Project-Based Science Curriculum on Minority Student Achievement, Attitudes, and Careers: {{The}} Effects of Teacher Content and Pedagogical Content Knowledge and Inquiry-Based Practices},
  author = {Kanter, David E. and Konstantopoulos, Spyros},
  year = {2010},
  journal = {Science Education},
  volume = {94},
  pages = {855--887},
  abstract = {Project-based science (PBS) curricula have project- and inquiry-based aspects that leverage the strengths of urban students from ethnic and racial groups under- represented in science careers, potentially impacting positively these students science achievement and attitudes and thus their college and career plans. We aimed to determine the extent to which a PBS curriculum would show this. We provided professional development to bolster urban teachers science content knowledge (CK) and science pedagogical content knowledge (PCK) to observe the maximal impact of the PBS curriculum. We found that students science achievement improved with the PBS curriculum, but that their attitudes toward science and plans to pursue science did not. Increases in teachers CK and PCK with the professional development correlated with the improvements in student science achievement but did not correlate with improvements in student science attitudes or plans. However, the frequency of teachers use of specific inquiry-based activities did correlate with improvements in students science attitudes and plans. In sum, the extent of the success of a PBS curriculum with students from groups underrepresented in science careers appears to be dependent on elements of both teacher knowledge (CK and PCK) and teachers frequency of use of inquiry-based activities that are consistent with culturally relevant pedagogical practices.}
}

@article{kaplan2007,
  title = {Computing and Introductory Statistics},
  author = {Kaplan, Daniel},
  year = {2007},
  journal = {Technology Innovations in Statistics Education},
  volume = {1},
  number = {1},
  keywords = {to read}
}

@book{kaplan2013,
  title = {Data and {{Computing Fundamentals}}},
  author = {Kaplan, Danny and Shoop, Libby},
  year = {2013}
}

@article{kaplan2020,
  title = {{{StatPREP}}: {{Living}} in {{Interesting Times}} for {{Introductory Statistics Education}}},
  shorttitle = {{{StatPREP}}},
  author = {Kaplan, Daniel},
  year = {2020},
  journal = {AmstatNews},
  urldate = {2021-07-21},
  abstract = {In these interesting times, statistics educators and computer science educators need to work together and integrate their curriculum to prepare students for careers in data science.},
  langid = {american},
  file = {/Users/amcnamara/Zotero/storage/X4QEWRKB/statprep-intro-stat-ed.html}
}

@article{kaplanetal2010,
  title = {Lexical {{Ambiguity}} in {{Statistics}}: {{How Students Use}} and {{Define}} the {{Words}}: {{Association}}, {{Average}}, {{Confidence}}, {{Random}} and {{Spread}}},
  shorttitle = {Lexical {{Ambiguity}} in {{Statistics}}},
  author = {Kaplan, Jennifer and Fisher, Diane G. and Rogness, Neal T.},
  year = {2010},
  month = jul,
  journal = {Journal of Statistics Education},
  volume = {18},
  number = {2},
  pages = {null},
  publisher = {Taylor \& Francis},
  issn = {null},
  doi = {10.1080/10691898.2010.11889491},
  urldate = {2023-07-07},
  abstract = {Language plays a crucial role in the classroom. The use of specialized language in a domain can cause a subject to seem more difficult to students than it actually is. When words that are part of everyday English are used differently in a domain, these words are said to have lexical ambiguity. Studies in other fields, such as mathematics and chemistry education, suggest that in order to help students learn vocabulary instructors should exploit the lexical ambiguity of the words. The study presented here is the second in a sequence of studies designed to understand the effects of and develop techniques for exploiting lexical ambiguities in statistics classrooms. In particular, this paper looks at five statistical terms and the meanings of these terms most commonly expressed by students at the end of an undergraduate statistics course.},
  keywords = {Language,Lexical ambiguity,Statistics education,Word usage},
  file = {/Users/amcnamara/Zotero/storage/TFWKQGM4/Kaplan et al. - 2010 - Lexical Ambiguity in Statistics How Students Use .pdf}
}

@article{kaplanetal2014,
  title = {Investigating {{Student Understanding}} of {{Histograms}}},
  author = {Kaplan, Jennifer J. and Gabrosek, John G. and Curtiss, Phyllis and Malone, Chris},
  year = {2014},
  month = jul,
  journal = {Journal of Statistics Education},
  volume = {22},
  number = {2},
  pages = {null},
  publisher = {Taylor \& Francis},
  issn = {null},
  doi = {10.1080/10691898.2014.11889701},
  urldate = {2023-06-08},
  abstract = {Histograms are adept at revealing the distribution of data values, especially the shape of the distribution and any outlier values. They are included in introductory statistics texts, research methods texts, and in the popular press, yet students often have difficulty interpreting the information conveyed by a histogram. This research identifies and discusses four misconceptions prevalent in student understanding of histograms. In addition, it presents pre- and post-test results on an instrument designed to measure the extent to which the misconceptions persist after instruction. The results presented indicate not only that the misconceptions are commonly held by students prior to instruction, but also that they persist after instruction. Future directions for teaching and research are considered.},
  keywords = {Histograms,Introductory Statistics,Misconceptions,Undergraduate Student Learning},
  file = {/Users/amcnamara/Zotero/storage/U62STMFL/Kaplan et al. - 2014 - Investigating Student Understanding of Histograms.pdf}
}

@manual{kaplanpruim2020,
  type = {manual},
  title = {\texttt{ggformula}: Formula Interface to the Grammar of Graphics},
  author = {Kaplan, Daniel and Pruim, Randall},
  year = {2020}
}

@article{katz2013,
  title = {How {{Y}}'all, {{Youse}} and {{You Guys Talk}}},
  author = {Katz, Josh and Andrews, Wilson},
  year = {2013},
  journal = {The New York Times}
}

@article{kay1984,
  title = {Computer {{Software}}},
  author = {Kay, Alan},
  year = {1984},
  journal = {Scientific American}
}

@incollection{kay1990,
  title = {The {{Art}} of {{Human-Computer Interface Design}}},
  author = {Kay, Alan},
  editor = {Laurel, Brenda},
  year = {1990},
  publisher = {Addison-Wesley Publishing Company},
  chapter = {User Interface: A personal view}
}

@inproceedings{kazaketal2022,
  title = {Investigation {{Cycle}} for {{Analysing Image-Based Data}}: {{Perspectives From Three Contexts}}},
  shorttitle = {Investigation {{Cycle}} for {{Analysing Image-Based Data}}},
  booktitle = {Bridging the {{Gap}}: {{Empowering}} and {{Educating Today}}'s {{Learners}} in {{Statistics}}. {{Proceedings}} of the {{Eleventh International Conference}} on {{Teaching Statistics}}},
  author = {Kazak, Sibel and Fielding, Jill and {Zapata-Cardona}, Lucia},
  year = {2022},
  month = dec,
  publisher = {International Association for Statistical Education},
  doi = {10.52041/iase.icots11.T8D1},
  urldate = {2023-07-07},
  abstract = {A traditional data investigation cycle includes problem posing, planning and collecting data, analysing data, and making conclusions. This research studies the data investigation cycle for analysing image-based data. In three independent research projects, students at different educational levels and from different countries were provided photographic data of families and their environments around the world from the Dollar Street project. Data collected included classroom video-recordings (Australia), individual student interviews (Colombia), and pre-service mathematics teachers' interviews (Turkey). Analysis focused on the sequence of actions that helped students when attempting to pose and answer questions based on the data set. Findings suggested a similar, iterative sequence of actions across all cohorts: context and data set familiarisation, variable identification/generation, problem posing and planning, data organisation and analysis, and drawing conclusions.},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/ENVB8ZL7/Pamukkale Üniversitesi et al. - 2022 - Investigation Cycle for Analysing Image-Based Data.pdf}
}

@inproceedings{keim2006,
  title = {Challenges in Visual Data Analysis},
  booktitle = {Proceedings of the {{Information Visualization}} ({{IV}}'06)},
  author = {Keim, Daniel A. and Mansmann, Florian and Schneidewind, J{\"o}rn and Ziegler, Hartmut},
  year = {2006},
  abstract = {In today's applications data is produced at unprecedented rates. While the capacity to collect and store new data grows rapidly, the ability to analyze these data volumes increases at much lower pace. This gap leads to new challenges in the analysis process, since analysts, decision makers, engineers, or emergency response teams depend on information ``concealed'' in the data. The emerging field of visual analytics focuses on handling massive, heterogenous, and dynamic volumes of information through integration of human judgement by means of visual representations and interaction techniques in the analysis process. Further- more, it is the combination of related research areas including visualization, data mining, and statistics that turns visual analytics into a promising field of research. This paper aims at providing an overview of visual analytics, its scope and concepts, and details the most important technical re- search challenges in the field.},
  keywords = {to read}
}

@article{kelleher2005,
  title = {Lowering the Barriers to Programming: {{A}} Taxonomy of Programming Environments and Languages for Novice Programmers},
  author = {Kelleher, Caitlin and Pausch, Randy},
  year = {2005},
  journal = {ACM Computing Surveys},
  volume = {37},
  number = {2},
  pages = {83--137},
  abstract = {Since the early 1960's, researchers have built a number of programming languages and environments with the intention of making programming accessible to a larger number of people. This paper presents a taxonomy of languages and environments designed to make programming more accessible to novice programmers of all ages. The systems are organized by their primary goal, either to teach programming or to use programming to empower their users, and then by the authors' approach to making learning to program easier for novice programmers. The paper explains all categories in the taxonomy, provides a brief description of the systems in each category, and suggests some avenues for future work in novice programming environments and languages.},
  keywords = {computer science,education}
}

@incollection{kennedy2001,
  title = {Mathematics and {{Democracy}}: {{The Case}} for {{Quantitative Literacy}}},
  author = {Kennedy, Dan},
  year = {2001},
  pages = {55--60},
  publisher = {{National Council on Education and the Disciplines}},
  chapter = {The Emperor's Vanishing Clothes}
}

@article{khairiree,
  title = {Enhancing Students' Understanding Statistics with {{TinkerPlots}}: Problem-Based Learning Approach},
  author = {Khairiree, Krongthong and Kurusatian, Piromya},
  abstract = {The purpose of the research study was to explore the effectiveness of using TinkerPlots dynamic software and problem-based learning approach in statistics classes. The research study was conducted in the Education Year of 2009. The subjects were students and teachers from secondary school in Bangkok, Thailand. The pre-test and post-test control group design was used in the study. The experimental group learned statistics using TinkerPlots dynamic software and problem-based learning approach. The control group learned statistics through problem-based learning. The data collection included quantitative and qualitative methods. The research finding showed that if it is appropriately employed, TinkerPlots dynamic software can be used as an effective tool in enhancing active learning and students' understanding in statistics. In addition, the students had positive attitude toward statistics after they learned statistics using TinkerPlots dynamic software.},
  keywords = {education,statistics}
}

@article{kidwell2008,
  title = {Visualizing Incomplete and Partially Ranked Data},
  author = {Kidwell, Paul and Lebanon, Guy and Cleveland, William S.},
  year = {2008},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {14},
  number = {6},
  abstract = {Ranking data, which result from m raters ranking n items, are difficult to visualize due to their discrete algebraic structure, and the computational difficulties associated with them when n is large. This problem becomes worse when raters provide tied rankings or not all items are ranked. We develop an approach for the visualization of ranking data for large n which is intuitive, easy to use, and computationally efficient. The approach overcomes the structural and computational difficulties by utilizing a natural measure of dissimilarity for raters, and projecting the raters into a low dimensional vector space where they are viewed. The visualization techniques are demonstrated using voting data, jokes, and movie preferences.},
  keywords = {IEEEviz,theoretical}
}

@article{kimhenke2021,
  title = {Easy-to-{{Use Cloud Computing}} for {{Teaching Data Science}}},
  author = {Kim, Brian and Henke, Graham},
  year = {2021},
  month = jan,
  journal = {Journal of Statistics and Data Science Education},
  volume = {29},
  number = {sup1},
  pages = {S103-S111},
  issn = {2693-9169},
  doi = {10.1080/10691898.2020.1860726},
  urldate = {2021-06-24},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/B686NJ8J/Kim and Henke - 2021 - Easy-to-Use Cloud Computing for Teaching Data Scie.pdf}
}

@inproceedings{kindlmann2014,
  title = {An {{Algebraic Process}} for {{Visualization Design}}},
  booktitle = {{{IEEE Transactions}} on {{Visualization}} and {{Computer Graphics}}},
  author = {Kindlmann, Gordon and Scheidegger, Carlos},
  year = {2014},
  abstract = {We present a model of visualization design based on algebraic considerations of the visualization process. The model helps characterize visual encodings, guide their design, evaluate their effectiveness, and highlight their shortcomings. The model has three components: the underlying mathematical structure of the data or object being visualized, the concrete representation of the data in a computer, and (to the extent possible) a mathematical description of how humans perceive the visualization. Because we believe the value of our model lies in its practical application, we propose three general principles for good visualization design. We work through a collection of examples where our model helps explain the known properties of existing visualizations methods, both good and not-so-good, as well as suggesting some novel methods. We describe how to use the model alongside experimental user studies, since it can help frame experiment outcomes in an actionable manner. Exploring the implications and applications of our model and its design principles should provide many directions for future visualization research.}
}

@article{kirkpatrick2015,
  title = {Putting the Data Science into Journalism},
  author = {Kirkpatrick, Keith},
  year = {2015},
  journal = {Communications of the ACM},
  volume = {58},
  number = {5},
  pages = {15--17}
}

@article{kirsh2010,
  title = {Thinking with External Representations},
  author = {Kirsh, David},
  year = {2010},
  month = nov,
  journal = {AI \& SOCIETY},
  volume = {25},
  number = {4},
  pages = {441--454},
  issn = {1435-5655},
  doi = {10.1007/s00146-010-0272-8},
  urldate = {2023-07-10},
  abstract = {Why do people create extra representations to help them make sense of situations, diagrams, illustrations, instructions and problems? The obvious explanation---external representations save internal memory and computation---is only part of the story. I discuss seven ways external representations enhance cognitive power: they change the cost structure of the inferential landscape; they provide a structure that can serve as a shareable object of thought; they create persistent referents; they facilitate re-representation; they are often a more natural representation of structure than mental representations; they facilitate the computation of more explicit encoding of information; they enable the construction of arbitrarily complex structure; and they lower the cost of controlling thought---they help coordinate thought. Jointly, these functions allow people to think more powerfully with external representations than without. They allow us to think the previously unthinkable.},
  langid = {english},
  keywords = {Cost structure,External representations,Interactivity,Sense making,Thinking},
  file = {/Users/amcnamara/Zotero/storage/8TU6GEKJ/Kirsh - 2010 - Thinking with external representations.pdf}
}

@inproceedings{kittur2008,
  title = {Crowdsourcing User Studies with {{Mechanical Turk}}},
  booktitle = {{{CHI}} 2008 {{Proceedings}}},
  author = {Kittur, Aniket and Chi, Ed H. and Suh, Bongwon},
  year = {2008},
  abstract = {User studies are important for many aspects of the design process and involve techniques ranging from informal surveys to rigorous laboratory studies. However, the costs involved in engaging users often requires practitioners to trade off between sample size, time requirements, and monetary costs. Micro-task markets, such as Amazon's Mechanical Turk, offer a potential paradigm for engaging a large number of users for low time and monetary costs. Here we investigate the utility of a micro-task market for collecting user measurements, and discuss design considerations for developing remote micro user evaluation tasks. Although micro-task markets have great potential for rapidly collecting user measurements at low costs, we found that special care is needed in formulating tasks in order to harness the capabilities of the approach.},
  keywords = {to read}
}

@incollection{kleinhoffman2012,
  title = {Macrocognition, {{Mental Models}}, and {{Cognitive Task Analysis Methodolody}}},
  booktitle = {Naturalistic {{Decision Making}} and {{Macrocognition}}},
  author = {Klein, Gary and Hoffman, Robert R},
  editor = {Schraagen, Jan Maarten and Militello, Laura and Ormerod, Tom and Lipshitz, Raanan},
  year = {2012},
  month = oct,
  publisher = {Ashgate Publishing, Ltd.},
  abstract = {This book presents the latest work in the area of naturalistic decision making (NDM) and its extension into the area of macrocognition. It contains 18 chapters relating research centered on the study of expertise in naturalistic settings, written by international experts in NDM and cognitive systems engineering. The objective of the book is to present the reader with exciting new developments in this field of research, which is characterized by its application-oriented focus.The work addresses only real-world problems and issues. For instance, how do multi-national teams collaborate effectively? How can surgeons best be supported by technology? How do detectives make sense of complex criminal cases? In all instances the studies have been carried out on experts within their respective domains.The traditional field of NDM is extended in this work by focusing on macrocognitive functions other than decision making, namely sense-making, coordination and planning. This has broadened the scope of the field. The book also contains a theoretical discussion of the macro-micro distinction.Naturalistic Decision Making and Macrocognition will be relevant to graduate students, researchers and professionals (including professionals and researchers in business, industry and government) who are interested in decision making, expertise, training methods and system design. The material may be used in two ways: theoretically, to advance understanding of the field of naturalistic decision making; and practically, to gain insight into how experts in various domains solve particular problems, understand and deal with issues and collaborate with others.},
  googlebooks = {k4TuD8tImMMC},
  isbn = {978-1-4094-8569-8},
  langid = {english},
  keywords = {Psychology / General}
}

@article{knuth1984,
  title = {Literate {{Programming}}},
  author = {Knuth, Donald E.},
  year = {1984},
  journal = {The Computer Journal},
  volume = {27},
  number = {2},
  pages = {97--111}
}

@book{knuth1992,
  title = {Literate {{Programming}}},
  author = {Knuth, Donald E.},
  year = {1992},
  publisher = {Stanford University}
}

@misc{koeze2018,
  title = {What {{Do Men Think It Means To Be A Man}}?},
  author = {Koeze, Ella},
  year = {2018},
  month = jun,
  journal = {FiveThirtyEight},
  urldate = {2023-06-07},
  abstract = {We asked more than 1,600 men whether \#MeToo changed their thinking on masculinity.},
  langid = {american},
  file = {/Users/amcnamara/Zotero/storage/KDG44MML/what-do-men-think-it-means-to-be-a-man.html}
}

@article{kofman2022,
  title = {Facebook {{Finally Agrees}} to {{Eliminate Tool That Enabled Discriminatory Advertising}}},
  author = {Kofman, Ava, Ariana Tobin},
  year = {2022},
  month = jun,
  journal = {ProPublica},
  urldate = {2023-07-17},
  abstract = {Six years after ProPublica revealed that Facebook allowed advertisers to exclude Black users and others, the company agreed to a settlement with the Justice Department to overhaul its ad algorithm system.},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/JI9FTK5N/facebook-doj-advertising-discrimination-settlement.html}
}

@article{kolling2010,
  title = {The {{Greenfoot}} Programming Environment},
  author = {K{\"o}lling, Michael},
  year = {2010},
  journal = {ACM Transactions on Computing Education},
  volume = {10},
  number = {4},
  pages = {14:1--14:21},
  keywords = {computer science,education}
}

@article{kollingetal2003,
  title = {The {{BlueJ System}} and Its {{Pedagogy}}},
  author = {K{\"o}lling, Michael and Quig, Bruce and Patterson, Andrew and Rosenberg, John},
  year = {2003},
  month = dec,
  journal = {Computer Science Education},
  volume = {13},
  number = {4},
  pages = {249--268},
  issn = {0899-3408, 1744-5175},
  doi = {10.1076/csed.13.4.249.17496},
  urldate = {2021-06-15},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/ZWE6SAKS/Kölling et al. - 2003 - The BlueJ System and its Pedagogy.pdf}
}

@article{kollingrosenberg2001,
  title = {Guidelines for {{Teaching Object Orientation}} with {{Java}}},
  author = {K{\"o}lling, Michael and Rosenberg, John},
  year = {2001},
  month = sep,
  journal = {ACM SIGCSE Bulletin},
  volume = {33},
  number = {3},
  pages = {33--36},
  issn = {0097-8418},
  doi = {10.1145/507758.377461},
  urldate = {2021-06-15},
  abstract = {How to best teach object orientation to first year students is currently a topic of much debate. One of the tools suggested to aid in this task is BlueJ, an integrated development environment specifically designed for teaching. BlueJ supports a unique style of introduction of OO concepts. In this paper we discuss a set of problems with OO teaching, present some guidelines for better course design and show how BlueJ can be used to make significant improvements to introductory OO courses. We end by presenting a description of a possible project sequence using this teaching approach.},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/4YLB453F/Kölling and Rosenberg - 2001 - Guidelines for teaching object orientation with Ja.pdf}
}

@article{konold1989,
  title = {Informal {{Conceptions}} of {{Probability}}},
  author = {Konold, Clifford},
  year = {1989},
  month = mar,
  journal = {Cognition and Instruction},
  volume = {6},
  number = {1},
  pages = {59--98},
  publisher = {Routledge},
  issn = {0737-0008},
  doi = {10.1207/s1532690xci0601_3},
  urldate = {2023-06-07},
  abstract = {A model of informal reasoning under conditions of uncertainty, the outcome approach, was developed to account for the nonnormative responses of a subset of 16 undergraduates who were interviewed. For individuals who reason according to the outcome approach, the goal in questions of uncertainty is to predict the outcome of an individual trial. Their predictions take the form of yes-no decisions on whether an outcome will occur on a particular trial. These predictions are then evaluated as having been either right or wrong. Their predictions are often based on a deterministic model of the situation. In follow-up interviews using a different set of problems, responses of outcome-oriented participants were predicted. In one problem, their responses were at variance both with normative interpretations of probability and with the "representativeness heuristic" (Kahneman \& Tversky, 1972). Although the outcome approach is inconsistent with formal theories of probability, its components are logically consistent and reasonable in the context of everyday decision making.}
}

@inproceedings{konold1999,
  title = {Center and {{Spread}}: {{A Pas}} de {{Deux}}},
  booktitle = {There's {{More}} to {{Life}} than {{Centers Research Pre-session}}, 77th {{Annual Meeting}} of the {{NCTM}}},
  author = {Konold, Clifford and Pollatsek, Alexander},
  year = {1999},
  keywords = {to read}
}

@article{konold2002,
  title = {Data Analysis as the Search for Signals in Noisy Processes},
  author = {Konold, Clifford and Pollatsek, Alexander},
  year = {2002},
  month = jul,
  journal = {Journal for Research in Mathematics Education},
  volume = {33},
  number = {4},
  pages = {259--289},
  keywords = {statistics}
}

@incollection{konold2003,
  title = {A Research Companion to {{Principles}} and {{Standards}} for {{School Mathematics}}},
  author = {Konold, Clifford and Higgins, Traci},
  editor = {Kilpatrick, Jeremy},
  year = {2003},
  publisher = {National Council of Teachers of Mathematics},
  chapter = {Reasoning about data},
  keywords = {to read}
}

@inproceedings{konold2003a,
  title = {If {{U}} Can {{Graff}} These Numbers--2,15,6--{{Your}} Stat Literit},
  booktitle = {Annual {{Meeting}} of the {{American Educational Research Association}}},
  author = {Konold, Clifford and Khalil, Khalimahtul},
  year = {2003},
  publisher = {American Educational Research Association},
  keywords = {education,statistical literacy,statistics}
}

@incollection{konold2007,
  title = {Designing a {{Data Analysis Tool}} for {{Learners}}},
  booktitle = {Thinking with {{Data}}},
  author = {Konold, Clifford},
  editor = {Lovett, Marsha C. and Shah, Priti},
  year = {2007},
  publisher = {Lawrence Erlbaum Associates},
  keywords = {computer science,data,education,statistics,Statistics education research,technology}
}

@article{konold2008,
  title = {Reconnecting Data and Chance},
  author = {Konold, Clifford and Kazak, Sibel},
  year = {2008},
  journal = {Technology Innovations in Statistics Education},
  volume = {2},
  number = {1},
  abstract = {For the past 15 years, pre-university students in many countries including the United States have encountered data analysis and probability as separate, mostly independent strands. Classroom- based research suggests, however, that some of the difficulties students have in learning basic skills in Exploratory Data Analysis stem from a lack of rudimentary ideas in probability. We describe a recent project that is developing materials to support middle-school students in coming to see the ``data in chance'' and the ``chance in data.'' Instruction focuses on four main ideas: model fit, distribution, signal-noise, and the Law of Large Numbers. Central to our approach is a new modeling and simulation capability that we are building into a future version of the data-analysis software TinkerPlots. We describe three classroom-tested probability investigations that employ an iterative model-fit process in which students evaluate successive theories by collecting and analyzing data. As distribution features become a focal point of students' explorations, signal and noise components of data become visible as variation around an ``expected'' distribution in repeated samples. An important part of students' learning experience, and one enhanced through visual aspects of TinkerPlots, is becoming able to see things in data they were previously unable to see.},
  keywords = {to read}
}

@article{konoldetal2014,
  title = {Data {{Seen Through Different Lenses}}},
  author = {Konold, Clifford and Higgins, Traci and Russell, Susan Jo and Khalil, Khalimahtul},
  year = {2014},
  journal = {Educational Studies in Mathematics},
  abstract = {Statistical reasoning focuses on properties that belong not to individual data values but to the entire aggregate. We analyze students' statements from three different sources to explore possible building blocks of the idea of data as aggregate and speculate on how young students go about putting these ideas together. We identify four general perspectives that students use in working with data, which in addition to an aggregate perspective include regarding data as pointers, as case values, and as classifiers. Some students seem inclined to view data from only one of these three alternative perspectives, which then influences the types of questions they ask, the data representations they generate or prefer, the interpretations they give to notions such as the average, and the conclusions they draw from the data.}
}

@article{konoldetal2017,
  title = {Modeling as a Core Component of Structuring Data},
  author = {Konold, Clifford and Finzer, William and Kreetong, Kosoom},
  year = {2017},
  journal = {Statistics Education Research Journal},
  volume = {16},
  number = {2},
  pages = {191--212}
}

@misc{konoldmiller2001,
  title = {{{TinkerPlots}} (Version 0.23). {{Data Analysis Software}}.},
  author = {Konold, Clifford and Miller, Craig D.},
  year = {2001},
  address = {University of Massachusetts, Amherst}
}

@article{konoldmiller2005,
  title = {{{TinkerPlots}}: {{Dynamic}} Data Exploration},
  author = {Konold, Clifford and Miller, Craig D.},
  year = {2005},
  journal = {Computer software Emeryville, CA: Key Curriculum Press}
}

@article{kornfeld1981,
  title = {The {{Scientific Community Metaphor}}},
  author = {Kornfeld, William A. and Hewitt, Carl E.},
  year = {1981},
  month = jan,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics},
  volume = {SMC-11},
  number = {1},
  abstract = {Scientific communities have proven to be extremely successful at solving problems. They are inherently parallel systems and their macroscopic nature makes them amenable to careful study. In this paper the character of scientific research is examined drawing on sources in the philosophy and history of science. We maintain that the success of scientific research depends critically on its concurrency and pluralism. A variant of the language Ether is developed that embodies notions of concurrency necessary to emulate some of the problem solving behavior of scientific communities. Capabilities of scientific communities are discussed in parallel with simplified models of these capabilities in this language.}
}

@inproceedings{kosara2010,
  title = {Do {{Mtechnical Turks}} Dream of Square Pie Charts?},
  booktitle = {{{BELIV}}'10},
  author = {Kosara, Robert and Ziemkiewicz, Caroline},
  year = {2010},
  abstract = {Online studies are an attractive alternative to the labor- intensive lab study, and promise the possibility of reaching a larger variety and number of people than at a typical university. There are also a number of draw-backs, however, that have made these studies largely impractical so far. Amazon's Mechanical Turk is a web service that facilitates the assignment of small, web-based tasks to a large pool of anonymous workers. We used it to conduct several perception and cognition studies, one of which was identical to a previous study performed in our lab. We report on our experiences and present ways to avoid common problems by taking them into account in the study design, and taking advantage of Mechanical Turk's features.}
}

@article{kosslyn1989,
  title = {Understanding Charts and Graphs},
  author = {Kosslyn, Stephen M.},
  year = {1989},
  journal = {Applied Cognitive Psychology},
  volume = {3},
  pages = {185--226},
  keywords = {to read}
}

@techreport{krasner1988,
  title = {A Description of the Model-View-Controller User Interface Paradigm in the {{Smalltalk-80}} System},
  author = {Krasner, Glenn E. and Pope, Stephen T.},
  year = {1988},
  institution = {ParcPlace Systems, Inc}
}

@article{krishnamurthietal2020,
  title = {Data {{Science}} as a {{Route}} to {{AI}} for {{Middle-}} and {{High-School Students}}},
  author = {Krishnamurthi, Shriram and Schanzer, Emmanuel and Politz, Joe Gibbs and Lerner, Benjamin S. and Fisler, Kathi and Dooman, Sam},
  year = {2020},
  month = apr,
  journal = {arXiv:2005.01794 [cs]},
  eprint = {2005.01794},
  primaryclass = {cs},
  urldate = {2021-07-22},
  abstract = {The Bootstrap Project's Data Science curriculum has trained about 100 teachers who are using it around the country. It is specifically designed to aid adoption at a wide range of institutions. It emphasizes valuable curricular goals by drawing on both the education literature and on prior experience with other computing outreach projects. It embraces "three P's" of data-oriented thinking: the promise, pitfalls, and perils. This paper briefly describes the curriculum's design, content, and outcomes, and explains its value on the road to AI curricula.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society},
  file = {/Users/amcnamara/Zotero/storage/N9GKW353/Krishnamurthi et al. - 2020 - Data Science as a Route to AI for Middle- and High.pdf;/Users/amcnamara/Zotero/storage/EWXSFKIH/2005.html}
}

@misc{kross2021,
  title = {Analyzing {{R Function Arguments}}},
  author = {Kross, Sean},
  year = {2021},
  month = feb
}

@phdthesis{kross2022,
  title = {Prototyping the {{Developer Experience}} for {{Data Science Practitioners}} and {{Instructors}}},
  author = {Kross, Sean},
  year = {2022},
  urldate = {2022-07-12},
  abstract = {Data science encompasses the most prominent collection of methods for creating scientific knowledge in the 21st century. Currently, data scientists must navigate a wide-ranging and often incoherent ecosystem of tools, in addition to organizing sociotechnical interactions with colleagues across many fields of expertise.This predicament motivates my thesis: The elements of data science work that are based in human expertise and social relationships must be integrated into existing programming workflows to create the developer experience that data scientists require to be successful. This dissertation supports my thesis by presenting three empirical studies and two tools. First, I investigated how professional data scientists teach novices about data science focused programming workflows, including how to adapt software development tools to their work, how to navigate the full depth of the stack of technologies that data science relies on, and how to use their tools to help communicate their findings. Then I explored how a team of academic data scientists repurposed the tools from their everyday data science work to create a data science course designed to reach traditionally underrepresented groups in computing. Finally, I examined how consulting data scientists interact with their clients, how their working relationships take them beyond well-characterized programming-oriented cycles, and how they achieve success by integrating designerly work into their data analysis process. These studies inspired me to develop two tools: 1. Datamations animates each step in a data analysis pipeline via transitions that show how rows, columns, and cells move within a data frame. 2. Tidy Data Tutor creates step-by-step interactive illustrations for a data analysis pipeline, so that every individual cell can be tracked. The main research findings of this dissertation are that data scientists adapt software engineering tools to fit into their own workflows, and that data scientists must communicate the uncertainty that they face in their work to novices. Additionally, this dissertation found that several nested cycles are required for data scientists to achieve success in collaboration with their colleagues. Finally, my prototype tools showed that animations and illustrations derived from data wrangling code can help convey a clearer understanding of data analysis pipelines.},
  langid = {english},
  school = {UC San Diego},
  file = {/Users/amcnamara/Zotero/storage/7JKR5CQM/Kross - 2022 - Prototyping the Developer Experience for Data Scie.pdf}
}

@manual{kuhnetal2022,
  type = {manual},
  title = {\texttt{tidymodels}: Easily Install and Load the tidymodels Packages},
  shorttitle = {tidymodels},
  author = {Kuhn, Max and Wickham, Hadley and RStudio},
  year = {2022},
  month = jul,
  urldate = {2023-01-13},
  abstract = {The tidy modeling "verse" is a collection of packages for modeling and statistical analysis that share the underlying design philosophy, grammar, and data structures of the tidyverse.},
  copyright = {MIT + file LICENSE},
  keywords = {MachineLearning}
}

@inproceedings{kuittinensajaniemi2004,
  title = {Teaching Roles of Variables in Elementary Programming Courses},
  booktitle = {Proceedings of the 9th Annual {{SIGCSE}} Conference on {{Innovation}} and Technology in Computer Science Education  - {{ITiCSE}} '04},
  author = {Kuittinen, Marja and Sajaniemi, Jorma},
  year = {2004},
  pages = {57},
  publisher = {ACM Press},
  address = {Leeds, United Kingdom},
  doi = {10.1145/1007996.1008014},
  urldate = {2021-06-11},
  isbn = {978-1-58113-836-8},
  langid = {english}
}

@article{kyriakidis2004,
  title = {A Geostatistical Framework for Area-to-Point Spatial Interpolation},
  author = {Kyriakidis, Phaedon C.},
  year = {2004},
  journal = {Geographical Analysis},
  volume = {36},
  number = {3},
  abstract = {The spatial prediction of point values from areal data of the same attribute is ad- dressed within the general geostatistical framework of change of support; the tern support refers to the domain informed by each datum or unknown value. I t is demonstrated that the proposed geostatistical framework can explicitly and consistently account for the support differences between the available areal data and the sought-after point predictions. In particular, it is proved that appropriate modeling of all area-to-area and area-to-point covariances required by the geostatistical frame- work yields coherent (mass-preserving or pycnophylactic) predictions. In other words, the areal average (orareal total) of point predictions within any arbitrary area informed by an areal-average (orareal-total)datum is equal to that particular datum. In addition, the proposed geostatistical framework offers the unique advantage of providing a measure of the reliability (standard error) of each point prediction. It is also demonstrated that several existing approaches for area-to-point interpolation can be viewed within this geostatisticalframework. More precisely, it is shown that (i)the choropleth map case corresponds to the geostatistical solution under the assumption of spatial independence at the point support level; (ii) several forms of kernel smoothing can be regarded as alternative (albeit sometimes incoherent) implementations of the geostatistical approach; and (iii)Tobler's smooth pycnophylactic interpolation, on a quasi-injinite domain without non-negativity constraints, corresponds to the geostatistical solution when the semivariogram model adopted at the point support level is identijied to the free-space Green's functions(linear in 1-Dor logarithmic in 2-0)of Poisson's partial differential equation. In lieu of a f o m l case study, several 1-D examples are given to illustrate pertinent concepts.},
  keywords = {to read}
}

@inproceedings{lagerstrometal2015,
  title = {The {{Myth}} of the {{Six-Minute Rule}}: {{Student Engagement}} with {{Online Videos}}},
  shorttitle = {The {{Myth}} of the {{Six-Minute Rule}}},
  booktitle = {2015 {{ASEE Annual Conference}} \& {{Exposition}}},
  author = {Lagerstrom, Larry and Johanes, Petr and Ponsukcharoen, Umnouy},
  year = {2015},
  month = jun,
  pages = {26.1558.1-26.1558.17},
  issn = {2153-5965},
  urldate = {2021-11-05},
  isbn = {978-0-692-50180-1},
  file = {/Users/amcnamara/Zotero/storage/BVAT7FCG/Lagerstrom et al. - 2015 - The Myth of the Six-Minute Rule Student Engagemen.pdf;/Users/amcnamara/Zotero/storage/49DVN6JC/the-myth-of-the-six-minute-rule-student-engagement-with-online-videos.html}
}

@inproceedings{lane2008,
  title = {Urban Sensing Systems: {{Opportunistic}} or Participatory?},
  booktitle = {Proceedings of the 9th Workshop on {{Mobile}} Computing Systems and Applications},
  author = {Lane, Nicholas D. and Eisenman, shane B. and Musolesi, Mirco and Miluzzo, Emiliano and Campbell, Andrew Y.},
  year = {2008},
  publisher = {ACM},
  abstract = {The development of sensing systems for urban deployments is still in its infancy. An interesting unresolved issue is the precise role assumed by people within such systems. This issue has significant implications as to where the complexity and the main challenges in building urban sensing systems will reside. This issue will also impact the scale and diversity of applications that are able to be supported. We contrast two end-points of the spectrum of conscious human involvement, namely participatory sensing, and opportunistic sensing. We develop an evaluation model and argue that opportunistic sensing more easily supports larger scale applications and broader diversity within such applications. In this paper, we provide preliminary analysis which supports this conjecture, and outline techniques we are developing in support of opportunistic sensing systems.}
}

@techreport{lang2013,
  title = {Bugs in the {{System}}: {{Computer Science Teacher Certification}} in the {{US}}},
  author = {Lang, Karen and Ganalos, Ria and Goode, Joanna and Seehorn, Deborah and Trees, Fran and Phillips, Pat and Stephenson, Chris},
  year = {2013},
  institution = {Computer Science Teachers Association}
}

@incollection{lange2003,
  title = {Quantitative Literacy: {{Why}} Numeracy Matters for Schools and Colleges},
  author = {Lange, Jan De},
  year = {2003},
  pages = {75--89},
  chapter = {Mathematics for Literacy}
}

@inproceedings{laplanteetal2016,
  title = {Methods to {{Address}} the {{Change}} of {{Support}} and {{Modifiable Areal Unit Problems}}},
  booktitle = {Hudson {{River Undergraduate Math Conference}}},
  author = {LaPlante, Sara and Mao, Jessica and Vo, MyVan},
  year = {2016}
}

@article{lapowsky2015,
  title = {Inside the School {{Silicon Valley}} Thinks Will Save Education},
  author = {Lapowsky, Issie},
  year = {2015},
  month = may,
  journal = {Wired}
}

@article{lasseretal2021,
  title = {Introductory Data Science across Disciplines, Using {{Python}}, Case Studies, and Industry Consulting Projects},
  author = {Lasser, Jana and Manik, Debsankha and Silbersdorff, Alexander and S{\"a}fken, Benjamin and Kneib, Thomas},
  year = {2021},
  month = jul,
  journal = {Teaching Statistics},
  volume = {43},
  number = {S1},
  issn = {0141-982X, 1467-9639},
  doi = {10.1111/test.12243},
  urldate = {2021-07-27},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/GKVL8BP9/Lasser et al. - 2021 - Introductory data science across disciplines, usin.pdf}
}

@incollection{latour2010,
  title = {The {{Social After Gabriel Tarde}}: {{Debates}} and {{Assessments}}},
  author = {Latour, Bruno},
  editor = {Candea, Mattei},
  year = {2010},
  publisher = {Routledge},
  chapter = {Tarde's idea of quantification},
  keywords = {quantification}
}

@incollection{latulipemacneil2020,
  title = {A {{Model}} for {{Mentoring Faculty}} and {{Teaching Assistants}} in {{Active Learning}}},
  author = {Latulipe, Celine and Macneil, Stephen},
  year = {2020},
  month = apr,
  abstract = {Moving to an active learning mode of teaching requires a fair amount of change across a variety of dimensions. There is a mental mindset adjustment, changes to class structure, prep work to develop, new tools to learn, and new ways of interacting with teaching assistants (TAs), who can range from sophomore undergraduates to graduate students. Because this type of teaching can be so different, we propose that a collaborative/ interactive model at multiple levels is the best way to bring faculty and TAs on board for a particular course. For inexperienced instructors, a team-teaching model is employed so that more senior teaching faculty can help scaffold the teaching experience for less experienced faculty members. By sharing the workload, new classes can be created quickly to address emerging needs within the department. At the same time, pedagogies, best practices, and important tacit knowledge can be shared among instructors, a way to apply active learning to the act of education itself.},
  isbn = {978-1-4696-6003-5},
  file = {/Users/amcnamara/Zotero/storage/JRUBFMMX/Latulipe and Macneil - 2020 - A Model for Mentoring Faculty and Teaching Assista.pdf}
}

@article{lau2009,
  title = {Exploring the Effects of Gender and Learning Styles on Computer Programming Performance: Implications for Programming Pedagogy},
  author = {Lau, Wilfred W. F. and Yuen, Allan H. K.},
  year = {2009},
  journal = {British Journal of Educational Technology},
  volume = {40},
  number = {4}
}

@techreport{lavine2009,
  title = {{{NSF Award Search}}: {{Award}} \# 0839952 - {{An Innovative Model}} for {{Workforce Development}} in {{Statistics}}},
  author = {Lavine, Michael},
  year = {2009},
  urldate = {2022-07-21},
  file = {/Users/amcnamara/Zotero/storage/Q8QSKZ7J/showAward.html}
}

@article{lazar2013,
  title = {The Big Picture: {{Data}}, Data, Everywhere...},
  author = {Lazar, Nicole},
  year = {2013},
  journal = {Chance},
  volume = {26},
  number = {1}
}

@article{leavy2010,
  title = {The Challenge of Preparing Preservice Teachers to Teach Informal Inferential Reasoning},
  author = {Leavy, Aisling M.},
  year = {2010},
  month = may,
  journal = {Statistics Education Research Journal},
  volume = {9},
  number = {1},
  pages = {46--67},
  keywords = {education,statistics,Statistics education research,Teacher education,Teacher knowledge}
}

@article{lee2009,
  title = {Reasoning about Probabilistic Phenomena: {{Lesson}} Learned and Applied in Software Design},
  author = {Lee, Hollylynne S. and Lee, Todd J.},
  year = {2009},
  journal = {Technology Innovations in Statistics Education},
  volume = {3},
  number = {2},
  abstract = {In this paper we provide a glimpse of the iterations of design, research and theorizing of a probability simulation tool, Probability Explorer, that have occurred over the past decade. We provide a brief description of the key features of the technology designed to allow young students opportunities to explore probabilistic situations. This is followed by details about several research observations made in multiple investigations of student explorations with this probability micro- world software package. We then explicate how research results suggest that a focus on a bidirectional interplay between theoretical distribution and empirical data can promote reasoning about probabilistic phenomena, and offer implications for instruction. The paper concludes with a discussion of a next generation innovation in the software for representing a theoretical distribution that we believe may promote better students reasoning about the bidirectional connection between theoretical distributions and empirical data.},
  keywords = {to read}
}

@article{lee2011,
  title = {Engaging Culturally and Linguistically Diverse Students in Learning Science},
  author = {Lee, Ohkee and Buxton, Cory},
  year = {2011},
  journal = {Theory into Practice},
  volume = {50},
  pages = {277--285}
}

@book{lee2021,
  title = {Deconstructing {{Categorization}} in {{Visualization Recommendation}}: {{A Taxonomy}} and {{Comparative Study}}},
  author = {Lee, Doris Jung-Lin and Setlur, Vidya and Tory, Melanie and Karahalios, Karrie and Parameswaran, Aditya},
  year = {2021},
  abstract = {Visualization recommendation (VisRec) systems provide users with suggestions for potentially interesting and useful next steps during exploratory data analysis. These recommendations are typically organized into categories based on their analytical actions, i.e., operations employed to transition from the current exploration state to a recommended visualization. However, despite the emergence of a plethora of VisRec systems in recent work, the utility of the categories employed by these systems in analytical workflows has not been systematically investigated. Our paper explores the efficacy of recommendation categories by formalizing a taxonomy of common categories and developing a system, Frontier, that implements these categories. Using Frontier, we evaluate workflow strategies adopted by users and how categories influence those strategies. Participants found recommendations that add attributes to enhance the current visualization and recommendations that filter to sub-populations to be comparatively most useful during data exploration. Our findings pave the way for next-generation VisRec systems that are adaptive and personalized via carefully chosen, effective recommendation categories.}
}

@article{leedumont2010,
  title = {An {{Exploration}} into How {{Physical Activity Data-recording Devices}} Could Be Used in {{Computer-supported Data Investigations}}},
  author = {Lee, Victor R. and DuMont, Maneksha},
  year = {2010},
  journal = {International Journal of Computational Math Learning},
  volume = {15},
  pages = {167--189},
  keywords = {Averages,Mobile technology,participatory sensing,Physical activity data,Probeware,Sensors,Statistics,Visualizations}
}

@article{leeetal2021,
  title = {Lux: {{Always-on Visualization Recommendations}} for {{Exploratory Data Science}}},
  shorttitle = {Lux},
  author = {Lee, Doris Jung-Lin and Tang, Dixin and Agarwal, Kunal and Boonmark, Thyne and Chen, Caitlyn and Kang, Jake and Mukhopadhyay, Ujjaini and Song, Jerry and Yong, Micah and Hearst, Marti A. and Parameswaran, Aditya G.},
  year = {2021},
  month = apr,
  journal = {arXiv:2105.00121 [cs]},
  eprint = {2105.00121},
  primaryclass = {cs},
  urldate = {2021-07-18},
  abstract = {Exploratory data science largely happens in computational notebooks with dataframe API, such as pandas, that support flexible means to transform, clean, and analyze data. Yet, visually exploring data in dataframes remains tedious, requiring substantial programming effort for visualization and mental effort to determine what analysis to perform next. We propose Lux, an always-on framework for accelerating visual insight discovery in data science workflows. When users print a dataframe in their notebooks, Lux recommends visualizations to provide a quick overview of the patterns and trends and suggests promising analysis directions. Lux features a high-level language for generating visualizations on-demand to encourage rapid visual experimentation with data. We demonstrate that through the use of a careful design and three system optimizations, Lux adds no more than two seconds of overhead on top of pandas for over 98\% of datasets in the UCI repository. We evaluate Lux in terms of usability via a controlled first-use study and interviews with early adopters, finding that Lux helps fulfill the needs of data scientists for visualization support within their dataframe workflows. Lux has already been embraced by data science practitioners, with over 1.9k stars on Github within its first 15 months.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Databases,Computer Science - Human-Computer Interaction},
  file = {/Users/amcnamara/Zotero/storage/EMB3ZBWA/Lee et al. - 2021 - Lux Always-on Visualization Recommendations for E.pdf;/Users/amcnamara/Zotero/storage/ZRRSAA4K/2105.html}
}

@article{leeetal2021a,
  title = {A {{Call}} for a {{Humanistic Stance Toward K}}--12 {{Data Science Education}}},
  author = {Lee, Victor R. and Wilkerson, Michelle Hoda and Lanouette, Kathryn},
  year = {2021},
  month = sep,
  journal = {Educational Researcher},
  pages = {0013189X2110488},
  issn = {0013-189X, 1935-102X},
  doi = {10.3102/0013189X211048810},
  urldate = {2021-10-04},
  abstract = {There is growing interest in how to better prepare K--12 students to work with data. In this article, we assert that these discussions of teaching and learning must attend to the human dimensions of data work. Specifically, we draw from several established lines of research to argue that practices involving the creation and manipulation of data are shaped by a combination of personal experiences, cultural tools and practices, and political concerns. We demonstrate through two examples how our proposed humanistic stance highlights ways that efforts to make data personally relevant for youth also necessarily implicate cultural and sociopolitical dimensions that affect the design and learning opportunities in data-rich learning environments. We offer an interdisciplinary framework based on literature from multiple bodies of educational research to inform design, teaching and research for more effective, responsible, and inclusive student learning experiences with and about data.},
  langid = {english}
}

@inproceedings{leehollebrands2008,
  title = {Preparing to {{Teach Data Analysis}} and {{Probability}} with {{Technology}}},
  booktitle = {Joint {{ICMI}}/{{IASE}} Study: {{Teaching}} Statistics in School Mathematics. {{Challenges}} for Teaching and Teacher Education. {{Proceedings}} of the {{ICMI Study Study}} 18 and 2008 {{IASE Round Table Conference}}},
  author = {Lee, Hollylynne S. and Hollebrands, Karen F.},
  year = {2008},
  publisher = {ICMI/IASE},
  keywords = {data,education,statistics,technology}
}

@inproceedings{leek2016,
  title = {Data Science as a Science},
  booktitle = {Joint {{Statistical Meetings}}},
  author = {Leek, Jeff},
  year = {2016}
}

@techreport{leeuw2004,
  title = {On Abandoning {{Xlisp-Stat}}},
  author = {Leeuw, Jan De},
  year = {2004},
  institution = {University of California, Los Angeles}
}

@techreport{leeuw2009,
  title = {Statistical {{Software}} - {{Overview}}},
  author = {Leeuw, Jan De},
  year = {2009},
  month = dec,
  number = {570},
  institution = {Department of Statistics, University of California, Los Angeles}
}

@techreport{legacy2008,
  title = {{{AP Statistics Teacher}}'s {{Guide}}},
  author = {Legacy, Michael},
  year = {2008},
  institution = {The College Board},
  keywords = {education,statistics}
}

@article{legacyetal2022,
  title = {{{COMPUTES}}: {{DEVELOPMENT OF AN INSTRUMENT TO MEASURE INTRODUCTORY STATISTICS INSTRUCTORS}}' {{EMPHASIS ON COMPUTATIONAL PRACTICES}}},
  shorttitle = {{{COMPUTES}}},
  author = {Legacy, Chelsey and Zieffler, Andrew and Fry, Elizabeth Brondos and Le, Laura},
  year = {2022},
  month = feb,
  journal = {STATISTICS EDUCATION RESEARCH JOURNAL},
  volume = {21},
  number = {1},
  pages = {7--7},
  issn = {1570-1824},
  doi = {10.52041/serj.v21i1.63},
  urldate = {2022-07-12},
  abstract = {The influx of data and the advances in computing have led to calls to update the introductory statistics curriculum to better meet the needs of the contemporary workforce. To this end, we developed the COMputational Practices in Undergraduate TEaching of Statistics\&nbsp; (COMPUTES) instrument, which can be used to measure the extent to which computation practices---specifically data, simulation, and coding practices---are included in the introductory statistics curriculum. Data from 236 instructors were used in a psychometric analysis to evaluate the latent structure underlying instructors' response patterns and\&nbsp; understand the quality of the items. We also examined whether computational practices are being emphasized differently across institutional settings. Results suggest that the latent structure is best captured using a correlated multidimensional model and that most items were contributing information to the measurement process. Across institutional settings, curricular emphasis related to data and simulation practices seem quite similar, while emphasis on coding practices differs.},
  copyright = {Copyright (c) 2022 STATISTICS EDUCATION RESEARCH JOURNAL},
  langid = {english},
  keywords = {instructional practices},
  file = {/Users/amcnamara/Zotero/storage/QFGQTW4H/Legacy et al. - 2022 - COMPUTES DEVELOPMENT OF AN INSTRUMENT TO MEASURE .pdf}
}

@article{legacyetal2024,
  title = {The {{Teaching}} of {{Introductory Statistics}}: {{Results}} of a {{National Survey}}},
  shorttitle = {The {{Teaching}} of {{Introductory Statistics}}},
  author = {Legacy, Chelsey and Le, Laura and Zieffler, Andrew and Fry, Elizabeth and Vivas Corrales, Pablo},
  year = {2024},
  journal = {Journal of Statistics and Data Science Education},
  volume = {0},
  number = {0},
  pages = {1--9},
  publisher = {Taylor \& Francis},
  issn = {null},
  doi = {10.1080/26939169.2024.2333732},
  urldate = {2024-06-12},
  abstract = {The Statistics Teaching Inventory (STI) was designed to assess the teaching practices of U.S.-based, college-level introductory statistics instructors in a variety of institutions and departments. This instrument has now been updated to reflect current trends and recommendations in statistics education. In this study, we used the STI to examine the current state of the curricular and instructional practices being used by U.S.-based, college-level introductory statistics instructors. We explore the extent to which instructors report that their introductory statistics courses are aligned with recommended practices as outlined by the 2016 GAISE College Report. Data were collected from a sample of college-level U.S.-based, college-level introductory statistics instructors. Results based on 228 usable responses indicated that instructors, by-and-large, tended to be following the GAISE recommendations, especially related to content. However, courses may not yet be aligned with newer content recommendations (e.g., provide students opportunities to work with multivariate data), and there is still a large percentage of instructors that are not embracing student-oriented pedagogies and assessment methods. Supplementary materials for this article are available online.},
  keywords = {Instructional practices,Statistics education research,Statistics teaching,Teaching inventory},
  file = {/Users/amcnamara/Zotero/storage/3KY4P594/Legacy et al. - 2024 - The Teaching of Introductory Statistics Results o.pdf}
}

@inproceedings{lehrer2007,
  title = {Introducing Students to Data Representation and Statistics},
  booktitle = {Proceedings of the 30th Annual Conference of the {{Mathematics Education Research Group}} of {{Australia}}},
  author = {Lehrer, Richard},
  editor = {Watson, J. and Beswick, K.},
  year = {2007},
  keywords = {data visualization,education,statistics}
}

@incollection{lehrerenglish2018,
  title = {Introducing {{Children}} to {{Modeling Variability}}},
  booktitle = {International {{Handbook}} of {{Research}} in {{Statistics Education}}},
  author = {Lehrer, Richard and English, Lyn},
  editor = {{Ben-Zvi}, Dani and Makar, Katie and Garfield, Joan},
  year = {2018},
  series = {Springer {{International Handbooks}} of {{Education}}},
  pages = {229--260},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-66195-7_7},
  urldate = {2023-07-10},
  abstract = {This chapter synthesizes diverse research investigating the potential of inducting elementary grade children into the statistical practice of modeling variability in light of uncertainty. In doing so, we take a genetic perspective toward the development of knowledge, attempting to locate productive seeds of understandings of variability that can be cultivated during instruction in ways that expand students' grasp of different aspects and sources of variability. To balance the complexity and tractability of this enterprise, we focus on a framework we refer to as data modeling. This framework suggests the inadvisability of piecewise approaches focusing narrowly on, for instance, computation of statistics, in favor of more systematic and cohesive involvement of children in practices of inquiring, visualizing, and measuring variability in service of informal inference. Modeling variability paves the way for children in the upper elementary grades to make informal inferences in light of probability structures. All of these practices can be elaborated and even transformed with new generations of digital technologies.},
  isbn = {978-3-319-66195-7},
  langid = {english},
  keywords = {Aggregate perspectives,Attributes,Case-based reasoning,Chance variability,Classifier perspectives,Data modeling,Data structure,Digital technologies,Informal inferences,Inquiry,Investigating,Measures,Posing questions,Representations,Variability,Young learners},
  file = {/Users/amcnamara/Zotero/storage/97KJC2IP/Lehrer and English - 2018 - Introducing Children to Modeling Variability.pdf}
}

@article{lehreretal2010,
  title = {{{HOW STUDENTS}}' {{SPONTANEOUS USE OF STATISTICAL TOOLS SHAPES THEIR THINKING ABOUT PRECISION}}},
  author = {Lehrer, Rich and Kim, Min-Joung and Konold, Cliff},
  year = {2010},
  journal = {ICOTS8},
  pages = {6},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/JHVCSIGN/Lehrer et al. - 2010 - HOW STUDENTS’ SPONTANEOUS USE OF STATISTICAL TOOLS.pdf}
}

@article{lehrerromberg1996,
  title = {Exploring {{Children}}'s {{Data Modeling}}},
  author = {Lehrer, Richard and Romberg, Thomas},
  year = {1996},
  month = mar,
  journal = {Cognition and Instruction},
  volume = {14},
  number = {1},
  pages = {69--108},
  publisher = {Routledge},
  issn = {0737-0008},
  doi = {10.1207/s1532690xci1401_3},
  urldate = {2023-07-10},
  abstract = {Elementary students' reasoning about data modeling is explored by conducting two design experiments. In the first design experiment, a class of fifth-grade students worked in six different design teams to develop hypermedia documents about Colonial America. Students compared the lifestyles of colonists with their own lifestyles. To this end, 10 "data analysts" developed a survey, collected and coded data, and used the dynamic notations of a computer-based tool, Tabletop (Hancock, Kaput, \& Goldsmith, 1992), to develop and examine patterns of interest in their data. Our general approach was to let the reasoning and thinking displayed in one session with the data analysts provoke and steer the tasks and problems posed in the next session. Analysis of student conversations, including their dialogue with the teacher-researcher, indicated that the construction of data was an important preamble to description and inference. Moreover, students' ideas about many elements of data modeling were tied closely to forms of notation. In the second design experiment, two children and their classroom teacher were consulted about the use of a simple randomization distribution to test hypotheses about the nature of ESP. Here, experimentation afforded a framework for teaching about inference grounded by the creation of a randomization distribution of the students' data. We conclude that data construction and analysis provide an opportunity to involve students in the important enterprise of mathematical modeling.}
}

@incollection{lehrerschauble2007,
  title = {Contrasting {{Emerging Conceptions}} of {{Distribution}} in {{Contexts}} of {{Error}} and {{Natural Variation}}},
  booktitle = {Thinking with {{Data}}},
  author = {Lehrer, Richard and Schauble, Leona},
  editor = {Lovett, Marsha C. and Shah, Priti},
  year = {2007},
  pages = {149--176},
  publisher = {Lawrence Erlbaum Associates},
  address = {New York, NY},
  keywords = {education,statistics}
}

@incollection{lehrerschauble2010,
  title = {What {{Kind}} of {{Explanation}} Is a {{Model}}?},
  booktitle = {Instructional {{Explanations}} in the {{Disciplines}}},
  author = {Lehrer, Richard and Schauble, Leona},
  editor = {Stein, Mary Kay and Kucan, Linda},
  year = {2010},
  pages = {9--22},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-1-4419-0594-9_2},
  urldate = {2023-07-07},
  abstract = {We describe modeling as a form of explanation that is particular to science and, based on a research program conducted over the last 15 years, identify the conceptual resources and practices that must be developed for school students to become initiated into this kind of reasoning. We point out that modeling is difficult for novices to grasp but is treated by school science as self-evident, which may account for the fact that it is widely misunderstood by learners and educators alike. We close by considering components of instruction, especially classroom norms and tasks, that best support the long-term development of modeling.},
  isbn = {978-1-4419-0594-9},
  langid = {english},
  keywords = {Adequate Yearly Progress,Modeling Game,Representational Competence,Representational Form,Research Meeting}
}

@article{leisch2002,
  title = {Sweave, {{Part I}}: {{Mixing R}} and {{LaTeX}}},
  author = {Leisch, Friedrich},
  year = {2002},
  journal = {R News},
  volume = {2},
  number = {3}
}

@article{lemetal2013,
  title = {On the Misinterpretation of Histograms and Box Plots},
  author = {Lem, Stephanie and Onghena, Patrick and Verschaffel, Lieven and Van Dooren, Wim},
  year = {2013},
  month = mar,
  journal = {Educational Psychology},
  volume = {33},
  number = {2},
  pages = {155--174},
  publisher = {Routledge},
  issn = {0144-3410},
  doi = {10.1080/01443410.2012.674006},
  urldate = {2023-06-14},
  abstract = {Recent studies have shown that the interpretation of graphs is not always easy for students. In order to reason properly about distributions of data, however, one needs to be able to interpret graphical representations of these distributions correctly. In this study, we used Tversky's principles for the design of graphs to explain how 125 first-year university students interpreted histograms and box plots. We systematically varied the representation that accompanied the tasks between students to identify how the design principles affected students' reasoning. Many students displayed misinterpretations of histograms and box plots, despite the fact that they had the required knowledge and time to interpret the representations correctly. We argue that the combination of dual process theories and Tversky's design principles provides a promising theoretical framework, which leads to various possibilities for future research.},
  keywords = {data distribution,graphical representations,problem solving,reasoning,statistics education}
}

@article{lemetal2014,
  title = {Interpreting Histograms. {{As}} Easy as It Seems?},
  author = {Lem, Stephanie and Onghena, Patrick and Verschaffel, Lieven and Van Dooren, Wim},
  year = {2014},
  journal = {European Journal of Psychology of Education},
  volume = {29},
  number = {4},
  eprint = {43551158},
  eprinttype = {jstor},
  pages = {557--575},
  publisher = {Springer},
  issn = {0256-2928},
  urldate = {2023-06-08},
  abstract = {Histograms are widely used, but recent studies have shown that they are not as easy to interpret as it might seem. In this article, we report on three studies on the interpretation of histograms in which we investigated, namely, (1) whether the misinterpretation by university students can be considered to be the result of heuristic reasoning, (2) whether we could influence performance by stimulating or hindering the analytic processing of histograms, and (3) whether experts still show signs of this heuristic misinterpretation. We found that both university students and experts show signs of incorrect heuristic reasoning when comparing the mean of two data sets presented by histograms. Stimulating or hindering analytic processing did not affect performance. These indications of heuristic reasoning and the impossibility to affect this analytic processing suggest that the incorrect heuristic misinterpretation of histograms is very persistent. Implications for theory and methodology, scientific and educational practice are discussed.},
  file = {/Users/amcnamara/Zotero/storage/ZMENP9JT/Lem et al. - 2014 - Interpreting histograms. As easy as it seems.pdf}
}

@book{lenth2012,
  title = {{{StatWeave Users}}' {{Manual}}},
  author = {Lenth, Russell V.},
  year = {2012},
  publisher = {University of Iowa}
}

@book{leonhardtd.2014,
  title = {@{{DLeonhardt}}: ``{{The}} Most Visited Page in {{NYT}} History Is the Dialect Quiz: {{http://nyti.ms/1bYNB1z}} . @jshkatz Made It. {{He}}'s Joined Our Team."},
  author = {{Leonhardt, D.}},
  year = {2014},
  annotation = {Published: 14 February 2014, 9:35 a.m. Tweet.}
}

@unpublished{lerner2014,
  title = {{{RDataTracker}}: {{Collecting}} Provenance in an Interactive Scripting Environment},
  author = {Lerner, Barbara S. and Boose, Emery R.},
  year = {2014},
  abstract = {Scientific data provenance is often cited as a valuable tool for scientists to use to document their data collection and analysis processes, allowing improved understanding and sharing of data and results. However, most software that supports data provenance requires scientists to adopt new technologies rather than adding these capabilities to technologies that scientists already use. In this paper, we introduce RDataTracker, an R library that supports the collection of data provenance from executed R scripts.},
  keywords = {to read}
}

@article{lesser2009,
  title = {English Language Learners in Introductory Statistics},
  author = {Lesser, Lawrence M. and Winsor, Matthew S.},
  year = {2009},
  journal = {Statistics Education Research Journal},
  volume = {8},
  number = {2},
  pages = {5--32},
  keywords = {education,language,Spanish,statistics,Statistics education research,Teacher education}
}

@article{lesser2011,
  title = {Setting the Tone: A Discursive Case Study of Problem-Based Inquiry Learning to Start a Graduate Statistics Course for in-Service Teachers},
  author = {Lesser, Lawrence M. and Kephart, Kerrie},
  year = {2011},
  journal = {Journal of Statistics Education},
  volume = {19},
  number = {3},
  abstract = {The first day of a course has great potential to set the tone for the entire course, planting the seeds for habits of mind and questioning and setting in motion expectations for classroom discourse. Rather than let the first meeting contain little besides going over the syllabus, the instructor (Lesser) decided to use two sustained open-ended scenarios to put in place from the start the problem-based inquiry learning approach he wanted to use throughout most of the course. After reviewing the literatures involved, this paper shares a description of the lesson`s design and instructional cycle and a discourse analysis of that lesson`s implementation. Strategies identified by the case study analysis include varying participation structures, well-crafted problems, and the instructor`s role as facilitator and co-learner.},
  keywords = {education,statistics}
}

@inproceedings{lewis2010,
  title = {How {{Programming Environment Shapes Perception}}, {{Learning}} and {{Goals}}: {{Logo}} vs. {{Scratch}}},
  shorttitle = {How Programming Environment Shapes Perception, Learning and Goals},
  booktitle = {Proceedings of the 41st {{ACM}} Technical Symposium on {{Computer}} Science Education - {{SIGCSE}} '10},
  author = {Lewis, Colleen M.},
  year = {2010},
  pages = {346},
  publisher = {ACM Press},
  address = {Milwaukee, Wisconsin, USA},
  doi = {10.1145/1734263.1734383},
  urldate = {2021-07-23},
  isbn = {978-1-4503-0006-3},
  langid = {english}
}

@book{likert1932,
  title = {A Technique for the Measurement of Attitudes},
  author = {Likert, Rensis},
  year = {1932},
  urldate = {2021-10-01},
  abstract = {Likert's original paper},
  copyright = {http://creativecommons.org/publicdomain/mark/1.0/},
  langid = {english},
  keywords = {likert scale}
}

@book{lin2012,
  title = {Comparing Two Data Visualization Tools: {{Tableau Public}} vs. {{Infogr}}.Am},
  author = {Lin, Mu},
  year = {2012},
  month = jun
}

@inproceedings{lincke2012,
  title = {The {{Lively PartsBin}}: {{A}} Cloud-Based Repository for Collaborative Development of Active Web Content},
  booktitle = {Proceedings of {{Collaboration Systems}} and {{Technology Track}} at the {{Hawaii International Conference}} on {{System Sciences}} ({{HICSS}})},
  author = {Lincke, Jens and Krahn, Robert and Ingalls, Dan and R{\"o}der, Marko and Hirshfeld, Robert},
  year = {2012}
}

@inproceedings{lipson2002,
  title = {The Role of Computer Based Technology in Developing Understanding of the Concept of Sampling Distribution},
  booktitle = {Research Papers from {{ICOTS}} 6},
  author = {Lipson, Kay},
  year = {2002},
  keywords = {computer science,education,statistics,technology}
}

@article{liustasko2010,
  title = {Mental {{Models}}, {{Visual Reasoning}} and {{Interaction}} in {{Information Visualization}}: {{A Top-down Perspective}}},
  shorttitle = {Mental {{Models}}, {{Visual Reasoning}} and {{Interaction}} in {{Information Visualization}}},
  author = {Liu, Zhicheng and Stasko, John},
  year = {2010},
  month = nov,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {16},
  number = {6},
  pages = {999--1008},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2010.177},
  abstract = {Although previous research has suggested that examining the interplay between internal and external representations can benefit our understanding of the role of information visualization (InfoVis) in human cognitive activities, there has been little work detailing the nature of internal representations, the relationship between internal and external representations and how interaction is related to these representations. In this paper, we identify and illustrate a specific kind of internal representation, mental models, and outline the high-level relationships between mental models and external visualizations. We present a top-down perspective of reasoning as model construction and simulation, and discuss the role of visualization in model based reasoning. From this perspective, interaction can be understood as active modeling for three primary purposes: external anchoring, information foraging, and cognitive offloading. Finally we discuss the implications of our approach for design, evaluation and theory development},
  keywords = {Brain modeling,Cognition,Cognitive science,Computational modeling,Data visualization,distributed cognition,Humans,information visualization,interaction,mental model,model-based reasoning,theory,Visualization},
  file = {/Users/amcnamara/Zotero/storage/FFNBBCL8/Liu and Stasko - 2010 - Mental Models, Visual Reasoning and Interaction in.pdf}
}

@inproceedings{lock2010,
  title = {Real-Life Module Statistics: {{A}} Happy {{HHarvard}} Experiment},
  booktitle = {{{ICOTS8}}},
  author = {Lock, Kari and Meng, Xiao-Li},
  year = {2010},
  abstract = {Five years ago, a discussion ensued over wine about how to make learning statistics a ``happy'' experience. This turned into many discussions over dinners and wine, and the formation of the ``happy team'': a team of faculty and grad students dedicated to creating the course ``Real-Life Statistics: Your Chance for Happiness (or Misery)''. The course is module based, featuring modules such as ``Romance'', ``Wine and Chocolate'', ``Finance'', ``Medical'', and more. We've taught this course at Harvard three times; twice as a second level course and once with no prerequisites. Here we discuss the team approach to creating a course, the module approach to teaching statistics, and the happiness (and misery) involved both for us and our students.},
  keywords = {to read}
}

@techreport{lock2012,
  title = {Overview of {{Statistics}}: {{UnLlocking}} the {{Power}} of {{Data}}},
  author = {Lock, Robin H. and Lock, Patti Frazer and Morgan, Kari Lock and Lock, Eric F. and Lock, Dennis F.},
  year = {2012},
  institution = {{John Wiley and Sons}}
}

@book{lock2012a,
  title = {Statistics: {{Unlocking}} the {{Power}} of {{Data}}},
  author = {Lock, Patti Frazer and Lock, Robin H. and Lock, Dennis F. and Morgan, Kari Lock and Lock, Eric F.},
  year = {2012},
  publisher = {Wiley}
}

@inproceedings{locker2008,
  title = {Pedagogical Strategies for Teaching Digital Media},
  booktitle = {{{NICOGRAPH}} '08},
  author = {Locker, Martin},
  year = {2008}
}

@book{locketal2017,
  title = {Statistics: {{Unlocking}} the {{Power}} of {{Data}}, 2nd {{Edition}}},
  shorttitle = {Statistics},
  author = {Lock, Robin H. and Lock, Patti Frazer and Morgan, Kari Lock and Lock, Eric F. and Lock, Dennis F.},
  year = {2017},
  publisher = {Wiley},
  urldate = {2021-09-27},
  abstract = {Statistics: Unlocking the Power of Data, 2nd Edition continues to utilize these intuitive methods like randomization and bootstrap intervals to introduce the fundamental idea of statistical inference. These methods are brought to life through authentically relevant examples, enabled through easy to use statistical software, and are accessible at very early stages of a course. The program includes the more traditional methods like t-tests, chi-square texts, etc. but only after students have developed a strong intuitive understanding of inference through randomization methods. The focus throughout is on data analysis and the primary goal is to enable students to effectively collect data, analyze data, and interpret conclusions drawn from data. The program is driven by real data and real applications.},
  langid = {american},
  file = {/Users/amcnamara/Zotero/storage/C3IM95B9/Statistics+Unlocking+the+Power+of+Data,+2nd+Edition-p-9781119308843.html}
}

@book{locketal2020,
  title = {Statistics: {{Unlocking}} the Power of Data},
  shorttitle = {Statistics},
  author = {Lock, Robin H. and Lock, Patti Frazer and Morgan, Kari Lock and Lock, Eric F. and Lock, Dennis F.},
  year = {2020},
  edition = {Third},
  publisher = {Wiley},
  address = {Hoboken},
  abstract = {"This book takes a unique approach of utilizing computer simulation methods to introduce students to the key ideas of statistical inference.Methods such as bootstrap intervals and randomization tests are very intuitive to novice students and capitalize on visual learning skills students bring to the classroom. With proper use of computer support, they are accessible at very early stages of a course with little formal background. Our text introduces statistical inference through these resampling and randomization methods, not only because these methods are becoming increasingly important for statisticians in their own right but also because they are outstanding in building students' conceptual understanding of the key ideas"--},
  isbn = {978-1-119-67416-0 978-1-119-68221-9},
  lccn = {QA276.12},
  keywords = {Statistics,Textbooks}
}

@article{loecher2015,
  title = {{{RgoogleMaps}} and Loa: {{Unleashing R}} Graphics Power on Map Tiles},
  author = {Loecher, Markus and Ropkins, Karl},
  year = {2015},
  journal = {Journal of Statistical Software},
  volume = {63},
  number = {4},
  keywords = {to read}
}

@article{lohr2009,
  title = {For {{Today}}'s Graduate, Just One Word: {{Statistics}}},
  author = {Lohr, Steve},
  year = {2009},
  month = aug,
  journal = {New York Times},
  keywords = {education,statistics}
}

@article{lohr2009a,
  title = {For {{Today}}'s {{Graduate}}, {{Just One Word}}: {{Statistics}}},
  shorttitle = {For {{Today}}'s {{Graduate}}, {{Just One Word}}},
  author = {Lohr, Steve},
  year = {2009},
  month = aug,
  journal = {The New York Times},
  issn = {0362-4331},
  urldate = {2021-06-24},
  abstract = {With the explosion of digital data, statisticians can earn \$125,000 in their first year after getting a doctorate.},
  chapter = {Technology},
  langid = {american},
  keywords = {Computers and the Internet,Google Inc,Mathematics,Research,Statistics},
  file = {/Users/amcnamara/Zotero/storage/IB67WFJS/06stats.html}
}

@book{lovett2007,
  title = {Thinking with {{Data}}},
  editor = {Lovett, Marsha C. and Shah, Priti},
  year = {2007},
  publisher = {Lawrence Erlbaum Associates},
  keywords = {statistics}
}

@article{lovettgreenhouse2000,
  title = {Applying {{Cognitive Theory}} to {{Statistics Instruction}}},
  author = {Lovett, Marsha C. and Greenhouse, Joel B.},
  year = {2000},
  month = aug,
  journal = {The American Statistician},
  volume = {54},
  number = {3},
  pages = {196--206},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2000.10474545},
  urldate = {2022-01-24},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/MLL5VRXT/Lovett and Greenhouse - 2000 - Applying Cognitive Theory to Statistics Instructio.pdf}
}

@article{lowndes2017,
  title = {Our Path to Better Science in Less Time Using Open Data Science Tools},
  author = {Lowndes, Julia Stewart and Best, Benjamin D. and Scarborough, Courtney and Afflerbach, Jamie C. and Frazier, Melanie R. and O'Hara, Casey C. and Jiang, Ning and Halpern, Benjamin S.},
  year = {2017},
  journal = {Nature Ecology \& Evolution},
  volume = {1},
  pages = {0160}
}

@book{lunneborg1999,
  title = {Data {{Analysis}} by {{Resampling}}},
  author = {Lunneborg, Clifford E.},
  year = {1999},
  publisher = {Cengage Learning},
  address = {Boston, MA},
  keywords = {to buy}
}

@article{lunzer2008,
  title = {Subjunctive {{Interfaces}}: {{Extending Applications}} to {{Support Parallel Setup}}, {{Viewing}} and {{Control}} of {{Alternative Scenarios}}},
  author = {Lunzer, Aran and Hornb{\ae}k, Kasper},
  year = {2008},
  journal = {ACM Transactions on Computer-Human Interaction},
  volume = {14},
  number = {4}
}

@inproceedings{lunzer2014,
  title = {It {{Ain}}'t {{Necessarily So}}: {{Checking Charts}} for {{Robustness}}},
  booktitle = {{{IEEE Vis}} 2014},
  author = {Lunzer, Aran and McNamara, Amelia},
  year = {2014}
}

@book{lunzer2014a,
  title = {{{LivelyR}} Introductory Demo},
  author = {Lunzer, Aran},
  year = {2014},
  month = may
}

@inproceedings{lunzer2014b,
  title = {{{LivelyR}}: {{Making R}} Charts Livelier},
  booktitle = {{{useR}}! {{Conference}}},
  author = {Lunzer, Aran and McNamara, Amelia and Krahn, Robert},
  year = {2014}
}

@book{lunzermcnamara2016,
  title = {Spatial Aggregation Explorer},
  author = {Lunzer, Aran and McNamara, Amelia},
  year = {2016}
}

@book{lunzermcnamara2017,
  title = {Exploring Histograms},
  author = {Lunzer, Aran and McNamara, Amelia},
  year = {2017}
}

@article{maceachren2005,
  title = {Visualizing Geospatial Information Uncertainty: {{What}} We Know and What We Need to Know},
  author = {MacEachren, Alan M. and Robinson, Anthony and Hopper, Susan and Gardner, Steven and Murray, Robert and Gahegan, Mark and Hetzler, Elisabeth},
  year = {2005},
  journal = {Cartography and Geographic Information Systems},
  volume = {32},
  number = {3},
  pages = {139--160},
  abstract = {Developing reliable methods for representing and managing information uncertainty remains a persistent and relevant challenge to GIScience. Information uncertainty is an intricate idea, and recent examinations of this concept have generated many perspectives on its representation and visualization, with perspectives emerging from a wide range of disciplines and application contexts. In this paper, we review and assess progress toward visual tools and methods to help analysts manage and understand information uncertainty. Specifically, we report on efforts to conceptualize uncertainty, decision making with uncertainty, frameworks for representing uncertainty, visual representation and user control of displays of information uncertainty, and evaluative efforts to assess the use and usability of visual displays of uncertainty. We conclude by identifying seven key research challenges in visualizing information uncertainty, particularly as it applies to decision making and analysis.}
}

@article{mackenzie2013,
  title = {Programming Subjects in the Regime of Anticipation: {{Software}} Studies and Subjectivity},
  shorttitle = {Programming Subjects in the Regime of Anticipation},
  author = {Mackenzie, Adrian},
  year = {2013},
  month = dec,
  journal = {Subjectivity},
  volume = {6},
  number = {4},
  pages = {391--405},
  issn = {1755-6341, 1755-635X},
  doi = {10.1057/sub.2013.12},
  urldate = {2022-03-30},
  langid = {english}
}

@article{mackinlay1986,
  title = {Automating the Design of Graphical Presentations of Relational Information},
  author = {Mackinlay, Jock D.},
  year = {1986},
  journal = {ACM Transactions on Graphics},
  volume = {5},
  number = {2},
  pages = {110--141},
  keywords = {to read}
}

@article{mackinlay2007,
  title = {Show Me: {{Automatic Presentation}} for {{Visual Analysis}}},
  author = {Mackinlay, Jock D. and Hanrahan, Pat and Stolte, Chris},
  year = {2007},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {13},
  number = {6},
  pages = {1137--1144},
  abstract = {This paper describes Show Me, an integrated set of user interface commands and defaults that incorporate automatic presentation into a commercial visual analysis system called Tableau. A key aspect of Tableau is VizQL, a language for specifying views, which is used by Show Me to extend automatic presentation to the generation of tables of views (commonly called small multiple displays). A key research issue for the commercial application of automatic presentation is the user experience, which must support the flow of visual analysis. User experience has not been the focus of previous research on automatic presentation. The Show Me user experience includes the automatic selection of mark types, a command to add a single field to a view, and a pair of commands to build views for multiple fields. Although the use of these defaults and commands is optional, user interface logs indicate that Show Me is used by commercial users.}
}

@book{maeda1999,
  title = {Design by {{Numbers}}},
  author = {Maeda, John},
  year = {1999},
  publisher = {The MIT Press},
  keywords = {computer science,data visualization}
}

@article{majumder2013,
  title = {Validation of Visual Statistical Inference, Applied to Linear Models},
  author = {Majumder, Mahbubul and Hofmann, Heike and Cook, Dianne},
  year = {2013},
  journal = {Journal of the American Statistical Association},
  volume = {108},
  number = {503},
  pages = {942--956},
  abstract = {Statistical graphics play a crucial role in exploratory data analysis, model checking, and diagnosis. The lineup protocol enables statistical significance testing of visual findings, bridging the gulf between exploratory and inferential statistics. In this article, inferential methods for statistical graphics are developed further by refining the terminology of visual inference and framing the lineup protocol in a context that allows direct comparison with conventional tests in scenarios when a conventional test exists. This framework is used to compare the performance of the lineup protocol against conventional statistical testing in the scenario of fitting linear models. A human subjects experiment is conducted using simulated data to provide controlled conditions. Results suggest that the lineup protocol performs comparably with the conventional tests, and expectedly outperforms them when data are contaminated, a scenario where assumptions required for performing a conventional test are violated. Surprisingly, visual tests have higher power than the conventional tests when the effect size is large. And, interestingly, there may be some super-visual individuals who yield better performance and power than the conventional test even in the most difficult tasks. Supplementary materials for this article are available online.}
}

@inproceedings{makar2010,
  title = {Teaching Primary Teachers to Teach Statistics: {{The}} Uniqueness of Initial Experiences},
  booktitle = {{{ICOTS-8}}},
  author = {Makar, Katie},
  year = {2010}
}

@inproceedings{makar2014,
  title = {Informal {{Statistical Inference Revisited}}},
  booktitle = {{{ICOTS-9}}},
  author = {Makar, Katie and Rubin, Andee},
  editor = {Makar, Katie and de Sousa, B. and Gould, Robert},
  year = {2014}
}

@article{makarrubin2009,
  title = {A Framework for Thinking about Informal Statistical Inference},
  author = {Makar, Katie and Rubin, Andee},
  year = {2009},
  month = may,
  journal = {Statistics Education Research Journal},
  volume = {8},
  number = {1},
  pages = {82--105},
  issn = {1570-1824},
  doi = {10.52041/serj.v8i1.457},
  urldate = {2023-07-10},
  abstract = {Informal inferential reasoning has shown some promise in developing students' deeper understanding of statistical processes. This paper presents a framework to think about three key principles of informal inference -- generalizations `beyond the data,' probabilistic language, and data as evidence. The authors use primary school classroom episodes and excerpts of interviews with the teachers to illustrate the framework and reiterate the importance of embedding statistical learning within the context of statistical inquiry. Implications for the teaching of more powerful statistical concepts at the primary school level are discussed. First published May 2009 at Statistics Education Research Journal: Archives},
  copyright = {Copyright (c) 2009 STATISTICS EDUCATION RESEARCH JOURNAL},
  langid = {english},
  keywords = {Teacher professional development},
  file = {/Users/amcnamara/Zotero/storage/R26AQUB3/Makar and Rubin - 2009 - A FRAMEWORK FOR THINKING ABOUT INFORMAL STATISTICA.pdf}
}

@inproceedings{malan2007,
  title = {Scratch for Budding Computer Scientists},
  booktitle = {{{SIGCSE}}'07},
  author = {Malan, David J. and Leitner, Henry H.},
  year = {2007},
  abstract = {Scratch is a ``media-rich programming environment'' recently developed by MIT's Media Lab that ``lets you create your own animations, games, and interactive art.'' Although Scratch is intended to ``enhance the development of technological fluency [among youths] at after-school centers in economically disadvantaged communities,'' we find remarkable potential in this pro- gramming environment for higher education as well. We propose Scratch as a first language for first-time programmers in introductory courses, for majors and non-majors alike. Scratch allows students to program with a mouse: programmatic constructs are represented as puzzle pieces that only fit together if ``syntactically'' appropriate. We argue that this environment allows students not only to master programmatic constructs be- fore syntax but also to focus on problems of logic before syntax. We view Scratch as a gateway to languages like Java. To validate our proposal, we recently deployed Scratch for the first time in higher education via Harvard Summer School's Computer Science S-1: Great Ideas in Computer Science, the summer- time version of a course at Harvard College. Our goal was not to improve scores but instead to improve first-time programmers' experiences. We ultimately transitioned to Java, but we first introduced programming itself via Scratch. We present in this paper the results of our trial. We find that, not only did Scratch excite students at a critical time (i.e., their first foray into computer science), it also familiarized the inexperienced among them with fundamentals of programming without the distraction of syntax. Moreover, when asked via surveys at term's end to reflect on how their initial experience with Scratch affected their subsequent experience with Java, most students (76\%) felt that Scratch was a positive influence, particularly those without prior background. Those stu- dents (16\%) who felt that Scratch was not an influence, positive or negative, all had prior programming experience.},
  keywords = {to read}
}

@article{mannes2013,
  title = {I Know {{I}}'m Right: {{A}} Behavioral View of Overconfidence},
  author = {Mannes, Albert and Moore, Don},
  year = {2013},
  month = aug,
  journal = {Significance}
}

@article{mannila2006,
  title = {What about a Simple Language? {{Analyzing}} the Difficulties in Learning to Program},
  shorttitle = {What about a Simple Language?},
  author = {Mannila, Linda and Peltom{\"a}ki, Mia and Salakoski, Tapio},
  year = {2006},
  month = sep,
  journal = {Computer Science Education},
  volume = {16},
  number = {3},
  pages = {211--227},
  issn = {0899-3408, 1744-5175},
  doi = {10.1080/08993400600912384},
  urldate = {2021-06-11},
  langid = {english}
}

@article{marchak1991,
  title = {Interactive versus Passive Dynamics and the Exploratory Analysis of Multivariate Data},
  author = {Marchak, Frank M. and Marchak, Lori Case},
  year = {1991},
  journal = {Behavior Research Methods, Instruments \& Computers},
  volume = {23},
  number = {2},
  pages = {296--300},
  abstract = {In two experiments, we examined subjects' ability to infer structure from three-dimensional data presented dynamically in interactive versus passive mode and in static scatterplots. Accuracy of estimating the number of clusters in three-dimensional data sets was measured for each of these conditions, as a function of cluster discriminability.}
}

@book{margolis2001,
  title = {Unlocking the Clubhouse: Women in Computing},
  author = {Margolis, Jane and Fisher, Allan},
  year = {2001},
  publisher = {The MIT Press}
}

@book{margolis2008,
  title = {Stuck in the {{Shallow End}}: {{Education}}, {{Race}} and {{Computing}}},
  author = {Margolis, Jane and Estrella, Rachel and Goode, Joanna and Holme, Jennifer Jellison and Nao, Kimberly},
  year = {2008},
  publisher = {MIT Press},
  keywords = {computer science,education,equity}
}

@inproceedings{marron1998,
  title = {When Is a Feature Really There? The {{SiZer}} Approach},
  booktitle = {{{SPIE Conference}} on {{Automatic Target Recognition}}},
  author = {Marron, J. S. and Chaudhuri, P.},
  year = {1998},
  abstract = {Statistical smoothing methods are useful for finding important and non-obvious structure in data.However, some of the features discovered in this way can be spurious sampling artifacts.The SiZer approach (based on studying statistical Significance of ZERo crossings of smoothed estimates) to analyzing which visible features represent important underlying structures, is discussed.}
}

@article{marron2005,
  title = {{{SiZe}} for Smoothing Splines},
  author = {Marron, J. S. and Zhang, Jin-Ting},
  year = {2005},
  journal = {Journal of computational statistics},
  volume = {20},
  number = {3},
  pages = {481--502}
}

@misc{martin-anderson2014,
  title = {Building Precise Maps with {{Disser}}},
  author = {{Martin-Anderson}, Brandon},
  year = {2014},
  month = apr,
  howpublished = {http://conveyal.com/blog/2014/04/08/aggregate-disser/}
}

@inproceedings{mashhadi2011,
  title = {Quality Control for Real-Time Ubiquitous Crowdsourcing},
  booktitle = {{{UbiCrowd}}'11},
  author = {Mashhadi, Afra J. and Capra, Licia},
  year = {2011},
  abstract = {Crowdsourcing has become a successful paradigm in the past decade, as Web 2.0 users have taken a more active role in producing content as well as consuming it. Recently this paradigm has broadened to incorporate ubiquitous applications, in which the smart-phone users contribute information about their surrounding, thus providing a collective knowledge about the physical world. However the acceptance and openness of such applications has made it easy to contribute poor quality content. Various solutions have been proposed for the Web-based domain, to assist with monitoring and filtering poor quality content, but these methods fall short when applied to ubiquitous crowdsourcing, where the task of collecting information has to be performed continuously and in real-time, by an always changing crowd. In this paper we discuss the challenges for quality control in ubiquitous crowdsourcing and propose a novel technique that reasons on users mobility patterns and quality of their past contributions to estimate user's credibility.}
}

@article{mathews2013,
  title = {Getting Students Excited about Data Analysis},
  author = {Mathews, Susann M. and Reed, Michelle and Angel, Nancy},
  year = {2013},
  journal = {Ohio Journal of School Mathematics}
}

@article{mattu2016,
  title = {Machine {{Bias}}},
  author = {Mattu, Jeff Larson,Lauren Kirchner,Surya, Julia Angwin},
  year = {2016},
  month = may,
  journal = {ProPublica},
  urldate = {2023-07-17},
  abstract = {There's software used across the country to predict future criminals. And it's biased against blacks.},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/E3R72P33/machine-bias-risk-assessments-in-criminal-sentencing.html}
}

@inproceedings{mayretal2016,
  title = {Looking at the {{Representations}} in Our {{Mind}}: {{Measuring Mental Models}} of {{Information Visualizations}}},
  shorttitle = {Looking at the {{Representations}} in Our {{Mind}}},
  booktitle = {Proceedings of the {{Sixth Workshop}} on {{Beyond Time}} and {{Errors}} on {{Novel Evaluation Methods}} for {{Visualization}}},
  author = {Mayr, Eva and Schreder, G{\"u}nther and Smuc, Michael and Windhager, Florian},
  year = {2016},
  month = oct,
  series = {{{BELIV}} '16},
  pages = {96--103},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2993901.2993914},
  urldate = {2023-07-10},
  abstract = {Users of information visualization systems build up internal representations of the displayed information and the system --mental models -- and constantly update them during interaction with the system. Though this theoretical approach was postulated as promising for information visualization, measures for empirical studies are missing. In this paper, we present different measures and evaluation procedures that have been developed for the assessment of mental models in other domains and discuss their suitability for the evaluation of internal and external representations in information visualization.},
  isbn = {978-1-4503-4818-8},
  keywords = {Evaluation,Information Visualization,Mental Models},
  file = {/Users/amcnamara/Zotero/storage/94V6G5I6/Mayr et al. - 2016 - Looking at the Representations in our Mind Measur.pdf}
}

@inproceedings{mccrackenetal2001,
  title = {A Multi-National, Multi-Institutional Study of Assessment of Programming Skills of First-Year {{CS}} Students},
  booktitle = {Working Group Reports from {{ITiCSE}} on {{Innovation}} and Technology in Computer Science Education},
  author = {McCracken, Michael and Almstrum, Vicki and Diaz, Danny and Guzdial, Mark and Hagan, Dianne and Kolikant, Yifat Ben-David and Laxer, Cary and Thomas, Lynda and Utting, Ian and Wilusz, Tadeusz},
  year = {2001},
  month = dec,
  series = {{{ITiCSE-WGR}} '01},
  pages = {125--180},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/572133.572137},
  urldate = {2022-07-21},
  abstract = {In computer science, an expected outcome of a student's education is programming skill. This working group investigated the programming competency students have as they complete their first one or two courses in computer science. In order to explore options for assessing students, the working group developed a trial assessment of whether students can program. The underlying goal of this work was to initiate dialog in the Computer Science community on how to develop these types of assessments. Several universities participated in our trial assessment and the disappointing results suggest that many students do not know how to program at the conclusion of their introductory courses. For a combined sample of 216 students from four universities, the average score was 22.89 out of 110 points on the general evaluation criteria developed for this study. From this trial assessment we developed a framework of expectations for first-year courses and suggestions for further work to develop more comprehensive assessments.},
  isbn = {978-1-4503-7359-3},
  file = {/Users/amcnamara/Zotero/storage/PQ846MM5/McCracken et al. - 2001 - A multi-national, multi-institutional study of ass.pdf}
}

@article{mccullough2004,
  title = {On the Accuracy of Statistical Procedures in {{Microsoft Excel}} 2003},
  author = {McCullough, B. D. and Wilson, Berry},
  year = {2004},
  journal = {Computational Statistics \& Data Analysis},
  volume = {49},
  number = {4},
  pages = {1244--1252}
}

@article{mccullough2008,
  title = {On the Accuracy of Statistical Procedures in {{Microsoft Excel}} 2007},
  author = {McCullough, B. D. and Heiser, David A.},
  year = {2008},
  journal = {Computational Statistics \& Data Analysis},
  volume = {52},
  pages = {4570--4578},
  abstract = {Excel 2007, like its predecessors, fails a standard set of intermediate-level accuracy tests in three areas: statistical distributions, random number generation, and estimation. Additional errors in specific Excel procedures are discussed. Microsoft's continuing inability to correctly fix errors is discussed. No statistical procedure in Excel should be used until Microsoft documents that the procedure is correct; it is not safe to assume that Microsoft Excel's statistical procedures give the correct answer. Persons who wish to conduct statistical analyses should use some other package.}
}

@article{mcgowanetal2021,
  title = {Design {{Principles}} for {{Data Analysis}}},
  author = {McGowan, Lucy D'Agostino and Peng, Roger D. and Hicks, Stephanie C.},
  year = {2021},
  month = mar,
  journal = {arXiv:2103.05689 [stat]},
  eprint = {2103.05689},
  primaryclass = {stat},
  urldate = {2021-07-18},
  abstract = {The data science revolution has led to an increased interest in the practice of data analysis. While much has been written about statistical thinking, a complementary form of thinking that appears in the practice of data analysis is design thinking -- the problem-solving process to understand the people for whom a product is being designed. For a given problem, there can be significant or subtle differences in how a data analyst (or producer of a data analysis) constructs, creates, or designs a data analysis, including differences in the choice of methods, tooling, and workflow. These choices can affect the data analysis products themselves and the experience of the consumer of the data analysis. Therefore, the role of a producer can be thought of as designing the data analysis with a set of design principles. Here, we introduce design principles for data analysis and describe how they can be mapped to data analyses in a quantitative, objective and informative manner. We also provide empirical evidence of variation of principles within and between both producers and consumers of data analyses. Our work leads to two insights: it suggests a formal mechanism to describe data analyses based on the design principles for data analysis, and it provides a framework to teach students how to build data analyses using formal design principles.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Applications,Statistics - Methodology,Statistics - Other Statistics},
  file = {/Users/amcnamara/Zotero/storage/CRGUESDA/McGowan et al. - 2021 - Design Principles for Data Analysis.pdf;/Users/amcnamara/Zotero/storage/8IWWRSGM/2103.html}
}

@inproceedings{mciver1996,
  title = {Seven Deadly Sins of Introductory Programming Language Design},
  booktitle = {Proceedings 1996 {{International Conference Software Engineering}}: {{Education}} and {{Practice}}},
  author = {McIver, L. and Conway, D.},
  year = {1996},
  pages = {309--316},
  publisher = {IEEE Comput. Soc. Press},
  address = {Dunedin, New Zealand},
  doi = {10.1109/SEEP.1996.534015},
  urldate = {2021-06-11},
  isbn = {978-0-8186-7379-5}
}

@article{mckenna2014,
  title = {Design Activity Framework for Visualization Design},
  author = {McKenna, Sean and Mazur, Dominika and Agutter, James and Meyer, Miriah},
  year = {2014},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {20},
  number = {12},
  abstract = {An important aspect in visualization design is the connection between what a designer does and the decisions the designer makes. Existing design process models, however, do not explicitly link back to models for visualization design decisions. We bridge this gap by introducing the design activity framework, a process model that explicitly connects to the nested model, a well- known visualization design decision model. The framework includes four overlapping activities that characterize the design process, with each activity explicating outcomes related to the nested model. Additionally, we describe and characterize a list of exemplar methods and how they overlap among these activities. The design activity framework is the result of reflective discussions from a collaboration on a visualization redesign project, the details of which we describe to ground the framework in a real-world design process. Lastly, from this redesign project we provide several research outcomes in the domain of cybersecurity, including an extended data abstraction and rich opportunities for future visualization research.},
  keywords = {IEEEviz,theoretical}
}

@inproceedings{mckinney2010,
  title = {Data Structures for Statistical Computing in Python},
  booktitle = {Proceedings of the 9th {{Python}} in {{Science Conference}}},
  author = {McKinney, Wes},
  year = {2010},
  volume = {445},
  pages = {51--56},
  publisher = {Austin, TX},
  file = {/Users/amcnamara/Zotero/storage/QC3FYMP8/McKinney - 2010 - Data structures for statistical computing in pytho.pdf}
}

@book{mckinney2012,
  title = {Python for Data Analysis: {{Data}} Wrangling with {{Pandas}}, {{NumPy}}, and {{iPython}}},
  author = {McKinney, Wes},
  year = {2012},
  publisher = {O'Reilly}
}

@unpublished{mcnamara,
  title = {{{AmeliaMN GitHub}} Profile},
  author = {McNamara, Amelia}
}

@book{mcnamara2013,
  title = {Mobilize {{Wiki}}: {{RStudio}}},
  author = {McNamara, Amelia},
  year = {2013}
}

@book{mcnamara2013a,
  title = {{{MobilizeSimple}}},
  author = {McNamara, Amelia},
  year = {2013},
  annotation = {Published: Github repository, https://github.com/mobilizingcs/MobilizeSimple}
}

@book{mcnamara2014,
  title = {Kid {{Pix}}},
  author = {McNamara, Amelia},
  year = {2014},
  month = oct
}

@phdthesis{mcnamara2015,
  title = {Bridging the {{Gap Between Tools}} for {{Learning}} and for {{Doing Statistics}}},
  author = {McNamara, Amelia},
  year = {2015},
  month = jun,
  school = {University of California, Los Angeles}
}

@unpublished{mcnamara2015a,
  title = {Community Engagement and Subgroup Meta-Knowledge: {{Some}} Factors in the Soul of a Community [{{Reproducible}} Materials]},
  author = {McNamara, Amelia},
  year = {2015}
}

@book{mcnamara2016,
  title = {Do You Know {{Nothing}} When You See It?},
  author = {McNamara, Amelia},
  year = {2016},
  month = apr,
  annotation = {Published: In OpenVisConf, https://www.youtube.com/watch?v=hps9r7JZQP8}
}

@inproceedings{mcnamara2016a,
  title = {Integrated {{R}} Labs for High School Students},
  booktitle = {{{useR}}! {{Conference}}},
  author = {McNamara, Amelia and Molyneux, James and Johnson, Terri},
  year = {2016}
}

@unpublished{mcnamara2016b,
  title = {On the State of Computing in Statistics Education},
  author = {McNamara, Amelia},
  year = {2016},
  journal = {Technology Innovations in Statistics Education}
}

@inproceedings{mcnamara2016c,
  title = {The Importance of Parameter Manipulation for Statistics},
  booktitle = {Society for {{Industrial}} and {{Applied Mathematics}} ({{SIAM}}) Annual Meeting},
  author = {McNamara, Amelia},
  year = {2016}
}

@misc{mcnamara2017,
  title = {{{ENAR}} Workshop: {{Data Science}} for {{Statisticians}}},
  author = {McNamara, Amelia},
  year = {2017}
}

@book{mcnamara2017a,
  title = {How Spatial Polygons Shape Our World},
  author = {McNamara, Amelia},
  year = {2017},
  month = apr,
  annotation = {Published: In OpenVisConf, https://www.youtube.com/watch?v=wn5larsRHro}
}

@book{mcnamara2018,
  title = {R {{Syntax Comparison Cheatsheet}}},
  author = {McNamara, Amelia},
  year = {2018}
}

@article{mcnamara2018a,
  title = {R {{Syntax Comparison Cheatsheet}}},
  author = {McNamara, Amelia},
  year = {2018},
  publisher = {Open Science Framework},
  doi = {10.17605/OSF.IO/2K8FW},
  urldate = {2021-07-07},
  abstract = {Comparative cheatsheet for the three most common R syntaxes},
  collaborator = {McNamara, Amelia}
}

@article{mcnamara2019,
  title = {Community Engagement and Subgroup Meta-Knowledge: {{Some}} Factors in the Soul of a Community},
  author = {McNamara, Amelia},
  year = {2019},
  journal = {Computational Statistics},
  volume = {34},
  number = {4}
}

@article{mcnamara2019a,
  title = {Key {{Attributes}} of a {{Modern Statistical Computing Tool}}},
  author = {McNamara, Amelia},
  year = {2019},
  journal = {The American Statistician},
  volume = {73},
  number = {4}
}

@misc{mcnamara2020,
  title = {Speaking {{R}}, {{useR}}! {{The International R Users Conference}}. {{Keynote}} Presentation},
  author = {McNamara, Amelia},
  year = {2020},
  month = jul
}

@article{mcnamara2021,
  title = {Reading {{R}} Code for "{{An}} Educator's Perspective of the Tidyverse"},
  author = {McNamara, Amelia},
  year = {2021},
  month = jul,
  publisher = {OSF},
  urldate = {2021-07-27},
  abstract = {Hosted on the Open Science Framework},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/D8I6V9EY/r8mez.html}
}

@article{mcnamara2022,
  title = {Teaching Modeling in Introductory Statistics: {{A}} Comparison of Formula and Tidyverse Syntaxes},
  shorttitle = {Teaching Modeling in Introductory Statistics},
  author = {McNamara, Amelia},
  year = {2022},
  month = jan,
  journal = {arXiv:2201.12960 [stat]},
  eprint = {2201.12960},
  primaryclass = {stat},
  urldate = {2022-02-14},
  abstract = {This paper reports on an experiment run in a pair of introductory statistics labs, attempting to determine which of two R syntaxes was better for introductory teaching and learning: formula or tidyverse. One lab was conducted fully in the formula syntax, the other in tidyverse. Analysis of incidental data from YouTube and RStudio Cloud show interesting distinctions. The formula section appeared to watch a larger proportion of pre-lab YouTube videos, but spend less time computing on RStudio Cloud. Conversely, the tidyverse section watched a smaller proportion of the videos and spent more time on RStudio Cloud. Analysis of lab materials showed that tidyverse labs tended to be slightly longer (in terms of lines in the provided RMarkdown materials, as well as minutes of the associated YouTube videos), and the tidyverse labs exposed students to more distinct R functions. However, both labs relied on a quite small vocabulary of consistent functions. Analysis of pre- and post-survey data show no differences between the two labs, so students appeared to have a positive experience regardless of section. This work provides additional evidence for instructors looking to choose between syntaxes for introductory statistics teaching.},
  archiveprefix = {arXiv},
  keywords = {Statistics - Other Statistics},
  file = {/Users/amcnamara/Zotero/storage/TST6PGQT/McNamara - 2022 - Teaching modeling in introductory statistics A co.pdf;/Users/amcnamara/Zotero/storage/4M5UWP3P/2201.html}
}

@misc{mcnamara2022a,
  title = {{{STAT}} 220 Labs (Formula and Tidyverse Versions)},
  author = {McNamara, Amelia},
  year = {2022},
  month = nov,
  urldate = {2023-01-11},
  abstract = {Pre-lab exercises for STAT 220 labs at the University of St Thomas},
  copyright = {CC-BY-SA-4.0}
}

@article{mcnamaraetal2017,
  title = {Greater {{Data Science}} at {{Baccalaureate Institutions}}},
  author = {McNamara, Amelia and Horton, Nicholas J. and Baumer, Benjamin S.},
  year = {2017},
  month = oct,
  journal = {Journal of Computational and Graphical Statistics},
  volume = {26},
  number = {4},
  pages = {781--783},
  issn = {1061-8600, 1537-2715},
  doi = {10.1080/10618600.2017.1386568},
  urldate = {2021-06-24},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/5ZIEQGAH/McNamara et al. - 2017 - Greater Data Science at Baccalaureate Institutions.pdf}
}

@manual{mcnamaraetal2018,
  type = {manual},
  title = {\texttt{skimr}: Compact and Flexible Summaries of Data},
  author = {McNamara, Amelia and Rubia, Eduardo Arino de la and Zhu, Hao and Lowndes, Julia and Ellis, Shannon and Waring, Elin and Quinn, Michael and McLeod, Hope and Wickham, Hadley and Müller, Kirill and RStudio Inc. (Spark functions) and Kirkpatrick, Connor and Brenstuhl, Scott},
  year = {2018},
  month = jan,
  urldate = {2023-01-13},
  abstract = {A simple to use summary function that can be used with pipes and displays nicely in the console. The default summary statistics may be modified by the user as can the default formatting. Support for data frames and vectors is included, and users can implement their own skim methods for specific object types as described in a vignette. Default summaries include support for inline spark graphs. Instructions for managing these on specific operating systems are given in the "Using skimr" vignette and the README.},
  langid = {english}
}

@inproceedings{mcnamaraetal2021,
  title = {Computing in the {{Statistics Curriculum}}: {{Lessons Learned}} from the {{Educational Sciences}}},
  booktitle = {{{USCOTS}} 2021},
  author = {McNamara, Amelia and Zieffler, Andrew and Beckman, Matthew and Legacy, Chelsey and Butler Basner, Elle and {delMas}, Robert C. and Rao, V.N. Vimal},
  year = {2021}
}

@misc{mcnamaraetal2021a,
  title = {Computing in the {{Statistics Curriculum}}: {{Lessons Learned}} from the {{Educational Sciences}}},
  author = {McNamara, Amelia and Zieffler, Andrew and Beckman, Matthew and Legacy, Chelsey and Butler Basner, Elle and {delMas}, Robert and Rao, V.N. Vimal},
  year = {2021},
  month = aug
}

@inproceedings{mcnamarahansen2014,
  title = {Teaching {{Data Science}} to {{Teenagers}}},
  booktitle = {Sustainability in Statistics Education. {{Proceedings}} of the 9th {{International Conference}} on {{Teaching Statistics}} ({{ICOTS9}})},
  author = {McNamara, Amelia and Hansen, Mark},
  year = {2014}
}

@article{mcnamarahorton2017,
  title = {Wrangling Categorical Data in {{R}}},
  author = {McNamara, Amelia and Horton, Nicholas J.},
  year = {2017},
  journal = {The American Statistician},
  volume = {72},
  number = {1}
}

@misc{mcnamarakessler2020,
  title = {{{NICAR}} Workshop: {{Exploring}} the {{Tidyverse}} in {{R}}},
  author = {McNamara, Amelia and Kessler, Aaron},
  year = {2020}
}

@inproceedings{mcnamaramolyneux2014,
  title = {Teaching {{R}} to {{High School Students}}},
  booktitle = {{{useR}}! {{The International R Users Conference}}},
  author = {McNamara, Amelia and Molyneux, James},
  year = {2014}
}

@misc{mcnamarawickham2020,
  title = {rstudio::conf Workshop: Introduction to Data Science in the tidyverse},
  author = {McNamara, Amelia and Wickham, Hadley},
  year = {2020},
  month = jan
}

@techreport{means2011,
  title = {Teachers' Ability to Use Data to Inform Instruction: {{Challenges}} and Supports},
  author = {Means, Barbara and Chen, Eva and DeBarger, Angela and Padilla, Christine},
  year = {2011},
  institution = {U.S. Department of Education}
}

@book{medialeaders2014,
  title = {Kid {{Pix Paint Program}} Went from {{Idea}} to {{Mass Success}} with {{Craig Hickman}}},
  author = {{Media Leaders}},
  year = {2014},
  month = aug
}

@inproceedings{meirelles2011,
  title = {Visualizing Data: {{New}} Pedagogical Challenges},
  booktitle = {Selected {{Readings}} of the 4th {{Information Design International Conference}}},
  author = {Meirelles, Isabel},
  editor = {Spinillo, Farias and {Padovani}},
  year = {2011},
  abstract = {The paper examines the burgeoning practice of visualizing data. It begins with a brief overview of this broad field and the nature of the practice throughout history. The focus is on computational interactive visualizations and the ways in which technology has given way to an expansive and expanding practice mainly centered on current issues. Information visualizations are ubiquitous and critically important to understanding several fields today, covering a wide range of content and functionality: from scientific visualizations to visual explanations of socio-political events. Technology has affected the practice in several ways, from graphical methods to the agents involved with such complex data representations: the authors and users of these applications. Selected graphical tools are examined as a means to identify recent trends. The paper concludes with questioning the ways in which we are (or not) preparing design students to tackle these new information communication challenges. The goal is to discuss ---and ultimately suggest--- the relevance of integrating theoretical,visual and technical aspects of structuring and representing large amounts of data into design undergraduate education.},
  keywords = {data visualization,education}
}

@article{melard2014,
  title = {On the Accuracy of Statistical Procedures in {{Microsoft Excel}} 2010},
  author = {Melard, Guy},
  year = {2014},
  journal = {Computational Statistics},
  volume = {29},
  number = {1095}
}

@inproceedings{mendez2012,
  title = {Density {{Maps}}: {{Determining}} Where to Sample in Participatory Sensing Systems},
  booktitle = {2012 {{Third FTRA International Conference}} on {{Mobile}}, {{Ubiquitous}}, and {{Intelligent Computing}}},
  author = {Mendez, Diego and Labrador, Miguel A.},
  year = {2012},
  publisher = {IEEE},
  abstract = {Participatory sensing (PS) systems are a new emerginterativeing sensing paradigm based on the participation of cellular users. While they present interesting characteristics, such as spatiotemporal granularity and low cost, they also create new problems and challenges. One key challenge in PS systems is that of the determination of the locations and number of users where to obtain samples from so that we can accurately represent the variable of interest with a low number of participants. This paper proposes the use of density maps, based on the current estimations of the variable, to address this challenge. The density maps are then utilized by the incentive mechanism in order to encourage the participation of those users indicated in the map. Our results show how the density maps greatly improve the quality of the estimations while maintaining a stable and low total number of users in the system.}
}

@article{mendez2013,
  title = {Data {{Interpolation}} for Participatory Sensing Systems},
  author = {Mendez, D. and Labrador, M. and Ramachandran, K.},
  year = {2013},
  journal = {Pervasive and Mobile Computing},
  volume = {9},
  pages = {123--148},
  abstract = {In this paper, we study the problem of applying data interpolation techniques in Participatory Sensing (PS) systems using an air quality/pollution monitoring application as an example. While traditional environmental monitoring systems consist of very few static measuring stations, PS systems rely on the participation of many mobile stations. As a result, the structure of the data provided by each system is different and instead of a multivariate time series with a few gaps in the same space, now we have a multivariate time-space series with many gaps in time and space. First, two data interpolation techniques, Markov Random Fields and kriging, are analyzed. After showing the trade-offs and superiority of kriging, this technique is used to perform a one-variable data interpolation. Then, the problems of cokriging for multivariate interpolation are introduced and Principal Component Analysis and Independent Component Analysis are utilized along with kriging to overcome these problems. Finally, an alternative approach to interpolate data in time and space is proposed, which is really useful for PS systems. The results indicate that the accuracy of the estimates improves with the amount of data, i.e., one variable, multiple variables, and space and time data. Also, the results clearly show the advantage of a PS system compared with a traditional measuring system in terms of the precision and granularity of the information provided to the users.}
}

@article{menezes2015,
  title = {Data Science Class Offers {{L}}.{{A}}. {{Unified}} Students a New Handle on Math},
  author = {Menezes, Ryan},
  year = {2015},
  month = apr,
  journal = {Los Angeles Times}
}

@article{mennis2009,
  title = {Spatial Data Mining and Geographic Knowledge Discovery--{{An}} Introduction},
  author = {Mennis, Jeremy and Guo, Diansheng},
  year = {2009},
  journal = {Computers, Environment and Urban Systems},
  volume = {33},
  pages = {403--408},
  keywords = {geographic knowledge discovery,spatial data mining,statistics}
}

@inproceedings{meyerovich2012,
  title = {Socio-{{PLT}}: {{Principles}} for {{Programming Language Adoption}}},
  booktitle = {Proceedings of the {{ACM}} International Symposium on {{New}} Ideas, New Paradigms, and Reflections on Programming and Software},
  author = {Meyerovich, Leo A. and Rabkin, Ariel},
  year = {2012},
  publisher = {ACM},
  abstract = {Why do some programming languages fail and others succeed? What does the answer tell us about programming language design, implementation, and principles? To help answer these and other questions, we argue for a sociologically- grounded programming language theory: socio-PLT. Researchers in the social sciences have studied adoption in many contexts. We show how their findings are applicable to programming language design. For example, many programming language features provide benefits that programmers cannot directly or immediately observe and therefore may not find compelling. From clean water to safe sex, the health community has long examined how to surmount similar observability barriers. We use such results from outside of programming language theory to frame a research agenda that should help us understand the social foundations of languages. Finally, we examine implications of our approach, such as for the design space of language features and the assessment of scientific research into programming languages.}
}

@inproceedings{meyerovich2013,
  title = {Empirical {{Analysis}} of {{Programming Language Adoption}}},
  booktitle = {{{OOPSLA}}'13},
  author = {Meyerovich, Leo A. and Rabkin, Ariel},
  year = {2013},
  abstract = {Some programming languages become widely popular while others fail to grow beyond their niche or disappear altogether. This paper uses survey methodology to identify the factors that lead to language adoption. We analyze large datasets, including over 200,000 SourceForge projects, 590,000 projects tracked by Ohloh, and multiple surveys of 1,000-13,000 programmers. We report several prominent findings. First, language adoption follows a power law; a small number of languages account for most language use, but the programming mar- ket supports many languages with niche user bases. Second, intrinsic features have only secondary importance in adoption. Open source libraries, existing code, and experience strongly influence developers when selecting a language for a project. Language features such as performance, reliability, and simple semantics do not. Third, developers will steadily learn and forget languages, and the overall number of languages developers are familiar with is independent of age. Developers select more varied languages if their education exposed them to different language families. Finally, when considering intrinsic aspects of languages, developers prioritize expressivity over correctness. They perceive static types as more valuable for properties such as the former rather than for correctness checking.},
  keywords = {to read}
}

@article{miller1955,
  title = {The Magical Number Seven, plus or Minus Two: {{Some}} Limits on Our Capacity for Processing Information},
  author = {Miller, George A.},
  year = {1955},
  journal = {Psychological review},
  volume = {101},
  number = {2},
  pages = {343--352},
  keywords = {to read}
}

@book{miller2014,
  title = {Wizard: {{Data Analysis}} for the {{Non-Statistician}}},
  author = {Miller, Evan},
  year = {2014},
  annotation = {Published: Computer software, http://www.wizardmac.com/}
}

@article{mircioiuatkinson2017,
  title = {A {{Comparison}} of {{Parametric}} and {{Non-Parametric Methods Applied}} to a {{Likert Scale}}},
  author = {Mircioiu, Constantin and Atkinson, Jeffrey},
  year = {2017},
  month = jun,
  journal = {Pharmacy},
  volume = {5},
  number = {2},
  pages = {26},
  publisher = {Multidisciplinary Digital Publishing Institute},
  doi = {10.3390/pharmacy5020026},
  urldate = {2021-10-01},
  abstract = {A trenchant and passionate dispute over the use of parametric versus non-parametric methods for the analysis of Likert scale ordinal data has raged for the past eight decades. The answer is not a simple ``yes'' or ``no'' but is related to hypotheses, objectives, risks, and paradigms. In this paper, we took a pragmatic approach. We applied both types of methods to the analysis of actual Likert data on responses from different professional subgroups of European pharmacists regarding competencies for practice. Results obtained show that with ``large'' (\&gt;15) numbers of responses and similar (but clearly not normal) distributions from different subgroups, parametric and non-parametric analyses give in almost all cases the same significant or non-significant results for inter-subgroup comparisons. Parametric methods were more discriminant in the cases of non-similar conclusions. Considering that the largest differences in opinions occurred in the upper part of the 4-point Likert scale (ranks 3 ``very important'' and 4 ``essential''), a ``score analysis'' based on this part of the data was undertaken. This transformation of the ordinal Likert data into binary scores produced a graphical representation that was visually easier to understand as differences were accentuated. In conclusion, in this case of Likert ordinal data with high response rates, restraining the analysis to non-parametric methods leads to a loss of information. The addition of parametric methods, graphical analysis, analysis of subsets, and transformation of data leads to more in-depth analyses.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {Likert,non-parametric,parametric,ranking,scores},
  file = {/Users/amcnamara/Zotero/storage/FMEI5J7H/Mircioiu and Atkinson - 2017 - A Comparison of Parametric and Non-Parametric Meth.pdf;/Users/amcnamara/Zotero/storage/MA7XGG6U/htm.html}
}

@misc{mobilizingcs,
  title = {Mobilize},
  howpublished = {http://www.mobilizingcs.org/}
}

@book{molyneuxetal2014,
  title = {Introduction to {{Data Science Labs}}},
  author = {Molyneux, James and Johnson, Terri and McNamara, Amelia},
  year = {2014},
  month = dec
}

@book{monmonier1996,
  title = {How to {{Lie}} with {{Maps}}},
  author = {Monmonier, Mark},
  year = {1996},
  publisher = {University of Chicago Press}
}

@incollection{moore1990,
  title = {Uncertainty},
  booktitle = {On the Shoulders of Giants: New Approaches to Numeracy},
  author = {Moore, David S.},
  editor = {Steen, Lynn Arthur and National Research Council (U.S.)},
  year = {1990},
  publisher = {National Academy Press},
  address = {Washington, D.C},
  isbn = {978-0-309-04234-5},
  langid = {english},
  lccn = {QA13 .O53 1990},
  keywords = {Mathematics,Study and teaching,United States},
  file = {/Users/amcnamara/Zotero/storage/DWV2NYM4/Steen and National Research Council (U.S.) - 1990 - On the shoulders of giants new approaches to nume.pdf}
}

@inproceedings{morandatetal2012,
  title = {Evaluating the {{Design}} of the {{R Language}}: {{Objects}} and {{Functions For Data Analysis}}},
  booktitle = {{{ECOOP}}'12 {{Proceedings}} of the 26th {{European}} Conference on {{Object-Oriented Programming}}},
  author = {Morandat, Flor{\'e}al and Hill, Brandon and Osvald, Leo and Vitek, Jan},
  year = {2012},
  abstract = {R is a dynamic language for statistical computing that combines lazy functional features and object-oriented programming. This rather unlikely linguistic cocktail would probably never have been prepared by computer scientists, yet the language has become surprisingly popular. With millions of lines of R code available in repositories, we have an opportunity to evaluate the fundamental choices underlying the R language design. Using a combination of static and dynamic program analysis we assess the success of different language features.}
}

@book{morgan2014,
  title = {{{StatKey}}: {{Online}} Tools for Bootstrap Intervals and Randomization Tests},
  author = {Morgan, Kari Lock and Lock, Robin H. and Lock, Patti Frazer and Lock, Eric F. and Lock, Dennis F.},
  year = {2014},
  annotation = {Published: ICOTS-9, http://www2.stat.duke.edu/ kfl5/Lock2014.pdf}
}

@inproceedings{morganetal2014,
  title = {{{StatKey}}: {{Online Tools}} for {{Bootstrap Intervals}} and {{Randomization Tests}}},
  booktitle = {Sustainability in Statistics Education. {{Proceedings}} of the 9th {{International Conference}} on {{Teaching Statistics}} ({{ICOTS9}})},
  author = {Morgan, Kari Lock and Lock, Robin H. and Lock, Patti Frazer and Lock, Eric F. and Lock, Dennis F.},
  year = {2014},
  month = jul,
  address = {Flagstaff, AZ}
}

@phdthesis{moritz2006,
  title = {Develop Students' Understandings and Representations of Statistical Covariation},
  author = {Moritz, Jonathan},
  year = {2006},
  school = {University of Tasmania},
  keywords = {to read}
}

@article{muhlbacher2014,
  title = {Opening the Black Box: {{Strategies}} for Increased User Involvement in Existing Algorithm Implementations},
  author = {M{\"u}hlbacher, Thomas and Piringer, Harald and Gratzl, Samuel and Sedlmair, Michael and Streit, Marc},
  year = {2014},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {20},
  number = {12},
  pages = {1643--1652},
  abstract = {An increasing number of interactive visualization tools stress the integration with computational software like MATLAB and R to access a variety of proven algorithms. In many cases, however, the algorithms are used as black boxes that run to completion in isolation which contradicts the needs of interactive data exploration. This paper structures, formalizes, and discusses possibilities to enable user involvement in ongoing computations. Based on a structured characterization of needs regarding intermediate feedback and control, the main contribution is a formalization and comparison of strategies for achieving user involvement for algorithms with different characteristics. In the context of integration, we describe considerations for implementing these strategies either as part of the visualization tool or as part of the algorithm, and we identify requirements and guidelines for the design of algorithmic APIs. To assess the practical applicability, we provide a survey of frequently used algorithm implementations within R regarding the fulfillment of these guidelines. While echoing previous calls for analysis modules which support data exploration more directly, we conclude that a range of pragmatic options for enabling user involvement in ongoing computations exists on both the visualization and algorithm side and should be used.},
  keywords = {to read}
}

@article{muller2014,
  title = {Debugging Geographers: {{Teaching}} Programming to Non-Computer Scientists},
  author = {Muller, Catherine L. and Kidd, Chris},
  year = {2014},
  journal = {Journal of Geography in Higher Education},
  volume = {38},
  number = {2},
  pages = {175--192},
  abstract = {The steep learning curve associated with computer programming can be a daunting prospect, particularly for those not well-aligned with this way of logical thinking. However, programming is a skill that is becoming increasingly important. Geography graduates entering careers in atmospheric science are one example of a particularly diverse group who often require a better knowledge and understanding of computing. Critically, there is a necessity in the field for people with a diverse range of data analysis and modeling abilities. This paper outlines the module design and evaluation of an introductory programming course for non-computer scientists within a UK geography department.},
  keywords = {to read}
}

@incollection{muller2014a,
  title = {Curiosity, {{Creativity}}, and {{Surprise}} as {{Analytic Tools}}: {{Grounded Theory Method}}},
  shorttitle = {Curiosity, {{Creativity}}, and {{Surprise}} as {{Analytic Tools}}},
  booktitle = {Ways of {{Knowing}} in {{HCI}}},
  author = {Muller, Michael},
  editor = {Olson, Judith S. and Kellogg, Wendy A.},
  year = {2014},
  pages = {25--48},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-1-4939-0378-8_2},
  urldate = {2023-07-10},
  abstract = {Grounded Theory Method offers a rigorous way to explore a domain, with an emphasis on discovering new insights, testing those insights, and building partial understandings into a broader theory of the domain. It begins with observations of a phenomenon for which no theory yet exists. Through layered coding of these observations and continual reexamination of the data, a theory emerges.},
  isbn = {978-1-4939-0378-8},
  langid = {english},
  keywords = {Axial Code,Constant Comparison,Core Concept,Ground Theory,Open Code}
}

@inproceedings{mulleretal2019,
  title = {How {{Data Science Workers Work}} with {{Data}}: {{Discovery}}, {{Capture}}, {{Curation}}, {{Design}}, {{Creation}}},
  shorttitle = {How {{Data Science Workers Work}} with {{Data}}},
  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Muller, Michael and Lange, Ingrid and Wang, Dakuo and Piorkowski, David and Tsay, Jason and Liao, Q. Vera and Dugan, Casey and Erickson, Thomas},
  year = {2019},
  month = may,
  series = {{{CHI}} '19},
  pages = {1--15},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3290605.3300356},
  urldate = {2023-07-10},
  abstract = {With the rise of big data, there has been an increasing need for practitioners in this space and an increasing opportunity for researchers to understand their workflows and design new tools to improve it. Data science is often described as data-driven, comprising unambiguous data and proceeding through regularized steps of analysis. However, this view focuses more on abstract processes, pipelines, and workflows, and less on how data science workers engage with the data. In this paper, we build on the work of other CSCW and HCI researchers in describing the ways that scientists, scholars, engineers, and others work with their data, through analyses of interviews with 21 data science professionals. We set five approaches to data along a dimension of interventions: Data as given; as captured; as curated; as designed; and as created. Data science workers develop an intuitive sense of their data and processes, and actively shape their data. We propose new ways to apply these interventions analytically, to make sense of the complex activities around data practices.},
  isbn = {978-1-4503-5970-2},
  keywords = {data capture,data creation,data curation,data design,data discovery,data science,grounded theory,work-practices},
  file = {/Users/amcnamara/Zotero/storage/KULVID3U/Muller et al. - 2019 - How Data Science Workers Work with Data Discovery.pdf}
}

@manual{mullerwalthert2022,
  type = {manual},
  title = {\texttt{styler}: Non-Invasive Pretty Printing of  {R} Code},
  author = {Müller, Kirill and Walthert, Lorenz},
  year = {2022}
}

@article{munson2010,
  title = {A Method for Measuring the Relative Information Content of Data from Different Monitoring Protocols},
  author = {Munson, M. Arthur and others},
  year = {2010},
  journal = {Methods in Ecology and Evolution},
  volume = {1},
  pages = {263--273},
  abstract = {1. Species monitoring is an essential component of assessing conservation status, predicting effects of habitat change and establishing management and conservation priorities. The pervasive access to the Internet has led to the development of several extensive monitoring projects that engage massive networks of volunteers who provide observations following relatively unstructured protocols. How- ever, the value of these data is largely unknown. 2. We develop a novel cross-data validation method for measuring the value of survey data from one source (e.g. an Internet checklist program) relative to a second, benchmark data source. The method fits a model to the data of interest and validates the model using benchmark data, allowing us to isolate the training data's information content from its biases. We also define a data efficiency ratio to quantify the relative efficiency of the data sources. 3. We apply our cross-data validation method to quantify the value of data collected in eBird -- a western hemisphere, year-round citizen science bird checklist project -- relative to data from the highly standardized North American Breeding Bird Survey (BBS). The results show that eBird data contain information similar in quality to that in BBS data, while the information per BBS datum is higher. 4. We suggest that these methods have more general use in evaluating the suitability of sources of data for addressing specific questions for taxa of interest.},
  keywords = {statistics}
}

@techreport{munson2011,
  title = {The {{eBird}} Reference Dataset, Version 3.0},
  author = {Munson, M. Arthur and Webb, Kevin and Sheldon, Daniel and Fink, Daniel and Hochachka, Wesley M. and Iliff, Marshall and Riedewald, Mirek and Sorokina, Daria and Sullivan, Brian and Wood, Christopher and Kelling, Steve},
  year = {2011},
  month = dec,
  address = {Ithaca, NY},
  institution = {{Cornell Lab of Ornithology and National Audubon Society}},
  abstract = {This document describes the eBird reference data set and the processing steps taken during creation. We hope this data will be a useful resource for studying avian dynamics and for developing new ecological modeling techniques.},
  keywords = {data,statistics}
}

@article{munzner2009,
  title = {A Nested Model for Visualization Design and Validation},
  author = {Munzner, Tamara},
  year = {2009},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {15},
  number = {6},
  pages = {921--928},
  abstract = {We present a nested model for the visualization design and validation with four layers: characterize the task and data in the vocabulary of the problem domain, abstract into operations and data types, design visual encoding and interaction techniques, and create algorithms to execute techniques efficiently. The output from a level above is input to the level below, bringing attention to the design challenge that an upstream error inevitably cascades to all downstream levels. This model provides prescriptive guidance for determining appropriate evaluation approaches by identifying threats to validity unique to each level. We also provide three recommendations motivated by this model: authors should distinguish between these levels when claiming contributions at more than one of them, authors should explicitly state upstream assumptions at levels above the focus of a paper, and visualization venues should accept more papers on domain characterization.},
  keywords = {data visualization,statistics}
}

@article{munzner2009a,
  title = {A {{Nested Model}} for {{Visualization Design}} and {{Validation}}},
  author = {Munzner, Tamara},
  year = {2009},
  month = nov,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {15},
  number = {6},
  pages = {921--928},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2009.111},
  abstract = {We present a nested model for the visualization design and validation with four layers: characterize the task and data in the vocabulary of the problem domain, abstract into operations and data types, design visual encoding and interaction techniques, and create algorithms to execute techniques efficiently. The output from a level above is input to the level below, bringing attention to the design challenge that an upstream error inevitably cascades to all downstream levels. This model provides prescriptive guidance for determining appropriate evaluation approaches by identifying threats to validity unique to each level. We also provide three recommendations motivated by this model: authors should distinguish between these levels when claiming contributions at more than one of them, authors should explicitly state upstream assumptions at levels above the focus of a paper, and visualization venues should accept more papers on domain characterization.},
  keywords = {Algorithm design and analysis,Concrete,Coupled mode analysis,Data visualization,design,Diseases,Electronic mail,Encoding,evaluation.,frameworks,Models,Process design,Vocabulary,Writing},
  file = {/Users/amcnamara/Zotero/storage/ZKV2A6JD/Munzner - 2009 - A Nested Model for Visualization Design and Valida.pdf;/Users/amcnamara/Zotero/storage/FWI2R3YG/5290695.html}
}

@book{natacad2018,
  title = {Data {{Science}} for {{Undergraduates}}: {{Opportunities}} and {{Options}}},
  shorttitle = {Data {{Science}} for {{Undergraduates}}},
  author = {{Committee on Envisioning the Data Science Discipline: The Undergraduate Perspective} and {Computer Science and Telecommunications Board} and {Board on Mathematical Sciences and Analytics} and {Committee on Applied and Theoretical Statistics} and {Division on Engineering and Physical Sciences} and {Board on Science Education} and {Division of Behavioral and Social Sciences and Education} and {National Academies of Sciences, Engineering, and Medicine}},
  year = {2018},
  month = oct,
  pages = {25104},
  publisher = {National Academies Press},
  address = {Washington, D.C.},
  doi = {10.17226/25104},
  urldate = {2021-07-20},
  isbn = {978-0-309-47559-4}
}

@book{nationalacademiesofsciencesengineeringandmedicine2018,
  title = {Envisioning the {{Data Science Discipline}}: {{The Undergraduate Perspective}}: {{Interim Report}}},
  shorttitle = {Envisioning the {{Data Science Discipline}}},
  author = {{National Academies of Sciences, Engineering, and Medicine}},
  year = {2018},
  month = mar,
  publisher = {National Academies Press},
  address = {Washington, D.C.},
  doi = {10.17226/24886},
  urldate = {2023-07-07},
  isbn = {978-0-309-46502-1},
  keywords = {{Math, Chemistry, and Physics--Math and Statistics},Computers and Information Technology--Information Technology,Education--Higher Education,Education--Math and Science Education,Surveys and Statistics},
  file = {/Users/amcnamara/Zotero/storage/EDMAK5HM/Committee on Envisioning the Data Science Discipline The Undergraduate Perspective et al. - 2018 - Envisioning the Data Science Discipline The Under.pdf}
}

@misc{nationalcenterforfacultydevelopmentanddiversity2022,
  title = {Faculty {{Diversity}}},
  author = {{National Center for Faculty Development {and} Diversity}},
  year = {2022},
  urldate = {2022-07-21},
  abstract = {On-demand access to the mentoring, tools, and support you need to be successful in the Academy.},
  howpublished = {https://www.facultydiversity.org/},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/T29DPGNF/www.facultydiversity.org.html}
}

@book{nationalcouncilofteacherofmathematics2008,
  title = {The {{Role}} of {{Technology}} in the Teaching and Learning of Mathematics},
  author = {{National Council of Teacher of Mathematics}},
  year = {2008},
  month = mar,
  keywords = {education,mathematics,technology}
}

@book{nationalgovernorsassociationcenterforbestpractices2010,
  title = {Common {{Core State Standards}} for {{Mathematics}}: {{Designing High School Mathematics Courses Based}} on the {{Common Core State Standards}}},
  author = {{National Governors Association Center for Best Practices}},
  year = {2010},
  publisher = {National Governors Association Center for Best Practices, Council of Chief State School Officers},
  keywords = {education,mathematics,statistics}
}

@book{nationalgovernorsassociationcenterforbestpracticesandcouncilofchiefstateschoolofficers2010,
  title = {Common {{Core State Standards}} for {{Mathematics}}},
  author = {{National Governors Association Center for Best Practices and Council of Chief State School Officers}},
  year = {2010},
  publisher = {National Governors Association Center for Best Practices, Council of Chief State School Officers},
  keywords = {education,mathematics,statistics}
}

@article{nederbragtetal2020,
  title = {Ten Quick Tips for Teaching with Participatory Live Coding},
  author = {Nederbragt, Alexander and Harris, Rayna Michelle and Hill, Alison Presmanes and Wilson, Greg},
  editor = {Ouellette, Francis},
  year = {2020},
  month = sep,
  journal = {PLOS Computational Biology},
  volume = {16},
  number = {9},
  pages = {e1008090},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008090},
  urldate = {2022-10-21},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/EGFFC7JP/Nederbragt et al. - 2020 - Ten quick tips for teaching with participatory liv.pdf}
}

@book{nelson1974,
  title = {Computer {{Lib}} / {{Dream Machines}}},
  author = {Nelson, Theodor H.},
  year = {1974}
}

@article{neumann2009,
  title = {Statistics? {{You}} Must Be Joking: The Application and Evaluation of Humor When Teaching Statistics},
  author = {Neumann, David L. and Hood, Michelle and Neumann, Michelle M.},
  year = {2009},
  journal = {Journal of statistics education},
  volume = {17},
  number = {2},
  keywords = {education,statistics}
}

@article{neumann2011,
  title = {Evaluating Computer-Based Simulations, Multimedia and Animations That Help Integrate Blended Learning with Lectures in First Year Statistics},
  author = {Neumann, David L. and Neumann, Michelle M. and Hood, Michelle},
  year = {2011},
  journal = {Australasian Journal of Educational Technology},
  volume = {27},
  number = {2},
  pages = {274--289},
  keywords = {computer science,education,statistics,technology}
}

@article{newellcard1985,
  title = {The {{Prospects}} for {{Psychological Science}} in {{Human-Computer Interaction}}},
  author = {Newell, Allen and Card, Stuart K.},
  year = {1985},
  journal = {Human Computer Interaction},
  volume = {1},
  pages = {209--242}
}

@misc{newjerseydepartmentofeducation2023,
  title = {Overview of the {{Proposed}} 2023 {{New Jersey Student Learning Standards}} for {{Mathematics}}},
  author = {{New Jersey Department of Education}},
  year = {2023},
  month = apr,
  urldate = {2023-06-14},
  howpublished = {https://www.nj.gov/education/standards/docs/sboe/Presentation\_NJSLS\_Math\_April\_2023.pdf},
  file = {/Users/amcnamara/Zotero/storage/DPHLREQJ/Presentation_NJSLS_Math_April_2023.pdf}
}

@article{newyorktimes2012,
  title = {The {{Electoral Map}}: {{Building}} a {{Path}} to {{Victory}}},
  author = {{New York Times}},
  year = {2012},
  journal = {The New York Times}
}

@article{nielsen2020,
  title = {Students' Video Viewing Habits during a Flipped Classroom Course in Engineering Mathematics},
  author = {Nielsen, Kjetil Liest{\o}l},
  year = {2020},
  month = jul,
  journal = {Research in Learning Technology},
  volume = {28},
  number = {0},
  issn = {2156-7077},
  doi = {10.25304/rlt.v28.2404},
  urldate = {2021-11-05},
  file = {/Users/amcnamara/Zotero/storage/GLZW9NSL/Nielsen - 2020 - Students’ video viewing habits during a flipped cl.pdf}
}

@unpublished{nolan1999,
  title = {Teaching Statistics Theory through Applications},
  author = {Nolan, D. and Speed, T. P.},
  year = {1999},
  month = aug,
  keywords = {education,instruction,quantitative literacy,statistical thinking,statistics}
}

@article{nolan2007,
  title = {Dynamic, Interactive Documents for Teaching Statistical Practice},
  author = {Nolan, Deborah and Lang, Duncan Temple},
  year = {2007},
  journal = {International Statistical Review},
  volume = {75},
  number = {3},
  pages = {295--321},
  abstract = {Significant efforts have been made to overhaul the introductory statistics courses by placing greater emphasis on statistical thinking and literacy and less on rules, methods and procedures. We advocate broadening and increasing this effort to all levels of students and, importantly, using topical, interesting, substantive problems that come from the actual practice of statistics. We want students to understand the thought process of the ``masters'' in context, seeing their choices, different approaches and explorations. Similar to Open Source software, we think it is vital that the work of the community of researchers is accessible to the community of educators so that students can experience statistical applications and learn how to approach analyses themselves. We describe a mechanism by which one can collect all aspects or fragments of an analysis or simulation into a ``document'' so that the computations and results are reproducible, reusable and amenable to extensions. These documents contain various pieces of information (e.g. text, code, data, exploration paths) and can be processed to create regular descriptive papers in various formats (e.g. PDF, HTML), as well as acting as a database of the analysis which we can explore in rich new ways. Researchers, instructors and readers can control the various steps in the processing and rendering of the document. For example, this type of document supports interactive components with which a student can easily control and alter the inputs to the computations in a semi-guided fashion, gradually delve deeper into the details, and go on to her own free-form analysis. Our implementation for this system is based on widely used and standardized frameworks and readily supports multiple and different programming languages. Also, it is highly extensible which allows adaptation and future developments.},
  keywords = {to read}
}

@incollection{nolan2010,
  title = {Biomedical {{Informatics}} for {{Cancer Research}}},
  author = {Nolan, Deborah and Peng, Roger D. and Lang, Duncan Temple},
  year = {2010},
  publisher = {Springer Science \$+\$ Business Media},
  chapter = {Enhanced dynamic documents for reproducible research},
  keywords = {to read}
}

@article{nolan2010a,
  title = {Computing in the Statistics Curricula},
  author = {Nolan, Deborah and Lang, Duncan Temple},
  year = {2010},
  month = may,
  journal = {The American Statistician},
  volume = {64},
  number = {2},
  pages = {97--107},
  keywords = {computer science,education,statistics,technology}
}

@article{nolan2012,
  title = {Interactive and Animated Scalable Vector Graphics and {{R}} Data Displays},
  author = {Nolan, Deborah and Lang, Duncan Temple},
  year = {2012},
  journal = {Journal of Statistical Software},
  volume = {46},
  number = {1},
  abstract = {We describe an approach to creating interactive and animated graphical displays using R's graphics engine and Scalable Vector Graphics, an XML vocabulary for describing two-dimensional graphical displays. We use the svg() graphics device inR and then post- process the resulting XML documents. The post-processing identifies the elements in the SVG that correspond to the different components of the graphical display, e.g., points, axes, labels, lines. One can then annotate these elements to add interactivity and animation effects. One can also use JavaScript to provide dynamic interactive effects to the plot, enabling rich user interactions and compelling visualizations. The resulting SVG documents can be embedded within HTML documents and can involve JavaScript code that integrates the SVG and HTML objects. The functionality is provided via the SVGAnnotation package and makes static plots generated via R graphics functions available as stand-alone, interactive and animated plots for the Web and other venues.},
  keywords = {data visualization,R,statistics}
}

@book{nolan2014,
  title = {{{XML}} and {{Web}} Technologies for {{Data Sciences}} with {{R}}},
  author = {Nolan, Deborah and Lang, Duncan Temple},
  year = {2014},
  publisher = {Springer-Verlag}
}

@book{nolan2015,
  title = {Teaching and {{Learning Data Visualization}}: {{Ideas}} and {{Assignments}}},
  author = {Nolan, Deborah and Perrett, James},
  year = {2015}
}

@article{nolantemplelang2010,
  title = {Computing in the {{Statistics Curricula}}},
  author = {Nolan, Deborah and Temple Lang, Duncan},
  year = {2010},
  month = may,
  journal = {The American Statistician},
  volume = {64},
  number = {2},
  pages = {97--107},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1198/tast.2010.09132},
  urldate = {2022-01-31},
  abstract = {The nature of statistics is changing significantly with many opportunities to broaden the discipline and its impact on science and policy. To realize this potential, our curricula and educational culture must change. While there are opportunities for significant change in many dimensions, we focus more narrowly on computing and call for computing concepts to be integrated into the statistics curricula at all levels. Computational literacy and programming are as fundamental to statistical practice and research as mathematics. We advocate that our field needs to define statistical computing more broadly to include advancements in modern computing, beyond traditional numerical algorithms. Information technologies are increasingly important and should be added to the curriculum, as should the ability to reason about computational resources, work with large datasets, and perform computationally intensive tasks. We present an approach to teaching these topics in combination with scientific problems and modern statistical methods that focuses on ideas and skills for statistical inquiry and working with data. We outline the broad set of computational topics we might want students to encounter and offer ideas on how to teach them. We also discuss efforts to share pedagogical resources to help faculty teach this modern material (including supplemental materials).},
  keywords = {Computational literacy,Curriculum reform,Information technology},
  file = {/Users/amcnamara/Zotero/storage/M9HEEAJB/Nolan and Temple Lang - 2010 - Computing in the Statistics Curricula.pdf;/Users/amcnamara/Zotero/storage/CC8RGGDZ/tast.2010.html}
}

@inproceedings{nolisnolis2020,
  title = {We're Hitting {{R}} a Million Times a Day so We Made a Talk about It},
  booktitle = {Rstudio::Conf 2020},
  author = {Nolis, Heather and Nolis, Jacqueline},
  year = {2020},
  month = jan,
  urldate = {2022-03-30},
  abstract = {For the past year, we at T-Mobile have been sludging through production outages, nation-wide product launches, and all of the muck that floods from R models being hit over a million times every day.},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/IKSN3D38/we-re-hitting-r-a-million-times-a-day-so-we-made-a-talk-about-it.html}
}

@article{noll2011,
  title = {Graduate Teaching Assistants' Statistical Content Knowledge of Sampling},
  author = {Noll, Jennifer},
  year = {2011},
  journal = {Statistics Education Research Journal},
  volume = {10},
  number = {2},
  keywords = {to read}
}

@article{nollkirin2016,
  title = {Student {{Approaches}} to {{Constructing Statistical Models}} Using {{TinkerPlots}} {\textsuperscript{ }}{{{\textsuperscript{TM}}}}{\textsuperscript{ }}},
  author = {Noll, Jennifer and Kirin, Dana},
  year = {2016},
  journal = {Technology Innovations in Statistics Education},
  volume = {9},
  number = {1},
  issn = {1933-4214},
  doi = {10.5070/T591023693},
  urldate = {2022-07-17},
  file = {/Users/amcnamara/Zotero/storage/V6YIP9WM/Noll and Kirin - 2016 - Student Approaches to Constructing Statistical Mod.pdf}
}

@article{nollkirin2017,
  title = {{{TinkerPlots Model Construction Approaches}} for {{Comparing Two Groups}}: {{Student Perspectives}}},
  author = {Noll, Jennifer and Kirin, Dana},
  year = {2017},
  journal = {Statistics Education Research Journal},
  volume = {16},
  number = {2},
  pages = {2130243}
}

@book{norman1986,
  title = {User Centered System Design: New Perspectives on Human-Computer Interaction},
  shorttitle = {User Centered System Design},
  editor = {Norman, Donald A.},
  year = {1986},
  edition = {9. [print.]},
  publisher = {Erlbaum},
  address = {Hillsdale, NJ},
  isbn = {978-0-89859-872-8 978-0-89859-781-3},
  langid = {english}
}

@article{norman2010,
  title = {Likert Scales, Levels of Measurement and the ``Laws'' of Statistics},
  author = {Norman, Geoff},
  year = {2010},
  month = dec,
  journal = {Advances in Health Sciences Education},
  volume = {15},
  number = {5},
  pages = {625--632},
  issn = {1382-4996, 1573-1677},
  doi = {10.1007/s10459-010-9222-y},
  urldate = {2021-10-01},
  langid = {english}
}

@article{north2000,
  title = {Snap-Together Visualization: {{Can}} Users Construct and Operate Coordinated Visualizations?},
  author = {North, Chris and Shneiderman, Ben},
  year = {2000},
  journal = {International Journal of Human-Computer Studies},
  volume = {53},
  pages = {715--739},
  abstract = {Multiple coordinated visualizations enable users to rapidly explore complex information. However, users often need unforeseen combinations of coordinated visualizations. Snap-together visualization (Snap) enables users to rapidly and dynamically construct coordinated visualization interfaces, customized for their data, without programming. Users load data into desired visualizations, then construct coordinations between them for brushing and linking, overview and detail view, drill down, etc. Snap formalizes a conceptual model of visualization coordination based on the relational data model. Visualization developers can easily Snap-enable their independent visualizations using a simple API. Empirical evaluation reveals benefits, cognitive issues and usability concerns with coordination concepts and Snap. Two user studies explore coordination construction and operation. Data-savvy users successfully, enthusiastically and rapidly constructed powerful coordinated visualization interfaces of their own. Operating an overview-and-detail coordination reliably improved user performance by 30-80\% over detail-only and uncoordinated interfaces for most tasks.},
  keywords = {to read}
}

@article{nusrat2018,
  title = {Evaluating {{Cartogram Effectiveness}}},
  author = {Nusrat, Sabrina and Alam, Md. Jawaherul and Kobourov, Stephen},
  year = {2018},
  month = feb,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {24},
  number = {2},
  pages = {1077--1090},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2016.2642109},
  abstract = {Cartograms are maps in which areas of geographic regions, such as countries and states, appear in proportion to some variable of interest, such as population or income. Cartograms are popular visualizations for geo-referenced data that have been used for over a century to illustrate patterns and trends in the world around us. Despite the popularity of cartograms, and the large number of cartogram types, there are few studies evaluating the effectiveness of cartograms in conveying information. Based on a recent task taxonomy for cartograms, we evaluate four major types of cartograms: contiguous, non-contiguous, rectangular, and Dorling cartograms. We first evaluate the effectiveness of these cartogram types by quantitative performance analysis (time and error). Second, we collect qualitative data with an attitude study and by analyzing subjective preferences. Third, we compare the quantitative and qualitative results with the results of a metrics-based cartogram evaluation. Fourth, we analyze the results of our study in the context of cartography, geography, visual perception, and demography. Finally, we consider implications for design and possible improvements.},
  keywords = {Cartograms,Data visualization,geo-visualization,Geography,Shape,Sociology,Statistics,subjective evaluation,Topology,Voting},
  file = {/Users/amcnamara/Zotero/storage/XEUG4RNV/Nusrat et al. - 2018 - Evaluating Cartogram Effectiveness.pdf;/Users/amcnamara/Zotero/storage/92CCQ74D/7792176.html}
}

@book{oakes1990,
  title = {Multiplying {{Inequalities}}: The Effects of Race, Social Class, and Tracking on Opportunities to Learn Mathematics and Science},
  author = {Oakes, Jeannie},
  year = {1990},
  publisher = {RAND corporation},
  keywords = {education,equity,mathematics}
}

@article{oboyleetal2017,
  title = {The {{Chrysalis Effect}}: {{How Ugly Initial Results Metamorphosize Into Beautiful Articles}}},
  shorttitle = {The {{Chrysalis Effect}}},
  author = {O'Boyle, Ernest Hugh and Banks, George Christopher and {Gonzalez-Mul{\'e}}, Erik},
  year = {2017},
  month = feb,
  journal = {Journal of Management},
  volume = {43},
  number = {2},
  pages = {376--399},
  issn = {0149-2063, 1557-1211},
  doi = {10.1177/0149206314527133},
  urldate = {2022-09-29},
  abstract = {The issue of a published literature not representative of the population of research is most often discussed in terms of entire studies being suppressed. However, alternative sources of publication bias are questionable research practices (QRPs) that entail post hoc alterations of hypotheses to support data or post hoc alterations of data to support hypotheses. Using general strain theory as an explanatory framework, we outline the means, motives, and opportunities for researchers to better their chances of publication independent of rigor and relevance. We then assess the frequency of QRPs in management research by tracking differences between dissertations and their resulting journal publications. Our primary finding is that from dissertation to journal article, the ratio of supported to unsupported hypotheses more than doubled (0.82 to 1.00 versus 1.94 to 1.00). The rise in predictive accuracy resulted from the dropping of statistically nonsignificant hypotheses, the addition of statistically significant hypotheses, the reversing of predicted direction of hypotheses, and alterations to data. We conclude with recommendations to help mitigate the problem of an unrepresentative literature that we label the ``Chrysalis Effect.''},
  langid = {english}
}

@book{oceanhealthindex2017,
  title = {Ocean {{Health Index}} for the {{Baltic Sea}}},
  author = {{Ocean Health Index}},
  year = {2017}
}

@techreport{officeofthechiefinformationofficer2001,
  title = {Requirements for Accessible Electronic and Information Technology Design},
  author = {{Office of the Chief Information Officer}},
  year = {2001},
  institution = {Department of Education}
}

@book{ogden2015,
  title = {Dat},
  author = {Ogden, Max and Buus, Mathias and McKelvey, Karissa},
  year = {2015}
}

@book{Ohio2007,
  title = {Ohio's {{Fourth-Year Modeling}} and {{Quantitative Reasoning Course}}},
  year = {2007},
  month = apr,
  publisher = {Ohio Department of Education},
  keywords = {education,mathematics}
}

@book{ohri2013,
  title = {Interview {{Dr}}. {{Ian Fellows Fellstat}}.Com \#rstats {{Deducer}}},
  author = {Ohri, Ajay},
  year = {2013}
}

@incollection{olive2010,
  title = {Mathematics {{Education}} and {{Technology-Rethinking}} the {{Terrain}}},
  author = {Olive, John and Makar, Katie and Hoyos, Ver{\'o}nica and Kor, Liew Kee and Kosheleva, Olga and Str{\"a}{\ss}er, Rudolf},
  editor = {Hoyles, C. and Lagrange, J.-B.},
  year = {2010},
  publisher = {Springer Science \$+\$ Business Media},
  abstract = {Through an extensive review of the literature we indicate how technology has influenced the contexts for learning mathematics, and the emergence of a new learning ecology that results from the integration of technology into these learning contexts. Conversely, we argue that the mathematics on which the technologies are based influences their design, especially the affordances and constraints for learning of the specific design. The literature indicates that interactions among students, teachers, tasks, and technologies can bring about a shift in empowerment from teacher or external authority to the students as generators of mathematical knowledge and practices; and that feedback provided through the use of different technologies can contribute to students' learning. Recent developments in dynamic technologies have the potential to promote new mathematical practices in different contexts: for example, dynamic geometry, statistical education, robotics and digital games. We propose a transformation of the traditional didactic triangle into a didactic tetrahedron through the introduction of technology and conclude by restructuring this model so as to redefine the space in which new mathematical knowledge and practices can emerge.},
  chapter = {Mathematical Knowledge and Practices Resulting from Access to Digital Technologies},
  keywords = {to read}
}

@article{ooms2013,
  title = {Possible Directions for Improving Dependency Versioning in {{R}}},
  author = {Ooms, Jeroen},
  year = {2013},
  journal = {The R Journal},
  volume = {5},
  number = {1}
}

@book{oreillymediainc2012,
  title = {Big {{Data Now}}: 2012 {{Edition}}},
  author = {{O'Reilly Media, Inc}},
  year = {2012},
  publisher = {O'Reilly}
}

@techreport{ottley2012,
  title = {Visually {{Communicating Bayesian Statistics}} to {{Laypersons}}},
  author = {Ottley, Alvitta and Metevier, Blossom and Han, Paul K. J. and Chang, Remco},
  year = {2012},
  institution = {Tufts University},
  abstract = {Effectively communicating Bayesian statistics to laypersons has been an open challenge for many years. Recent research in psychology proposed that there is a direct correlation between comprehension and representation. Specifically, a series of studies suggests that pictorial representations with icon arrays may be better suited for communicating Bayesian statistics than Euler diagrams. Though these results are compelling, the experiments were conducted in controlled lab settings and with limited samples. In this paper, we extend the previous re- search by expanding the sample to a more diverse population through crowdsourcing. We conducted a user study that compares three different pictorial representations of Bayesian statistics icon arrays, Euler diagrams and discretized Euler diagrams. Our findings fail to replicate previous results and demonstrate no significant difference between the three representations. We discuss possible explanations for these findings and propose directions for future investigations.}
}

@article{paechteretal2017,
  title = {Mathematics {{Anxiety}} and {{Statistics Anxiety}}. {{Shared}} but {{Also Unshared Components}} and {{Antagonistic Contributions}} to {{Performance}} in {{Statistics}}},
  author = {Paechter, Manuela and Macher, Daniel and Martskvishvili, Khatuna and Wimmer, Sigrid and Papousek, Ilona},
  year = {2017},
  journal = {Frontiers in Psychology},
  volume = {8},
  issn = {1664-1078},
  urldate = {2023-06-07},
  abstract = {In many social science majors, e.g., psychology, students report high levels of statistics anxiety. However, these majors are often chosen by students who are less prone to mathematics and who might have experienced difficulties and unpleasant feelings in their mathematics courses at school. The present study investigates whether statistics anxiety is a genuine form of anxiety that impairs students' achievements or whether learners mainly transfer previous experiences in mathematics and their anxiety in mathematics to statistics. The relationship between mathematics anxiety and statistics anxiety, their relationship to learning behaviors and to performance in a statistics examination were investigated in a sample of 225 undergraduate psychology students (164 women, 61 men). Data were recorded at three points in time: At the beginning of term students' mathematics anxiety, general proneness to anxiety, school grades, and demographic data were assessed; 2 weeks before the end of term, they completed questionnaires on statistics anxiety and their learning behaviors. At the end of term, examination scores were recorded. Mathematics anxiety and statistics anxiety correlated highly but the comparison of different structural equation models showed that they had genuine and even antagonistic contributions to learning behaviors and performance in the examination. Surprisingly, mathematics anxiety was positively related to performance. It might be that students realized over the course of their first term that knowledge and skills in higher secondary education mathematics are not sufficient to be successful in statistics. Part of mathematics anxiety may then have strengthened positive extrinsic effort motivation by the intention to avoid failure and may have led to higher effort for the exam preparation. However, via statistics anxiety mathematics anxiety also had a negative contribution to performance. Statistics anxiety led to higher procrastination in the structural equation model and, therefore, contributed indirectly and negatively to performance. Furthermore, it had a direct negative impact on performance (probably via increased tension and worry in the exam). The results of the study speak for shared but also unique components of statistics anxiety and mathematics anxiety. They are also important for instruction and give recommendations to learners as well as to instructors.},
  file = {/Users/amcnamara/Zotero/storage/9C7BN7R9/Paechter et al. - 2017 - Mathematics Anxiety and Statistics Anxiety. Shared.pdf}
}

@article{pandey2014,
  title = {The Persuasive Power of Data Visualization},
  author = {Pandey, Anshul Vikram and Manivannan, Anjali and Nov, Oded and Satterhwaite, Margaret L. and Bertini, Enrico},
  year = {2014},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {20},
  number = {12},
  abstract = {Data visualization has been used extensively to inform users. However, little research has been done to examine the effects of data visualization in influencing users or in making a message more persuasive. In this study, we present experimental research to fill this gap and present an evidence-based analysis of persuasive visualization. We built on persuasion research from psychology and user interfaces literature in order to explore the persuasive effects of visualization. In this experimental study we define the circumstances under which data visualization can make a message more persuasive, propose hypotheses, and perform quantitative and qualitative analyses on studies conducted to test these hypotheses. We compare visual treatments with data presented through barcharts and linecharts on the one hand, treatments with data presented through tables on the other, and then evaluate their persuasiveness. The findings represent a first step in exploring the effectiveness of persuasive visualization.},
  keywords = {experimental,IEEEviz}
}

@article{pape2001,
  title = {The Role of Representation(s) in Developing Mathematical Understanding},
  author = {Pape, Stephen J. and Tchoshanov, Mourat A.},
  year = {2001},
  journal = {Theory into Practice},
  volume = {40},
  number = {2},
  pages = {118--127},
  keywords = {to read}
}

@book{papert1979,
  title = {Final Report of the {{Brookline LOGO Project}}},
  author = {Papert, Seymour},
  year = {1979},
  publisher = {Massachusetts Institute of Technology, Artificial Intelligence Laboratory}
}

@article{parzen1962,
  title = {On Estimation of a Probability Density Function and Mode},
  author = {Parzen, Emanuel},
  year = {1962},
  journal = {Annals of Mathematical Statistics},
  volume = {33},
  number = {3},
  pages = {1065--1076},
  keywords = {to read}
}

@inproceedings{passijackson2017,
  title = {Data {{Vision}}: {{Learning}} to {{See Through Algorithmic Abstraction}}},
  shorttitle = {Data {{Vision}}},
  booktitle = {Proceedings of the 2017 {{ACM Conference}} on {{Computer Supported Cooperative Work}} and {{Social Computing}}},
  author = {Passi, Samir and Jackson, Steven},
  year = {2017},
  month = feb,
  series = {{{CSCW}} '17},
  pages = {2436--2447},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2998181.2998331},
  urldate = {2023-07-10},
  abstract = {Learning to see through data is central to contemporary forms of algorithmic knowledge production. While often represented as a mechanical application of rules, making algorithms work with data requires a great deal of situated work. This paper examines how the often-divergent demands of mechanization and discretion manifest in data analytic learning environments. Drawing on research in CSCW and the social sciences, and ethnographic fieldwork in two data learning environments, we show how an algorithm's application is seen sometimes as a mechanical sequence of rules and at other times as an array of situated decisions. Casting data analytics as a rule-based (rather than rule-bound) practice, we show that effective data vision requires would-be analysts to straddle the competing demands of formal abstraction and empirical contingency. We conclude by discussing how the notion of data vision can help better leverage the role of human work in data analytic learning, research, and practice.},
  isbn = {978-1-4503-4335-0},
  keywords = {data analysis,data vision,digital humanities,machine learning,professional vision,professionalization},
  file = {/Users/amcnamara/Zotero/storage/AWMLGSP8/Passi and Jackson - 2017 - Data Vision Learning to See Through Algorithmic A.pdf}
}

@article{payton2015,
  title = {Micromap: {{A}} Package for Linked Micromaps},
  author = {Payton, Quinn C. and McManus, Michael G. and Weber, Marc H. and Olsen, Anthony R. and Kincaid, Thomas M.},
  year = {2015},
  journal = {Journal of Statistical Software},
  volume = {63},
  number = {2},
  keywords = {to read}
}

@techreport{pea1983,
  title = {{{LOGO Programming}} and Problem Solving},
  author = {Pea, Roy D.},
  year = {1983},
  institution = {{Center for Children and Technology}},
  abstract = {In the world of educational computing, programming is a major activity, occupying several million precollege students a year in this country alone. As yet very little is known about what kinds of cognitive activities computer programming requires and whether, in the classroom contexts that are representative of microcomputer use in schools today, children are capable of making substantial progress in learning to program. In the cyclical program-development process of problem understanding, program design and planning, programming code composition, debugging, and comprehension, what gains do children make on the many developmental fronts represented in the complex of mental activities required by programming? Do conceptual limitations impede their understanding of any of the central program- ming concepts, such as flow of control structures, variables, procedurality, and the like? We have begun to address aspects of these questions in our developmental research on children learning to do Logo programming.},
  keywords = {to read}
}

@article{pea1985,
  title = {Beyond Amplification: {{Using}} the Computer to Reorganize Mental Functioning},
  author = {Pea, Roy D.},
  year = {1985},
  journal = {Educational Psychologist},
  volume = {20},
  number = {4},
  pages = {167--182},
  abstract = {Computers are classically viewed as amplifiers of cognition. A n alternative conceptualization is offered of computer as reorganizer of mental functioning. Software analyses illuminate the advantages of the latter approach for new visions of the potential cognitive benefits of computers. A new result emerges: Because the cognitive technologies we invent serve as instruments of cultural redefinition (shaping who we are by changing, not just amplifying, what we do), defining educational values becomes a foreground issue. The demands of an information society make an explicit emphasis on general cognitive skills a priority. The urgency of updating education's goals and methods recommends an activist research paradigm: to simultaneously create and study changes in processes and outcomes of human learning with new cognitive and educational tools.},
  keywords = {to read}
}

@inproceedings{pears2007,
  title = {A Survey of Literature on the Teaching of Introductory Programming},
  booktitle = {Working Group Reports on {{ITiCSE}} on {{Innovation}} and Technology in Computer Science Education  - {{ITiCSE-WGR}} '07},
  author = {Pears, Arnold and Seidman, Stephen and Malmi, Lauri and Mannila, Linda and Adams, Elizabeth and Bennedsen, Jens and Devlin, Marie and Paterson, James},
  year = {2007},
  pages = {204},
  publisher = {ACM Press},
  address = {Dundee, Scotland},
  issn = {00978418},
  doi = {10.1145/1345443.1345441},
  urldate = {2021-06-11},
  langid = {english}
}

@article{pebesma2015,
  title = {Software for Spatial Statistics},
  author = {Pebesma, Edzer and Bivand, Roger and Ribeiro, Paulo Justiano},
  year = {2015},
  journal = {Journal of Statistical Software},
  volume = {63},
  number = {1},
  keywords = {to read}
}

@inproceedings{pecketal2019,
  title = {Data Is {{Personal}}: {{Attitudes}} and {{Perceptions}} of {{Data Visualization}} in {{Rural Pennsylvania}}},
  shorttitle = {Data Is {{Personal}}},
  booktitle = {Proceedings of the 2019 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Peck, Evan M. and Ayuso, Sofia E. and {El-Etr}, Omar},
  year = {2019},
  month = may,
  series = {{{CHI}} '19},
  pages = {1--12},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3290605.3300474},
  urldate = {2023-06-29},
  abstract = {Many of the guidelines that inform how designers create data visualizations originate in studies that unintentionally exclude populations that are most likely to be among the 'data poor'. In this paper, we explore which factors may drive attention and trust in rural populations with diverse economic and educational backgrounds - a segment that is largely underrepresented in the data visualization literature. In 42 semi-structured interviews in rural Pennsylvania (USA), we find that a complex set of factors intermix to inform attitudes and perceptions about data visualization - including educational background, political affiliation, and personal experience. The data and materials for this research can be found at https://osf.io/uxwts/},
  isbn = {978-1-4503-5970-2},
  keywords = {data,information literacy,information visualization,rural},
  file = {/Users/amcnamara/Zotero/storage/EIRXRF88/Peck et al. - 2019 - Data is Personal Attitudes and Perceptions of Dat.pdf}
}

@article{pelissier2015,
  title = {Ads Package for {{R}}: {{A}} Fast Unbiased Implementation of the {{K-function}} Family for Studying Spatial Point Patterns in Irregular-Shaped Sampling Windows},
  author = {P{\'e}lissier, Rapha{\"e}l and Goreaud, Fran{\c c}ois},
  year = {2015},
  journal = {Journal of Statistical Software},
  volume = {63},
  number = {6},
  keywords = {to read}
}

@article{pengetal2021,
  title = {Diagnosing {{Data Analytic Problems}} in the {{Classroom}}},
  author = {Peng, Roger D. and Chen, Athena and Bridgeford, Eric and Leek, Jeffrey T. and Hicks, Stephanie C.},
  year = {2021},
  month = aug,
  journal = {Journal of Statistics and Data Science Education},
  volume = {0},
  number = {0},
  pages = {1--10},
  publisher = {Taylor \& Francis},
  issn = {null},
  doi = {10.1080/26939169.2021.1971586},
  urldate = {2021-11-09},
  abstract = {Teaching data analysis by providing students with real-world problems and datasets allows students to integrate a variety of skills in a situation that mirrors how data analysis actually works. However, whole data analyses may obscure the individual skills of data analytic practice that are generalizable across data analyses. One such skill is the ability to diagnose the cause of unexpected results in a data analysis. While experienced analysts can quickly iterate through a series of potential explanations when confronted with unexpected results, novice analysts often struggle to figure out how to move forward. The goal of this article is to describe an approach to teaching students skills in diagnosing data analytic problems. The exercise described here is targeted to allow students to practice this skill and to assess the depth of their knowledge about the statistical tools they have learned. We take a hypothetical case study approach and focus on the students' reasoning through their diagnoses and suggestions for follow-up action. We found the implementation of this exercise in a small graduate course to provide valuable information about the students' diagnostic thought processes, but further work is needed regarding structured approaches to implementation and the design of assessments. Supplementary materials for this article are available online.},
  keywords = {Case study,Data analysis,Data science},
  file = {/Users/amcnamara/Zotero/storage/5HJJY5MP/Peng et al. - 2021 - Diagnosing Data Analytic Problems in the Classroom.pdf;/Users/amcnamara/Zotero/storage/ZQ62V7IM/26939169.2021.html}
}

@book{pennstategeog4862014,
  title = {Choropleth {{Maps}}},
  author = {{Penn State GEOG 486}},
  year = {2014}
}

@article{perez2007,
  title = {{{iPython}}: A {{System}} for {{Interactive Scientific Computing}}},
  author = {P{\'e}rez, Fernando and Granger, Brian E.},
  year = {2007},
  month = may,
  journal = {Computing in Science \& Engineering},
  volume = {9},
  number = {3},
  pages = {21--29},
  keywords = {to read}
}

@book{perez2015,
  title = {Mobilize 2014-2015 {{Science Curriculum}}},
  author = {Perez, Leticia and John, Lynn Kim and Gould, Robert},
  year = {2015},
  publisher = {{Office of Curriculum, Instruction, and School Support}}
}

@techreport{perez2015a,
  title = {Project {{Jupyter}}: {{Computational Narratives}} as the {{Engine}} of {{Collaborative Data Science}}},
  author = {Perez, Fernando and Granger, Brian E.},
  year = {2015},
  institution = {Project Jupyter}
}

@article{perin2014,
  title = {Revisiting {{Bertin}} Matrices: {{New}} Interactions for Crafting Tabular Visualizations},
  author = {Perin, Charles and Dragicevic, Pierre and Fekete, Jean-Daniel},
  year = {2014},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {20},
  number = {12},
  abstract = {We present BERTIFIER, a web app for rapidly creating tabular visualizations from spreadsheets. BERTIFIER draws from Jacques Bertin's matrix analysis method, whose goal was to ``simplify without destroying'' by encoding cell values visually and grouping similar rows and columns. Although there were several attempts to bring this method to computers, no implementation exists today that is both exhaustive and accessible to a large audience. BERTIFIER remains faithful to Bertin's method while leveraging the power of today's interactive computers. Tables are formatted and manipulated through crossets, a new interaction technique for rapidly applying operations on rows and columns. We also introduce visual reordering, a semi-interactive reordering approach that lets users apply and tune automatic reordering algorithms in a WYSIWYG manner. Sessions with eight users from different backgrounds suggest that BERTIFIER has the potential to bring Bertin's method to a wider audience of both technical and non-technical users, and empower them with data analysis and communication tools that were so far only accessible to a handful of specialists.},
  keywords = {IEEEviz,theoretical}
}

@article{perin2015,
  title = {{{DIY Bertin}} Matrix},
  author = {Perin, Charles and Goc, Mathieu Le and Vozzo, Romain Di and Fekete, Jean-Daniel and Dragicevic, Pierre},
  year = {2015},
  journal = {CHI'15}
}

@article{perkel2005,
  title = {Biology by the Numbers},
  author = {Perkel, Jeffrey},
  year = {2005},
  month = jun,
  journal = {The Scientist}
}

@book{perlin2015,
  title = {Zen and the Art of Interactive Diagrams},
  author = {Perlin, Ken},
  year = {2015},
  month = feb
}

@misc{peters2004,
  title = {{{PEP}} 20 -- {{The Zen}} of {{Python}}},
  author = {Peters, Tim},
  year = {2004},
  journal = {Python.org},
  urldate = {2021-09-17},
  abstract = {The official home of the Python Programming Language},
  howpublished = {https://www.python.org/dev/peps/pep-0020/},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/4ISJRJ4I/pep-0020.html}
}

@phdthesis{peters2009,
  title = {Developing an Understanding of Variation: {{AP}} Statistics},
  author = {Peters, Susan A.},
  year = {2009},
  month = aug,
  school = {Pennsylvania State University},
  keywords = {education,statistics}
}

@article{petersohnetal2020,
  title = {Towards {{Scalable Dataframe Systems}}},
  author = {Petersohn, Devin and Macke, Stephen and Xin, Doris and Ma, William and Lee, Doris and Mo, Xiangxi and Gonzalez, Joseph E. and Hellerstein, Joseph M. and Joseph, Anthony D. and Parameswaran, Aditya},
  year = {2020},
  month = aug,
  journal = {Proceedings of the VLDB Endowment},
  volume = {13},
  number = {12},
  pages = {2033--2046},
  issn = {2150-8097},
  doi = {10.14778/3407790.3407807},
  urldate = {2021-07-18},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/27SUHMYS/Petersohn et al. - 2020 - Towards scalable dataframe systems.pdf}
}

@article{pfannkuch2006,
  title = {Comparing Box Plot Distributions: {{A}} Teachers' Reasoning},
  author = {Pfannkuch, Maxine},
  year = {2006},
  month = nov,
  journal = {Statistics Education Research Journal},
  volume = {5},
  number = {2},
  pages = {27--45},
  keywords = {box plots,distributional reasoning,education,informal statistical inference,secondary statistics teaching,statistics,Statistics education research}
}

@inproceedings{pfannkuch2008,
  title = {Training Teachers to Develop Statistical Thinking},
  booktitle = {Joint {{ICMI}}/{{IASE Study}}: {{Teaching Statistics}} in {{School Mathematics}}. {{Challenges}} for {{Teaching}} and {{Teacher Education}}. {{Proceedings}} of the {{ICMI Study Study}} 18 and 2008 {{IASE Round Table Conference}}},
  author = {Pfannkuch, Maxine},
  year = {2008},
  publisher = {ICMI/IASE},
  keywords = {education,statistics}
}

@incollection{pfannkuch2011,
  title = {Teaching {{Statistics}} in {{School Mathematics- Challenges}} for {{Teaching}} and {{Teacher Education}}},
  author = {Pfannkuch, Maxine and {Ben-Zvi}, Dani},
  year = {2011},
  publisher = {Springer Science \$+\$ Business Media},
  abstract = {In this chapter learning experiences that teachers need in order to develop their ability to think and reason statistically are described. It is argued that teacher courses should be designed around five major themes: developing understanding of key statistical concepts; developing the ability to explore and learn from data; developing statistical argumentation; using formative assessment; and learning to understand students' reasoning.},
  chapter = {Developing Teachers' Statistical Thinking}
}

@incollection{pfannkuch2014,
  title = {Using {{Tools}} for {{Learning Mathematics}} and {{Statistics}}},
  author = {Pfannkuch, Maxine and Wild, Chris and Regan, Matt},
  editor = {Wassong, Thomas and Frischemeier, Daniel and Rischer, Pascal R. and Hochmuth, Reinhard and Bender, Peter},
  year = {2014},
  pages = {393--403},
  publisher = {Springer},
  abstract = {When introducing students to statistical inference using bootstrapping and randomization methods and new infrastructure such as dynamic visualizations, new conceptual development issues may be revealed. From a pilot study and a main study involving over 3000 students from the final year of high school and introductory university statistics, we use preliminary results to conjecture potential conceptual issues and obstacles. In imitation of an insightful paper of Biehler (1997), we identify seven problem areas and difficulties of students related to us- ing bootstrapping and randomization inferential methods from our research. Alt- hough dynamic visualizations have the power to reveal chance variation and the depth of the conceptual structure underpinning the methods in ways that were not previously accessible, the identified areas indicate that attention to the necessity of precise verbal descriptions and the nature of the argumentation are important. In accord with Biehler we surmise that we may need to develop a habit of mind in students that is orientated towards a careful interpretation and understanding of graph visualizations.},
  chapter = {Students' difficulties in practicing computer-supported data analysis: Some hypothetical generalizations from results of two exploratory studies},
  keywords = {to read}
}

@article{phelps2017,
  title = {The {{Current Landscape}} of {{Teaching Analytics}} to {{Business Students}} at {{Institutions}} of {{Higher Education}}: {{Who}} Is {{Teaching What}}?},
  shorttitle = {The {{Current Landscape}} of {{Teaching Analytics}} to {{Business Students}} at {{Institutions}} of {{Higher Education}}},
  author = {Phelps, Amy L. and Szabat, Kathryn A.},
  year = {2017},
  month = apr,
  journal = {The American Statistician},
  volume = {71},
  number = {2},
  pages = {155--161},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2016.1277160},
  urldate = {2021-06-24},
  langid = {english}
}

@article{pickle2015,
  title = {{{micromapST}}: {{Exploring}} and {{Communicating Geospatial Patterns}} in {{US State Data}}},
  author = {Pickle, Linda Williams and James B Pearson, {\relax Jr}. and Carr, Daniel B.},
  year = {2015},
  journal = {Journal of Statistical Software},
  volume = {63},
  number = {3},
  keywords = {to read}
}

@techreport{pimenteletal2022,
  title = {Tools to {{Support Data Analysis}} and {{Data Science}} in {{K-12 Education}}},
  author = {Pimentel, Daniel R and Horton, Nicholas J. and Wilkerson, Michelle Hoda},
  year = {2022},
  month = sep,
  institution = {{National Academy of Sciences, Engineering, and Medicine}}
}

@inproceedings{plaue2015,
  title = {Data Journalism: {{Lessons}} Learned While Designing an Interdisciplinary Service Course},
  booktitle = {{{SIGCSE}}'15},
  author = {Plaue, Christopher and Cook, Lindsey R.},
  year = {2015}
}

@article{polito2014,
  title = {The Language of Comparisons: {{Communicating}} about Percentages},
  author = {Polito, Jessica},
  year = {2014},
  journal = {Numeracy},
  volume = {7},
  number = {1},
  abstract = {While comparisons between percentages or rates appear frequently in journalism and advertising, and are an essential component of quantitative writing, many students fail to understand precisely what percentages mean, and lack fluency with the language used for comparisons. After reviewing evidence demonstrating this weakness, this experience-based perspective lays out a framework for teaching the language of comparisons in a structured way, and illustrates it with several authentic examples that exemplify mistaken or misleading uses of such numbers. The framework includes three common types of erroneous or misleading quantitative writing: the missing comparison, where a key number is omitted; the apples-to-pineapples comparison, where two subtly incomparable rates are presented; and the implied fallacy, where an invalid quantitative conclusion is left to the reader to infer.},
  keywords = {to read}
}

@article{pollatseketal1981,
  title = {Concept or {{Computation}}: {{Students}}' {{Understanding}} of the {{Mean}}},
  shorttitle = {Concept or Computation},
  author = {Pollatsek, A. and Lima, S. and Well, A. D.},
  year = {1981},
  month = may,
  journal = {Educational Studies in Mathematics},
  volume = {12},
  number = {2},
  pages = {191--204},
  issn = {0013-1954, 1573-0816},
  doi = {10.1007/BF00305621},
  urldate = {2021-06-24},
  langid = {english}
}

@article{popatstarkey2019,
  title = {Learning to Code or Coding to Learn? {{A}} Systematic Review},
  shorttitle = {Learning to Code or Coding to Learn?},
  author = {Popat, Shahira and Starkey, Louise},
  year = {2019},
  month = jan,
  journal = {Computers \& Education},
  volume = {128},
  pages = {365--376},
  issn = {03601315},
  doi = {10.1016/j.compedu.2018.10.005},
  urldate = {2021-07-27},
  langid = {english}
}

@article{porteretal2013,
  title = {Success in Introductory Programming: What Works?},
  shorttitle = {Success in Introductory Programming},
  author = {Porter, Leo and Guzdial, Mark and McDowell, Charlie and Simon, Beth},
  year = {2013},
  month = aug,
  journal = {Communications of the ACM},
  volume = {56},
  number = {8},
  pages = {34--36},
  issn = {0001-0782},
  doi = {10.1145/2492007.2492020},
  urldate = {2022-07-21},
  abstract = {How pair programming, peer instruction, and media computation have improved computer science education.},
  file = {/Users/amcnamara/Zotero/storage/XG32MY5Z/Porter et al. - 2013 - Success in introductory programming what works.pdf}
}

@misc{positpbc2023,
  title = {{{RStudio Community}}},
  author = {{Posit PBC}},
  year = {2023},
  month = jan,
  journal = {RStudio Community},
  urldate = {2023-01-11},
  abstract = {I have seen the documentation of the chunk options lst-cap and lst-label in Quarto, but I can't figure out how they work in practice. Here's some example code that does not work:  --- title: "Untitled" format: html editor: visual ---  ```\{r\} \#{\textbar} lst-label: lst-code \#{\textbar} lst-cap: "Some R code" a {$<$}- 1 a+1 ```  I'd like to be able to reference the code chunk @lst-code.  Am I missing something about this?},
  howpublished = {https://community.rstudio.com/},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/CR4DP9LC/community.rstudio.com.html}
}

@article{pousman2007,
  title = {Casual Information Visualization: {{Depictions}} of Data in Everyday Life},
  author = {Pousman, Zachary and Stasko, John T. and Mateas, Michael},
  year = {2007},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {13},
  number = {6},
  pages = {1145--1152},
  abstract = {Abstract---Information visualization has often focused on providing deep insight for expert user populations and on techniques for amplifying cognition through complicated interactive visual models. This paper proposes a new subdomain for infovis research that complements the focus on analytic tasks and expert use. Instead of work-related and analytically driven infovis, we propose Casual Information Visualization (or Casual Infovis) as a complement to more traditional infovis domains. Traditional infovis systems, techniques, and methods do not easily lend themselves to the broad range of user populations, from expert to novices, or from work tasks to more everyday situations. We propose definitions, perspectives, and research directions for further investigations of this emerging subfield. These perspectives build from ambient information visualization [32], social visualization, and also from artistic work that visualizes information [41]. We seek to provide a perspective on infovis that integrates these research agendas under a coherent vocabulary and framework for design. We enumerate the following contributions. First, we demonstrate how blurry the boundary of infovis is by examining systems that exhibit many of the putative properties of infovis systems, but perhaps would not be considered so. Second, we explore the notion of insight and how, instead of a monolithic definition of insight, there may be multiple types, each with particular characteristics. Third, we discuss design challenge}
}

@book{powell2014,
  title = {{{CSV}} Fingerprints},
  author = {Powell, Victor},
  year = {2014},
  month = aug
}

@inproceedings{pozdnoukhov2011,
  title = {Area-to-Point Kernel Regression on Streaming Data},
  booktitle = {{{IWGS}}'11 {{Workshop}} at {{ACM GIS}}'11},
  author = {Pozdnoukhov, Alexei and Kaiser, Christian},
  year = {2011},
  abstract = {Spatial data streams are often referenced to an areal spatial unit such as a polygon rather than to a precise point location. This is the case when geo-referencing is done by user IP addresses or from a mobile phone cell ID in var- ious location-based service applications. One problem of interest in this case is spatial modeling of various spatially continuous quantities, such as an intensity of the usage of particular service in the area. This paper investigates a machine learning framework that account for area-to-point data processing. The approach is based on so-called vicinal risk minimization principle. It is elaborated in detail for a class of kernel recursive algorithms developed for distributed processing of streaming data. Concrete examples of kernel computations are provided and the method performance is investigated experimentally.},
  keywords = {to read}
}

@misc{prolific2021,
  title = {Prolific},
  author = {Prolific},
  year = {2021}
}

@book{pruim2014,
  title = {Minimal {{R}} for Intro Stats},
  author = {Pruim, Randall},
  year = {2014},
  month = aug,
  keywords = {to read}
}

@book{pruim2014a,
  title = {Start Teaching with {{R}}},
  author = {Pruim, Randall and Horton, Nicholas J. and Kaplan, Daniel},
  year = {2014},
  publisher = {Project MOSAIC}
}

@book{pruim2015,
  title = {Mosaic: {{Project MOSAIC}} (Mosaic-Web.Org) Statistics and Mathematics Teaching Utilities},
  author = {Pruim, Randall and Kaplan, Daniel and Horton, Nicholas J.},
  year = {2015},
  publisher = {R package version 0.9.2-2}
}

@manual{pruimetal2011,
  type = {manual},
  title = {\texttt{mosaic}: Project MOSAIC Statistics and Mathematics Teaching Utilities},
  author = {Pruim, Randall and Kaplan, Daniel T. and Horton, Nicholas J.},
  year = {2011}
}

@manual{pruimetal2015,
  type = {{manual}},
  title = {{mosaic: Project MOSAIC Statistics and Mathematics Teaching Utilities}},
  author = {Pruim, Randall and Kaplan, Daniel and Horton, Nicholas J.},
  year = {2015},
  langid = {dutch}
}

@article{pruimetal2017,
  title = {The \texttt{mosaic} {Package: Helping Students} {`Think with Data'} using {R}},
  author = {Pruim, Randall and Kaplan, Daniel and Horton, Nicholas J.},
  year = {2017},
  journal = {The R Journal},
  volume = {9},
  number = {1}
}

@article{pruimetal2023,
  title = {Fostering {{Better Coding Practices}} for {{Data Scientists}}},
  author = {Pruim, Randall and G{\^i}rj{\u a}u, Maria-Cristiana and Horton, Nicholas J.},
  year = {2023},
  month = jul,
  journal = {Harvard Data Science Review},
  volume = {5},
  number = {3},
  issn = {2644-2353, 688-8513},
  doi = {10.1162/99608f92.97c9f60f},
  urldate = {2024-06-12},
  abstract = {Many data science students and practitioners do not see the value in making time to learn and adopt good coding practices as long as the code `works.' However, code standards are an important part of modern data science practice, and they play an essential role in the development of data acumen. Good coding practices lead to more reliable code and save more time than they cost, making them important even for beginners. We believe that principled coding is vital for quality data science practice. To effectively instill these practices within academic programs, instructors and programs need to begin establishing these practices early, reinforce them often, and hold themselves to a higher standard while guiding students. We describe key aspects of good coding practices for data science, illustrating with examples in R and in Python, though similar standards are applicable to other software environments. Practical coding guidelines are organized into a top 10 list.},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/JCCAUI7F/Pruim et al. - 2023 - Fostering Better Coding Practices for Data Scienti.pdf}
}

@inproceedings{pruitt2003,
  title = {Personas: {{Practice}} and Theory},
  booktitle = {Proceedings of the 2003 Conference on {{Designing}} for User Experiences},
  author = {Pruitt, John and Grudin, Jonathan},
  year = {2003},
  publisher = {ACM}
}

@article{qian2017,
  title = {Students' {{Misconceptions}} and {{Other Difficulties}} in {{Introductory Programming}}: {{A Literature Review}}},
  shorttitle = {Students' {{Misconceptions}} and {{Other Difficulties}} in {{Introductory Programming}}},
  author = {Qian, Yizhou and Lehman, James},
  year = {2017},
  month = dec,
  journal = {ACM Transactions on Computing Education},
  volume = {18},
  number = {1},
  pages = {1--24},
  issn = {1946-6226},
  doi = {10.1145/3077618},
  urldate = {2021-06-07},
  abstract = {Efforts to improve computer science education are underway, and teachers of computer science are challenged in introductory programming courses to help learners develop their understanding of programming and computer science. Identifying and addressing students' misconceptions is a key part of a computer science teacher's competence. However, relevant research on this topic is not as fully developed in the computer science education field as it is in mathematics and science education. In this article, we first review relevant literature on general definitions of misconceptions and studies about students' misconceptions and other difficulties in introductory programming. Next, we investigate the factors that contribute to the difficulties. Finally, strategies and tools to address difficulties including misconceptions are discussed.             Based on the review of literature, we found that students exhibit various misconceptions and other difficulties in syntactic knowledge, conceptual knowledge, and strategic knowledge. These difficulties experienced by students are related to many factors including unfamiliarity of syntax, natural language, math knowledge, inaccurate mental models, lack of strategies, programming environments, and teachers' knowledge and instruction. However, many sources of students' difficulties have connections with students' prior knowledge. To better understand and address students' misconceptions and other difficulties, various instructional approaches and tools have been developed. Nevertheless, the dissemination of these approaches and tools has been limited. Thus, first, we suggest enhancing the dissemination of existing tools and approaches and investigating their long-term effects. Second, we recommend that computing education research move beyond documenting misconceptions to address the development of students' (mis)conceptions by integrating conceptual change theories. Third, we believe that developing and enhancing instructors' pedagogical content knowledge (PCK), including their knowledge of students' misconceptions and ability to apply effective instructional approaches and tools to address students' difficulties, is vital to the success of teaching introductory programming.},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/BZ46K2WT/Qian and Lehman - 2017 - Students’ Misconceptions and Other Difficulties in.pdf}
}

@misc{r4ds2023,
  title = {R for {{Data Science Online Learning Community}}},
  author = {{R4DS}},
  year = {2023},
  urldate = {2023-01-11},
  abstract = {R4DS Online Learning Community is a supportive and responsive online space for learners and mentors to gather and work through the R for Data Science},
  howpublished = {//},
  langid = {american},
  file = {/Users/amcnamara/Zotero/storage/PAXMILFL/www.rfordatasci.com.html}
}

@inproceedings{rafalskietal2019,
  title = {A {{Randomized Controlled Trial}} on the {{Wild Wild West}} of {{Scientific Computing}} with {{Student Learners}}},
  booktitle = {Proceedings of the 2019 {{ACM Conference}} on {{International Computing Education Research}}},
  author = {Rafalski, Timothy and Uesbeck, P Merlin and {Panks-Meloney}, Cristina and Daleiden, Patrick and Allee, William and McNamara, Amelia and Stefik, Andreas},
  year = {2019},
  pages = {239--247}
}

@inproceedings{ragan-kelley2014,
  title = {The {{Jupyter}}/{{IPython}} Architecture: A Unified View of Computational Research, from Interactive Exploration to Communication and Publication.},
  booktitle = {American {{Geophysical Union}}},
  author = {{Ragan-Kelley}, M. and Perez, F. and Granger, B. and Kluyver, T. and Ivanov, P. and Frederic, J. and Bussonier, M.},
  year = {2014}
}

@article{raja2014,
  title = {We Can Code It! {{Why}} Computer Literacy Is Key to Winning the 21st Century},
  author = {Raja, Tasneem},
  year = {2014},
  journal = {Mother Jones}
}

@article{rakotondravonyetal2023,
  title = {Probablement, {{Wahrscheinlich}}, {{Likely}}? {{A Cross-Language Study}} of {{How People Verbalize Probabilities}} in {{Icon Array Visualizations}}},
  shorttitle = {Probablement, {{Wahrscheinlich}}, {{Likely}}?},
  author = {Rakotondravony, No{\"e}lle and Ding, Yiren and Harrison, Lane},
  year = {2023},
  month = jan,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {29},
  number = {1},
  pages = {1189--1199},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2022.3209367},
  abstract = {Visualizations today are used across a wide range of languages and cultures. Yet the extent to which language impacts how we reason about data and visualizations remains unclear. In this paper, we explore the intersection of visualization and language through a cross-language study on estimative probability tasks with icon-array visualizations. Across Arabic, English, French, German, and Mandarin, n=50 participants per language both chose probability expressions --- e.g. likely, probable --- to describe icon-array visualizations (Vis-to-Expression), and drew icon-array visualizations to match a given expression (Expression-to-Vis). Results suggest that there is no clear one-to-one mapping of probability expressions and associated visual ranges between languages. Several translated expressions fell significantly above or below the range of the corresponding English expressions. Compared to other languages, French and German respondents appear to exhibit high levels of consistency between the visualizations they drew and the words they chose. Participants across languages used similar words when describing scenarios above 80\% chance, with more variance in expressions targeting mid-range and lower values. We discuss how these results suggest potential differences in the expressiveness of language as it relates to visualization interpretation and design goals, as well as practical implications for translation efforts and future studies at the intersection of languages, culture, and visualization. Experiment data, source code, and analysis scripts are available at the following repository: https://osf.io/g5d4r/.},
  keywords = {Climate change,Codes,Cross-Language Study,Data visualization,Icon-Arrays,Pandemics,Sociology,Soft sensors,Visualization},
  file = {/Users/amcnamara/Zotero/storage/H79T4X4H/Rakotondravony et al. - 2023 - Probablement, Wahrscheinlich, Likely A Cross-Lang.pdf;/Users/amcnamara/Zotero/storage/TAU2GMYS/9904569.html}
}

@inproceedings{ranum2006,
  title = {Successful Approaches to Teaching Introductory Computer Science Courses with Python},
  booktitle = {{{SIGCSE}}'06},
  author = {Ranum, David and Miller, Bradley and Zelle, John and Guzdial, Mark},
  year = {2006}
}

@article{rautek2014,
  title = {{{ViSlang}}: {{A}} System for Interpreted Domain-Specific Languages for Scientific Visualization},
  author = {Rautek, Peter and Bruckner, Stefan and Gr{\"o}ller, M. Eduard and Hadwiger, Markus},
  year = {2014},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {20},
  number = {12},
  abstract = {Researchers from many domains use scientific visualization in their daily practice. Existing implementations of algorithms usually come with a graphical user interface (high-level interface), or as software library or source code (low-level interface). In this paper we present a system that integrates domain-specific languages (DSLs) and facilitates the creation of new DSLs. DSLs provide an effective interface for domain scientists avoiding the difficulties involved with low-level interfaces and at the same time offering more flexibility than high-level interfaces. We describe the design and implementation of ViSlang, an interpreted language specifically tailored for scientific visualization. A major contribution of our design is the extensibility of the ViSlang language. Novel DSLs that are tailored to the problems of the domain can be created and integrated into ViSlang. We show that our approach can be added to existing user interfaces to increase the flexibility for expert users on demand, but at the same time does not interfere with the user experience of novice users. To demonstrate the flexibility of our approach we present new DSLs for volume processing, querying and visualization. We report the implementation effort for new DSLs and compare our approach with Matlab and Python implementations in terms of run-time performance.},
  keywords = {experimental,IEEEviz}
}

@techreport{rcoreteam2009,
  title = {An Introduction to {{R}}},
  author = {{R Core Team}},
  year = {2009}
}

@book{rcoreteam2015,
  title = {Comprehensive {{R Archive Network}}},
  author = {{R Core Team}},
  year = {2015}
}

@book{rcoreteam2020,
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2020},
  publisher = {R Foundation for Statistical Computing},
  address = {Vienna, Austria}
}

@inproceedings{redmond2019,
  title = {Visual {{Cues}} in {{Estimation}} of {{Part-To-Whole Comparisons}}},
  booktitle = {2019 {{IEEE Visualization Conference}} ({{VIS}})},
  author = {Redmond, Stephen},
  year = {2019},
  month = oct,
  pages = {1--5},
  doi = {10.1109/VISUAL.2019.8933718},
  abstract = {Pie charts were first published in 1801 by William Playfair and have caused some controversy since. Despite the suggestions of many experts against their use, several empirical studies have shown that pie charts are at least as good as alternatives. From Brinton to Few on one side and Eells to Kosara on the other, there appears to have been a hundred-year war waged on the humble pie. In this paper a set of experiments are reported that compare the performance of pie charts and horizontal bar charts with various visual cues. Amazon's Mechanical Turk service was employed to perform the tasks of estimating segments in various part-to-whole charts. The results lead to recommendations for data visualization professionals in developing dashboards.},
  keywords = {Bars,Data visualization,Empirical studies in visualization,Estimation,Human computer interaction,Human-centered computing,Indexes,Task analysis,Visualization,Visualization design and evaluation methods},
  file = {/Users/amcnamara/Zotero/storage/ZARDPWCY/Redmond - 2019 - Visual Cues in Estimation of Part-To-Whole Compari.pdf}
}

@article{reid2002,
  title = {Students' Conceptions of Statistics: {{A}} Phenomenographic Study},
  author = {Reid, Anna and Petocz, Peter},
  year = {2002},
  journal = {Journal of statistics education},
  volume = {10},
  number = {2},
  keywords = {education,statistics}
}

@article{reinhartetal2022,
  title = {Think-{{Aloud Interviews}}: {{A Tool}} for {{Exploring Student Statistical Reasoning}}},
  shorttitle = {Think-{{Aloud Interviews}}},
  author = {Reinhart, Alex and Evans, Ciaran and Luby, Amanda and Orellana, Josue and Meyer, Mikaela and Wieczorek, Jerzy and Elliott, Peter and Burckhardt, Philipp and Nugent, Rebecca},
  year = {2022},
  month = may,
  journal = {Journal of Statistics and Data Science Education},
  volume = {30},
  number = {2},
  pages = {100--113},
  publisher = {Taylor \& Francis},
  issn = {null},
  doi = {10.1080/26939169.2022.2063209},
  urldate = {2023-06-06},
  abstract = {Think-aloud interviews have been a valuable but underused tool in statistics education research. Think-alouds, in which students narrate their reasoning in real time while solving problems, differ in important ways from other types of cognitive interviews and related education research methods. Beyond the uses already found in the statistics literature---mostly validating the wording of statistical concept inventory questions and studying student misconceptions---we suggest other possible use cases for think-alouds and summarize best-practice guidelines for designing think-aloud interview studies. Using examples from our own experiences studying the local student body for our introductory statistics courses, we illustrate how research goals should inform study-design decisions and what kinds of insights think-alouds can provide. We hope that our overview of think-alouds encourages more statistics educators and researchers to begin using this method. Supplementary materials for this article are available online.},
  keywords = {Correlation and causation,Misconceptions,Sampling distributions,Think-aloud interviews},
  file = {/Users/amcnamara/Zotero/storage/H4GRJ6N7/Reinhart et al. - 2022 - Think-Aloud Interviews A Tool for Exploring Stude.pdf}
}

@book{repenning2010,
  title = {Scalable Game Design and the Development of a Checklist for Getting Computational Thinking into Public Schools},
  author = {Repenning, Alexander and Webb, David and Ioannidou, Andri},
  year = {2010},
  annotation = {Published: SIGCSE'10, https://dl.acm.org/citation.cfm?id=1734357}
}

@article{resnicketal2009,
  title = {Scratch: {{Programming}} for {{All}}},
  author = {Resnick, Mitchel and Maloney, John and {Monroy-Hernandez}, Andreas and Rusk, Natalie and Eastmond, Evelyn and Brennan, Karen and Millner, Amon and Rosenbaum, Eric and Silver, Jay and Silverman, Brian and Kafai, Yasmin},
  year = {2009},
  journal = {Communications of the ACM},
  volume = {52},
  number = {11},
  pages = {60--67}
}

@book{ribeiro2014,
  title = {Visualizing Data Manipulation Operations},
  author = {Ribeiro, B{\'a}rbara Borges},
  year = {2014}
}

@book{rich,
  title = {{{mosaicManip}}: {{Interactive Applets}} for {{Teaching}} with {{R}}},
  author = {Rich, Andrew and Kaplan, Daniel and Pruim, Randall and Horton, Nicholas J. and Allaire, J. J.}
}

@article{ridgeway2016,
  title = {Implications of the {{Data Revolution}} for {{Statistics Education}}},
  author = {Ridgeway, Jim},
  year = {2016},
  journal = {International Statistical Review},
  volume = {84},
  number = {3},
  pages = {528--549}
}

@article{ridgway2016,
  title = {Implications of the {{Data Revolution}} for {{Statistics Education}}},
  author = {Ridgway, Jim},
  year = {2016},
  journal = {International Statistical Review},
  volume = {84},
  number = {3},
  pages = {528--549},
  issn = {1751-5823},
  doi = {10.1111/insr.12110},
  urldate = {2023-07-07},
  abstract = {There has never been a more exciting time to be involved in statistics. Emerging data sources provide new sorts of evidence, provoke new sorts of questions, make possible new sorts of answers and shape the ways that evidence is used to influence policy, public opinion and business practices. Significant developments include open data, big data, data visualisation and the rise of data-driven journalism. These developments are changing the nature of the evidence that is available, the ways in which it is presented and used and the skills needed for its interpretation. Educators should place less emphasis on small samples and linear models and more emphasis on large samples, multivariate description and data visualisation. Techniques used to analyse big data need to be taught. The increasing diversity of data usage requires deeper conceptual analysis in the curriculum; this should include explorations of the functions of modelling, and the politics of data and ethics. The data revolution can invigorate the existing curriculum by exemplifying the perils of biassed sampling, corruption of measures and modelling failures. Students need to learn to think statistically and to develop an aesthetic for data handling and modelling based on solving practical problems.},
  copyright = {{\copyright} 2015 The Authors. International Statistical Review published by John Wiley \& Sons Ltd on behalf of International Statistical Institute.},
  langid = {english},
  keywords = {big data,change,curriculum,data-driven journalism,modelling,open data,statistical literacy,statistics education,visualisation},
  file = {/Users/amcnamara/Zotero/storage/YKSDJZXT/Ridgway - 2016 - Implications of the Data Revolution for Statistics.pdf}
}

@article{riedeletal2020,
  title = {{{ODDPub}} -- a {{Text-Mining Algorithm}} to {{Detect Data Sharing}} in {{Biomedical Publications}}},
  author = {Riedel, Nico and Kip, Miriam and Bobrov, Evgeny},
  year = {2020},
  month = oct,
  journal = {Data Science Journal},
  volume = {19},
  number = {1},
  pages = {42},
  publisher = {Ubiquity Press},
  issn = {1683-1470},
  doi = {10.5334/dsj-2020-042},
  urldate = {2022-07-15},
  abstract = {Article: ODDPub -- a Text-Mining Algorithm to Detect Data Sharing in Biomedical Publications},
  copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are {\copyright}, {\textregistered} or ™ of their respective owners. No challenge to any owner's rights is intended or should be inferred.},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/XJ9ZWWHC/Riedel et al. - 2020 - ODDPub – a Text-Mining Algorithm to Detect Data Sh.pdf;/Users/amcnamara/Zotero/storage/JLNNUNF4/dsj-2020-042.html}
}

@misc{riedeletal2020a,
  title = {{{ODDPub}} -- a {{Text-Mining Algorithm}} to {{Detect Data Sharing}} in {{Biomedical Publications}}},
  author = {Riedel, Nico and Kip, Miriam and Bobrov, Evgeny},
  year = {2020},
  month = may,
  primaryclass = {New Results},
  pages = {2020.05.11.088021},
  publisher = {bioRxiv},
  doi = {10.1101/2020.05.11.088021},
  urldate = {2022-07-15},
  abstract = {Open research data are increasingly recognized as a quality indicator and an important resource to increase transparency, robustness and collaboration in science. However, no standardized way of reporting Open Data in publications exists, making it difficult to find shared datasets and assess the prevalence of Open Data in an automated fashion. We developed ODDPub (Open Data Detection in Publications), a text-mining algorithm that screens biomedical publications and detects cases of Open Data. Using English-language original research publications from a single biomedical research institution (n=8689) and randomly selected from PubMed (n=1500) we iteratively developed a set of derived keyword categories. ODDPub can detect data sharing through field-specific repositories, general-purpose repositories or the supplement. Additionally, it can detect shared analysis code (Open Code). To validate ODDPub, we manually screened 792 publications randomly selected from PubMed. On this validation dataset, our algorithm detected Open Data publications with a sensitivity of 0.74 and specificity of 0.97. Open Data was detected for 11.5\% (n=91) of publications. Open Code was detected for 1.4\% (n=11) of publications with a sensitivity of 0.73 and specificity of 1.00. We compared our results to the linked datasets found in the databases PubMed and Web of Science. Our algorithm can automatically screen large numbers of publications for Open Data. It can thus be used to assess Open Data sharing rates on the level of subject areas, journals, or institutions. It can also identify individual Open Data publications in a larger publication corpus. ODDPub is published as an R package on GitHub.},
  archiveprefix = {bioRxiv},
  chapter = {New Results},
  copyright = {{\copyright} 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/EFL7XEGP/Riedel et al. - 2020 - ODDPub – a Text-Mining Algorithm to Detect Data Sh.pdf;/Users/amcnamara/Zotero/storage/ZF42YZDP/2020.05.11.088021v1.html}
}

@inproceedings{rimland2012,
  title = {Advances in Data Representation for Hard/Soft Information Fusion},
  booktitle = {Multisensor, Multisource Information Fusion: Architectures, Algorithms and Applications},
  author = {Rimland, Jeffrey C. and Coughlin, Dan and Hall, David L. and Graham, Jacob L.},
  editor = {Braun, Jerome J.},
  year = {2012},
  volume = {8407},
  keywords = {computer science,data,statistics}
}

@inproceedings{rising2014,
  title = {Reproducible Research in {{Stata}}},
  booktitle = {12th {{German Stata Users Group Meeting}}},
  author = {Rising, Bill},
  year = {2014}
}

@book{roback2021,
  title = {Beyond Multiple Linear Regression: Applied Generalized Linear Models and Multilevel Models In},
  shorttitle = {Beyond Multiple Linear Regression},
  author = {Roback, Paul and Legler, Julie},
  year = {2021},
  series = {Chapman and {{Hall Texts}} in {{Statistical Science}}},
  edition = {1},
  publisher = {CRC Press},
  address = {Boca Raton},
  abstract = {"Beyond Multiple Linear Regression: Applied Generalized Linear Models and Multilevel Models in R is designed for undergraduate students who have successfully completed a multiple linear regression course, helping them develop an expanded modeling toolkit that includes non-normal responses and correlated structure. Even though there is no mathematical prerequisite, Beyond Multiple Linear Regression still introduces fairly sophisticated topics such as likelihood theory, zero-inflated Poisson, and parametric bootstrapping in an intuitive and applied manner. Thecase studies and exercises feature real data and real research questions; thus, most of the data in the textbook comes from collaborative research conducted by the authors and their students, or from student projects. Every chapter features a variety of conceptual exercises, guided exercises, and open-ended exercises using real data. After working through this material, students will develop an expanded toolkit and a greater appreciation for the wider world of data and statistical modeling"--},
  isbn = {978-1-4398-8538-3}
}

@article{robbinsetal2003,
  title = {Learning and {{Teaching Programming}}: {{A Review}} and {{Discussion}}},
  author = {Robbins, Anthony and Rountree, Janet and Roundtree, Nathan},
  year = {2003},
  journal = {Computer Science Education},
  volume = {13},
  number = {2},
  pages = {137--172},
  abstract = {In this paper we review the literature relating to the psychological/educational study of programming. We identify general trends comparing novice and expert programmers, programming knowledge and strategies, program generation and comprehension, and object- oriented versus procedural programming. (We do not cover research relating specifically to other programming styles.) The main focus of the review is on novice programming and topics relating to novice teaching and learning. Various problems experienced by novices are identified, including issues relating to basic program design, to algorithmic complexity in certain language features, to the ``fragility'' of novice knowledge, and so on. We summarise this material and suggest some practical implications for teachers. We suggest that a key issue that emerges is the distinction between effective and ineffective novices. What characterises effective novices? Is it possible to identify the specific deficits of ineffective novices and help them to become effective learners of programming?},
  keywords = {computer science}
}

@phdthesis{roberts2015,
  title = {Measuring {{Formative Learning Behaviors}} of {{Introductory Statistical Programming}} in {{R}} via {{Content Clustering}}},
  author = {Roberts, Shane},
  year = {2015},
  school = {University of California, Los Angeles}
}

@book{robinson2014,
  title = {Don't Teach Built-in Plotting to Beginners (Teach Ggplot2)},
  author = {Robinson, David},
  year = {2014},
  month = dec
}

@misc{robinson2015,
  title = {Exploring {{Careers Data}} with Sqlstackr, Dplyr, and Ggplot2: {{Interal Stack Overflow Tutorial}}},
  author = {Robinson, David},
  year = {2015},
  journal = {RPubs},
  urldate = {2022-03-30},
  file = {/Users/amcnamara/Zotero/storage/CT7KUFNG/190325.html}
}

@book{robinson2016,
  title = {Broom: {{Convert}} Statistical Analysis Objects into Tidy Data Frames},
  author = {Robinson, David},
  year = {2016}
}

@misc{robinson2017,
  title = {Teach the Tidyverse to {{Beginners}}},
  author = {Robinson, David},
  year = {2017},
  month = jul,
  howpublished = {http://varianceexplained.org/r/teach-tidyverse/}
}

@book{robinsonnolis2020,
  title = {Build a {{Career}} in {{Data Science}}},
  author = {Robinson, Emily and Nolis, Jacqueline},
  year = {2020},
  publisher = {Manning},
  urldate = {2021-07-14},
  abstract = {You are going to need more than technical knowledge to succeed as a data scientist. Build a Career in Data Science teaches you what school leaves out, from how to land your first job to the lifecycle of a data science project, and even how to become a manager.},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/PIDDGPVX/build-a-career-in-data-science.html}
}

@article{rogati2014,
  title = {The Rise of the Data Natives},
  author = {Rogati, Monica},
  year = {2014},
  month = apr,
  journal = {{$<$}re/code{$>$}}
}

@book{rogers1962,
  title = {Diffusion of {{Innovations}}},
  author = {Rogers, Everett M.},
  year = {1962},
  publisher = {Free Press}
}

@inproceedings{romeike2008,
  title = {Applying {{Creativity}} in {{CS High School Education}} -- {{Criteria}}, {{Teaching Example}} and {{Evaluation}}},
  booktitle = {Seventh {{Baltic Sea Conference}} on {{Computing Educational Research}}},
  author = {Romeike, Ralf},
  year = {2008},
  publisher = {Australian Computer Society, Inc},
  abstract = {This paper describes an innovative method for teaching computer science in general high school education, illustrated with the example of introductory programming. Analyzing the literature in CS education research we found that creativity is rarely regarded, especially in high school education; although a few authors describe promising results from applying creativity. We designed and applied a framework for designing creative CS lessons based on a set of creativity criteria. The conducted teaching unit on introductory programming fulfilled the expectations: the students learned with high motivation and interest, the learning objectives were met and the students' picture of CS improved}
}

@inproceedings{rongasetal2004,
  title = {Classification of {{Computerized Learning Tools}} for {{Introductory Programming Courses}}: {{Learning Approach}}},
  booktitle = {Proceedings on the {{IEEE International Conference}} on {{Advanced Learning Technologies}}},
  author = {Rongas, Timo and Kaarna, Arto and K{\"a}lvi{\"a}inen, Heikki},
  year = {2004},
  keywords = {computer science,education}
}

@unpublished{ropensci,
  title = {{{rOpenSci}} - {{Open Tools}} for {{Open Science}}},
  author = {{rOpenSci}}
}

@article{rosenbergetal,
  title = {Big Data, Big Changes? {{The}} Technologies and Sources of Data Used in Science Classrooms},
  shorttitle = {Big Data, Big Changes?},
  author = {Rosenberg, Joshua M. and Schultheis, Elizabeth H. and Kjelvik, Melissa K. and Reedy, Aaron and Sultana, Omiya},
  journal = {British Journal of Educational Technology},
  volume = {n/a},
  number = {n/a},
  issn = {1467-8535},
  doi = {10.1111/bjet.13245},
  urldate = {2022-07-12},
  abstract = {With improving technology and monitoring efforts, the availability of scientific data is rapidly expanding. The tools that scientists and engineers use to analyse data are changing in response. At the same time, science education standards have shifted to emphasize the importance of students making sense of data in science classrooms. However, it is not yet known whether these exciting new datasets and tools are used science classrooms, and what it would take to facilitate their use. To identify opportunities, research is needed to capture the data practices currently performed in classrooms, and the roles of technology for student learning. Here, we report findings from a survey conducted in the United States of 330 science teachers on the data sources, practices and technologies common to their classroom. We found that teachers predominantly involve their students in analysing relatively small data sets that they collect. In support of this work, teachers tend to use the technologies that are available to them---namely, calculators and spreadsheets. In addition, we found that a subset of teachers used a wide variety of data sources of varying complexity. We discuss what these findings suggest for practice, research and policy, with an emphasis on supporting teachers based on their needs. Practitioner notes What is already known about this topic Collecting and analysing data are central to the practice of science, and these skills are taught in many science classrooms at the pre-collegiate (grades K-12) level. Data are increasingly important in society and STEM, and types and sources of data are rapidly expanding. These changes have implications for science teachers and students. What this paper adds We found that the predominant data source science teachers use is student-collected, small data sets. Teachers use digital tools familiar and available to them: spreadsheets and calculators. Teachers perceive the cost and time it would take to learn to use digital tools to analyse data with their students as key barriers to adopting new tools. Despite the predominance of small, student-collected data analysed using spreadsheets or calculators, we also found notable variability in the data sources and digital tools some teachers used with their students. Implications for practice and/or policy Many of the changes called for in science education standards and reform documents, regarding how students should collect and analyse data, have not yet been fully realized in pre-collegiate classrooms. Science teacher educators and science education researchers should build curricula and develop digital tools based on which kinds of data sources and digital tools teachers presently use, while encouraging more complex data useage in the future.},
  langid = {english},
  keywords = {analysing and interpreting data,data literacy,data science,science education,survey research methods}
}

@article{rosenbergetal2019,
  title = {Making {{Data Science Count In}} and {{For Education}}},
  author = {Rosenberg, Joshua M. and Lawson, Michael and Anderson, Daniel J. and Jones, Ryan Seth and Rutherford, Teomara},
  year = {2019}
}

@article{rosenbergetal2022,
  title = {Big Data, Big Changes? {{The}} Technologies and Sources of Data Used in Science Classrooms},
  shorttitle = {Big Data, Big Changes?},
  author = {Rosenberg, Joshua M. and Schultheis, Elizabeth H. and Kjelvik, Melissa~K. and Reedy, Aaron and Sultana, Omiya},
  year = {2022},
  month = sep,
  journal = {British Journal of Educational Technology},
  volume = {53},
  number = {5},
  pages = {1179--1201},
  issn = {0007-1013, 1467-8535},
  doi = {10.1111/bjet.13245},
  urldate = {2023-02-22},
  langid = {english}
}

@inproceedings{ross2010,
  title = {Who Are the Crowdworkers? {{Shifting}} Demographics in {{Mechanical Turk}}},
  booktitle = {{{CHI}}'10 Extended Abstracts on {{Human}} Factors in Computing Systems.},
  author = {Ross, Joel and Irani, Lilly and Silberman, M. Six and Zaldivar, Andrew and Tomlinson, Bill},
  year = {2010},
  publisher = {ACM},
  abstract = {Amazon Mechanical Turk (MTurk) is a crowdsourcing system in which tasks are distributed to a population of thousands of anonymous workers for completion. This system is increasingly popular with researchers and developers. Here we extend previous studies of the demographics and usage behaviors of MTurk workers. We describe how the worker population has changed over time, shifting from a primarily moderate-income, U.S.-based workforce towards an increasingly international group with a significant population of young, well-educated Indian workers. This change in population points to how workers may treat Turking as a full-time job, which they rely on to make ends meet.},
  keywords = {data,mTurk}
}

@article{ross2018,
  title = {Declutter Your {{R}} Workflow with Tidy Tools},
  author = {Ross, Zev and Wickham, Hadley and Robinson, David},
  year = {2018},
  journal = {The American Statistician},
  volume = {71},
  number = {5}
}

@inproceedings{rossmanchance2004,
  title = {Anticipating and {{Addressing Student Misconceptions}}},
  booktitle = {{{ARTIST Roundtable Conference}} on {{Assessment}} in {{Statistics}}},
  author = {Rossman, Allan and Chance, Beth},
  year = {2004},
  address = {Lawrence University}
}

@misc{rstudiopbc,
  title = {{{RStudio Customer Stories}}},
  author = {{RStudio PBC}},
  urldate = {2022-03-30},
  abstract = {Customer stories with examples of how our professional products perform in the real world. We hope that the challenges they faced and the solutions we provided will resonate with you.},
  howpublished = {https://www.rstudio.com/about/customer-stories/},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/WQG2PZD2/customer-stories.html}
}

@misc{rstudiopbc2021,
  title = {{{RStudio Cloud}} - {{Do}}, {{Share}}, {{Teach}}, and {{Learn Data Science}}},
  author = {{RStudio PBC}},
  year = {2021},
  urldate = {2021-07-24},
  howpublished = {https://rstudio.cloud/},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/ULJ8MJ22/rstudio.cloud.html}
}

@book{rstudioteam2014,
  title = {Movie {{Explorer}}},
  author = {{RStudio Team}},
  year = {2014}
}

@book{rstudioteam2014a,
  title = {{{RStudio}}: {{Integrated Development}} for {{R}}},
  author = {{RStudio Team}},
  year = {2014}
}

@book{rstudioteam2015,
  title = {Data Wrangling with Dplyr and Tidyr: {{Cheat}} Sheet},
  author = {{RStudio Team}},
  year = {2015},
  month = feb
}

@book{rstudioteam2016,
  title = {R {{Notebooks}}},
  author = {{RStudio Team}},
  year = {2016}
}

@book{rstudioteam2018,
  title = {R {{Markdown}}},
  author = {{RStudio Team}},
  year = {2018}
}

@inproceedings{rubin2002,
  title = {Interactive Visualizations of Statistical Relationships: What Do We Gain?},
  booktitle = {Research Papers from {{ICOTS}} 6},
  author = {Rubin, Andee},
  year = {2002},
  keywords = {data visualization,statistics,technology}
}

@article{rubin2007,
  title = {Much Has Changed; Little Has Changed: {{Revisiting}} the Role of Technology in Statistics Education 1992-2007},
  author = {Rubin, Andee},
  year = {2007},
  journal = {Technology Innovations in Statistics Education},
  volume = {1},
  number = {1},
  abstract = {The author of this article reflects on the uses of technology in statistics education, comparing the state of the art as described in her article from 1992 with current developments. She reviews five categories of software: software that uses video as data, Geographical Information Systems, graph construction tools, systems with distribution and data manipulation capabilities, and probability generation tools. Considering how software has changed in the past fifteen years, the author argues that while remarkable technological progress has been made, many of the same pedagogical caveats apply as in 1992. These concerns are an integral part of studying the uses of technology as a learning tool in any content area, so it is important that we put them front and center as this journal begins and keep them there as it grows.},
  keywords = {to read}
}

@article{rubin2021,
  title = {What to Consider When We Consider Data},
  author = {Rubin, Andee},
  year = {2021},
  month = jul,
  journal = {Teaching Statistics},
  volume = {43},
  number = {S1},
  issn = {0141-982X, 1467-9639},
  doi = {10.1111/test.12275},
  urldate = {2021-07-30},
  langid = {english}
}

@article{rubin2021a,
  title = {What to Consider When We Consider Data},
  author = {Rubin, Andee},
  year = {2021},
  journal = {Teaching Statistics},
  volume = {43},
  number = {S1},
  pages = {S23-S33},
  issn = {1467-9639},
  doi = {10.1111/test.12275},
  urldate = {2023-07-07},
  abstract = {The data sets used in statistics education have changed over time, from mathematically ``well-behaved'' ones that facilitated computation, to more context-rich sources and now, with the increasing influence of data science practices, to ``found'' data, often from open data sites. As data sources change, it is important for educators to take a fresh look at the ways we engage students in thinking about the processes that generated the data they encounter. The use of already collected data requires particular attention because many of the decisions that went into the processes of obtaining the data are hidden. Students need to learn to ask ``Who, When, How, Where, and Why?'' data were collected and to wonder if the data really measure what needs to be measured. Our advocacy in this paper is to deepen the educational treatment of data production to better reflect the current and future practice of statistics and data science.},
  copyright = {{\copyright} 2021 Teaching Statistics Trust.},
  langid = {english},
  keywords = {data clubs,data science,measurement,questioning data,teaching,teaching statistics},
  file = {/Users/amcnamara/Zotero/storage/3P5LVIWZ/Rubin - 2021 - What to consider when we consider data.pdf;/Users/amcnamara/Zotero/storage/N5NVYYLG/test.html}
}

@article{rubinetal2006,
  title = {{{EXPLORING INFORMAL INFERENCE WITH INTERACTIVE VISUALIZATION SOFTWARE}}},
  author = {Rubin, Andee and Hammerman, James K L and Konold, Cliff},
  year = {2006},
  journal = {ICOTS7},
  pages = {6},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/8JLVWY2N/Rubin et al. - EXPLORING INFORMAL INFERENCE WITH INTERACTIVE VISU.pdf}
}

@inproceedings{rubinetal2006a,
  title = {Exploring Informal Inference with Interactive Visualization Software},
  booktitle = {Research Papers from {{ICOTS}} 7},
  author = {Rubin, Andee and Hammerman, James K. and Konold, Clifford},
  year = {2006},
  keywords = {computer science,data visualization,statistics,technology}
}

@incollection{rubinhammerman2006,
  title = {Thinking and {{Reasoning}} with {{Data}} and {{Chance}}},
  author = {Rubin, Andee and Hammerman, James K.},
  year = {2006},
  publisher = {National Council of Teachers of Mathematics},
  chapter = {Understanding Data through New Software Representations}
}

@incollection{sack2007,
  title = {Database {{Aesthetics}}: {{Art}} in the Age of Information Overflow},
  author = {Sack, Warren},
  editor = {Vesna, Victoria},
  year = {2007},
  publisher = {University of Minnesota Press},
  chapter = {Network Aesthetics}
}

@incollection{salomon2006,
  title = {Instructional Psychology: Past, Present, and Future Trends},
  author = {Salomon, Gavriel and {Ben-Zvi}, Dani},
  editor = {Verschaffel, Lieven and Dochy, Filip and Boekaerts, Monique and Vosniadou, Stella},
  year = {2006},
  publisher = {Earli},
  chapter = {The difficult marriage between education and technology: Is the marriage doomed?},
  keywords = {to read}
}

@article{samarati2001,
  title = {Protecting Respondents' Identities in Micro Data Release},
  author = {Samarati, Pierangela},
  year = {2001},
  journal = {IEEE Transactions on Knowledge and Data Engineering},
  volume = {13},
  number = {6}
}

@book{samuels2012,
  title = {Statistics for the {{Life Sciences}}},
  author = {Samuels, Myra L. and Witmer, Jeffrey A. and Schaffner, Andrew},
  year = {2012},
  edition = {4},
  publisher = {Addison-Wesley Publishing Company},
  keywords = {statistics}
}

@article{sandve2013,
  title = {Ten Simple Rules for Reproducible Computational Research},
  author = {Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and Hovig, Eivind},
  year = {2013},
  journal = {PLoS Computational Biology},
  volume = {9},
  number = {10},
  pages = {e1003285}
}

@book{sarkar2008,
  title = {Lattice: {{Multivariate}} Data Visualization with {{R}}},
  author = {Sarkar, Deepayan},
  year = {2008},
  publisher = {Springer}
}

@book{sasinstitute2012,
  title = {{{JMP}}, {{Version}} 10},
  author = {SAS Institute, {\relax Inc}.},
  year = {2012}
}

@book{sasinstituteinc2015,
  title = {{{SAS}} 14.1},
  author = {{SAS Institute Inc}},
  year = {2015}
}

@article{satija2016,
  title = {Hell and {{High Water}}},
  author = {Satija, Neena and Collier, Kiah and Shaw, Al and Larson, Jeff},
  year = {2016},
  month = mar,
  journal = {The Texas Tribune}
}

@inproceedings{satyanarayan2014,
  title = {Lyra: {{An}} Interactive Visualization Design Environment},
  booktitle = {Eurographics {{Conference}} on {{Visualization}} ({{EuroVis}}) 2014},
  author = {Satyanarayan, Arvind and Heer, Jeffrey},
  year = {2014},
  volume = {33},
  pages = {3}
}

@article{satyanarayan2016,
  title = {Reactive {{Vega}}: {{A}} Streaming Data Flow Architecture for Declarative Interactive Visualization},
  author = {Satyanarayan, Arvind and Russell, Ryan and Hoffswell, Jane and Heer, Jeffrey},
  year = {2016},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {22},
  number = {1},
  pages = {659--668},
  abstract = {We present Reactive Vega, a system architecture that provides the first robust and comprehensive treatment of declarative visual and interaction design for data visualization. Starting from a single declarative specification, Reactive Vega constructs a dataflow graph in which input data, scene graph elements, and interaction events are all treated as first-class streaming data sources. To support expressive interactive visualizations that may involve time-varying scalar, relational, or hierarchical data, Reactive Vega's dataflow graph can dynamically re-write itself at runtime by extending or pruning branches in a data-driven fashion. We discuss both compile- and run-time optimizations applied within Reactive Vega, and share the results of benchmark studies that indicate superior interactive performance to both D3 and the original, non-reactive Vega system.},
  keywords = {experimental,IEEEviz}
}

@article{satyanarayan2017,
  title = {Vega-{{Lite}}: {{A}} Grammar of Interactive Graphics},
  author = {Satyanarayan, Arvind and Moritz, Dominik and Wongsuphasawat, Kanit and Heer, Jeffrey},
  year = {2017},
  journal = {IEEE Trans. Visualization \& Comp. Graphics (Proc. InfoVis)}
}

@article{scaffidiberman2011,
  title = {A Positive Postdoctoral Experience Is Related to Quality Supervision and Career Mentoring, Collaborations, Networking and a Nurturing Research Environment},
  author = {Scaffidi, Amelia K. and Berman, Judith E.},
  year = {2011},
  month = dec,
  journal = {Higher Education},
  volume = {62},
  number = {6},
  pages = {685--698},
  issn = {0018-1560, 1573-174X},
  doi = {10.1007/s10734-011-9407-1},
  urldate = {2021-07-22},
  langid = {english}
}

@article{schau2012,
  title = {Do Introductory Statistics Courses in the {{United States}} Improve Students' Attitudes?},
  author = {Schau, Candace and Emm{\'i}o{\u g}lu, Esma},
  year = {2012},
  journal = {Statistics Education Research Journal},
  volume = {11},
  number = {2},
  pages = {86--94}
}

@article{schau2012a,
  title = {Research on Attitudes towards Statistics},
  author = {Schau, Candace and Millar, Michele and Petocz, Peter},
  year = {2012},
  journal = {Statistics Education Research Journal},
  volume = {11},
  number = {2}
}

@article{schindleretal2022,
  title = {The Role of Software in Science: A Knowledge Graph-Based Analysis of Software Mentions in {{PubMed Central}}},
  shorttitle = {The Role of Software in Science},
  author = {Schindler, David and Bensmann, Felix and Dietze, Stefan and Kr{\"u}ger, Frank},
  year = {2022},
  month = jan,
  journal = {PeerJ Computer Science},
  volume = {8},
  pages = {e835},
  publisher = {PeerJ Inc.},
  issn = {2376-5992},
  doi = {10.7717/peerj-cs.835},
  urldate = {2022-01-28},
  abstract = {Science across all disciplines has become increasingly data-driven, leading to additional needs with respect to software for collecting, processing and analysing data. Thus, transparency about software used as part of the scientific process is crucial to understand provenance of individual research data and insights, is a prerequisite for reproducibility and can enable macro-analysis of the evolution of scientific methods over time. However, missing rigor in software citation practices renders the automated detection and disambiguation of software mentions a challenging problem. In this work, we provide a large-scale analysis of software usage and citation practices facilitated through an unprecedented knowledge graph of software mentions and affiliated metadata generated through supervised information extraction models trained on a unique gold standard corpus and applied to more than 3 million scientific articles. Our information extraction approach distinguishes different types of software and mentions, disambiguates mentions and outperforms the state-of-the-art significantly, leading to the most comprehensive corpus of 11.8 M software mentions that are described through a knowledge graph consisting of more than 300 M triples. Our analysis provides insights into the evolution of software usage and citation patterns across various fields, ranks of journals, and impact of publications. Whereas, to the best of our knowledge, this is the most comprehensive analysis of software use and citation at the time, all data and models are shared publicly to facilitate further research into scientific use and citation of software.},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/JU5LMEK4/Schindler et al. - 2022 - The role of software in science a knowledge graph.pdf;/Users/amcnamara/Zotero/storage/45QL42FP/cs-835.html}
}

@article{schuurman2000,
  title = {Trouble in the Heartland: {{GIS}} and Its Critics in the 1990s},
  author = {Schuurman, Nadine},
  year = {2000},
  journal = {Progress in Human Geography},
  volume = {24},
  number = {4},
  pages = {569--590},
  abstract = {GIS eased into geography without much discord until the 1990s, when a flurry of commentaries about the relative merits of GIS made their way into a number of geographic journals. The ensuing decade was marked by varying degrees of friction between GIS practitioners and their critics in human geography. Despite the methodological chasm between the two groups, little discussion of the implications of these differences has ensued. This article fills that gap with a historiographic examination of critiques of GIS. Critiques of GIS are organized into three waves or periods, each characterized by distinct arguments. The first wave, from 1990 to 1994, was marked by the intensity of debate as well as an emphasis on positivism. By 1995, the conversation waned as the number of critics grew, while GIS practitioners increasingly declined comment. This second wave marked the initiation of a greater degree of co-operation between GIS scholars and their critics, however. With the inception of the National Center for Geographic Information Analysis (NCGIA) Initiative 19, intended to study the social effects of GIS, many critics began to work closely with their peers in GIS. In the third wave, critiques of GIS expressed a greater commitment to the technology. Throughout the decade, debates about the technology shifted from simple attacks on positivism to incorporating more subtle analyses of the effects of the technology. These critiques have had considerable effect on the academic GIS community but are presently constrained by limited communication with GIS practitioners because of the absence of a common vocabulary. I argue that, if critiques of GIS are to be effective, they must find a way to address GIS researchers, using the language and conceptual framework of the discipline.}
}

@article{schwab-mccoy2019,
  title = {The {{State}} of {{Statistics Education Research}} in {{Client Disciplines}}: {{Themes}} and {{Trends Across}} the {{University}}},
  shorttitle = {The {{State}} of {{Statistics Education Research}} in {{Client Disciplines}}},
  author = {{Schwab-McCoy}, Aimee},
  year = {2019},
  month = sep,
  journal = {Journal of Statistics Education},
  volume = {27},
  number = {3},
  pages = {253--264},
  publisher = {Taylor \& Francis},
  issn = {null},
  doi = {10.1080/10691898.2019.1687369},
  urldate = {2021-06-24},
  abstract = {The reform movement in statistics education has led to a revitalization of the undergraduate introductory statistics course. However, many students satisfy their degree requirements by taking statistics courses in ``client departments'' such as business, the social sciences, and the lab sciences, typically taught by non-statisticians. This article presents the findings of a metasynthesis of the existing literature on teaching statistics in these client disciplines to learn (1) what is currently being taught and how, and (2) the most important challenges for statistics teachers in other departments. Articles were reviewed using qualitative axial coding and quantitative text analysis to identify common research themes and ideas in the literature for each discipline. Research themes, attitudes toward statistics instruction, and pedagogical techniques were found to vary from discipline to discipline. Collaboration with instructors in other disciplines may be a welcome step toward improving statistics instruction across the university.},
  keywords = {Client disciplines,Metasynthesis,Qualitative methods,Statistics education,Text analysis},
  file = {/Users/amcnamara/Zotero/storage/GI6ARV2E/Schwab-McCoy - 2019 - The State of Statistics Education Research in Clie.pdf;/Users/amcnamara/Zotero/storage/2EF7TWPR/10691898.2019.html}
}

@article{schweinsbergetal2021,
  title = {Same Data, Different Conclusions: {{Radical}} Dispersion in Empirical Results When Independent Analysts Operationalize and Test the Same Hypothesis - {{ScienceDirect}}},
  author = {Schweinsberg, Martin and Feldman, Michael and Staub, Nicola and {et al}},
  year = {2021},
  journal = {Organizational Behavior and Human Decision Processes},
  volume = {165},
  urldate = {2021-10-01},
  file = {/Users/amcnamara/Zotero/storage/E24LK5JE/S0749597821000200.html}
}

@article{sedlmair2012,
  title = {Design {{Study Methodology}}: {{Reflections}} from the Trenches and the Stacks},
  author = {Sedlmair, Michael and Meyer, Miriah and Munzner, Tamara},
  year = {2012},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {18},
  number = {12},
  pages = {2431--2440},
  abstract = {Design studies are an increasingly popular form of problem-driven visualization research, yet there is little guidance avail- able about how to do them effectively. In this paper we reflect on our combined experience of conducting twenty-one design studies, as well as reading and reviewing many more, and on an extensive literature review of other field work methods and methodologies. Based on this foundation we provide definitions, propose a methodological framework, and provide practical guidance for conducting design studies. We define a design study as a project in which visualization researchers analyze a specific real-world problem faced by domain experts, design a visualization system that supports solving this problem, validate the design, and reflect about lessons learned in order to refine visualization design guidelines. We characterize two axes --- a task clarity axis from fuzzy to crisp and an information location axis from the domain expert's head to the computer --- and use these axes to reason about design study contributions, their suitability, and uniqueness from other approaches. The proposed methodological framework consists of 9 stages: learn, winnow, cast, discover, design, implement, deploy, reflect, and write. For each stage we provide practical guidance and outline potential pitfalls. We also conducted an extensive literature survey of related methodological approaches that involve a significant amount of qualitative field work, and compare design study methodology to that of ethnography, grounded theory, and action research.},
  keywords = {to read}
}

@article{sedlmair2013,
  title = {Empirical Guidance on Scatterplot and Dimension Reduction Technique Choices},
  author = {Sedlmair, Michael and Munzner, Tamara and Tory, Melanie},
  year = {2013},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {19},
  number = {12},
  abstract = {To verify cluster separation in high-dimensional data, analysts often reduce the data with a dimension reduction (DR) technique, and then visualize it with 2D Scatterplots, interactive 3D Scatterplots, or Scatterplot Matrices (SPLOMs). With the goal of providing guidance between these visual encoding choices, we conducted an empirical data study in which two human coders manually inspected a broad set of 816 scatterplots derived from 75 datasets, 4 DR techniques, and the 3 previously mentioned scatterplot techniques. Each coder scored all color-coded classes in each scatterplot in terms of their separability from other classes. We analyze the resulting quantitative data with a heatmap approach, and qualitatively discuss interesting scatterplot examples. Our findings reveal that 2D scatterplots are often `good enough', that is, neither SPLOM nor interactive 3D adds notably more cluster separability with the chosen DR technique. If 2D is not good enough, the most promising approach is to use an alternative DR technique in 2D. Beyond that, SPLOM occasionally adds additional value, and interactive 3D rarely helps but often hurts in terms of poorer class separation and usability. We summarize these results as a workflow model and implications for design. Our results offer guidance to analysts during the DR exploration process.},
  keywords = {to read}
}

@article{sedlmair2014,
  title = {Visual Parameter Space Analysis: {{A}} Conceptual Framework},
  author = {Sedlmair, Michael and Heinzl, Christoph and Bruckner, Stefan and Piringer, Harald and M{\"o}ller, Torsten},
  year = {2014},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {20},
  number = {12},
  abstract = {Various case studies in different application domains have shown the great potential of visual parameter space analysis to support validating and using simulation models. In order to guide and systematize research endeavors in this area, we provide a conceptual framework for visual parameter space analysis problems. The framework is based on our own experience and a structured analysis of the visualization literature. It contains three major components: (1) a data flow model that helps to abstractly describe visual parameter space analysis problems independent of their application domain; (2) a set of four navigation strategies of how parameter space analysis can be supported by visualization tools; and (3) a characterization of six analysis tasks. Based on our framework, we analyze and classify the current body of literature, and identify three open research gaps in visual parameter space analysis. The framework and its discussion are meant to support visualization designers and researchers in characterizing parameter space analysis problems and to guide their design and evaluation processes.},
  keywords = {IEEEviz,theoretical}
}

@article{sedlmairetal2012,
  title = {Design {{Study Methodology}}: {{Reflections}} from the {{Trenches}} and the {{Stacks}}},
  shorttitle = {Design {{Study Methodology}}},
  author = {Sedlmair, Michael and Meyer, Miriah and Munzner, Tamara},
  year = {2012},
  month = dec,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {18},
  number = {12},
  pages = {2431--2440},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2012.213},
  abstract = {Design studies are an increasingly popular form of problem-driven visualization research, yet there is little guidance available about how to do them effectively. In this paper we reflect on our combined experience of conducting twenty-one design studies, as well as reading and reviewing many more, and on an extensive literature review of other field work methods and methodologies. Based on this foundation we provide definitions, propose a methodological framework, and provide practical guidance for conducting design studies. We define a design study as a project in which visualization researchers analyze a specific real-world problem faced by domain experts, design a visualization system that supports solving this problem, validate the design, and reflect about lessons learned in order to refine visualization design guidelines. We characterize two axes - a task clarity axis from fuzzy to crisp and an information location axis from the domain expert's head to the computer - and use these axes to reason about design study contributions, their suitability, and uniqueness from other approaches. The proposed methodological framework consists of 9 stages: learn, winnow, cast, discover, design, implement, deploy, reflect, and write. For each stage we provide practical guidance and outline potential pitfalls. We also conducted an extensive literature survey of related methodological approaches that involve a significant amount of qualitative field work, and compare design study methodology to that of ethnography, grounded theory, and action research.},
  keywords = {Algorithm design and analysis,Collaboration,Data visualization,Design methodology,Design study,framework,Logic gates,methodology,visualization,Visualization},
  file = {/Users/amcnamara/Zotero/storage/S7XVGJHV/Sedlmair et al. - 2012 - Design Study Methodology Reflections from the Tre.pdf}
}

@article{segel2010,
  title = {Narrative {{Visualization}}: {{Telling Stories}} with {{Data}}},
  author = {Segel, Edward and Heer, Jeffrey},
  year = {2010},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {16},
  number = {6},
  pages = {1139--1148},
  abstract = {Data visualization is regularly promoted for its ability to reveal stories within data, yet these ``data stories'' differ in important ways from traditional forms of storytelling. Storytellers, especially online journalists, have increasingly been integrating visualizations into their narratives, in some cases allowing the visualization to function in place of a written story. In this paper, we systematically review the design space of this emerging class of visualizations. Drawing on case studies from news media to visualization research, we identify distinct genres of narrative visualization. We characterize these design differences, together with interactivity and messaging, in terms of the balance between the narrative flow intended by the author (imposed by graphical elements and the interface) and story discovery on the part of the reader (often through interactive exploration). Our framework suggests design strategies for narrative visualization, including promising under-explored approaches to journalistic storytelling and educational media.},
  keywords = {to read}
}

@book{selber2004,
  title = {Multiliteracies for a Digital Age},
  author = {Selber, Stuart},
  year = {2004},
  publisher = {Southern Illinois University Press}
}

@article{shah2002,
  title = {Review of {{Graph Comprehension Research}}: {{Implications}} for {{Instruction}}},
  author = {Shah, Priti and Hoeffner, James},
  year = {2002},
  month = mar,
  journal = {Educational Psychology Review},
  volume = {14},
  number = {1},
  pages = {47--69},
  abstract = {Graphs are commonly used in textbooks and educational software, and can help students understand science and social science data. However, students sometimes have difficulty comprehending information depicted in graphs. What makes a graph better or worse at communicating relevant quantitative in- formation? How can students learn to interpret graphs more effectively? This article reviews the cognitive literature on how viewers comprehend graphs and the factors that influence viewers' interpretations. Three major factors are considered: the visual characteristics of a graph (e.g., format, animation, color, use of legend, size, etc.), a viewer's knowledge about graphs, and a viewer's knowledge and expectations about the content of the data in a graph. This article provides a set of guidelines for the presentation of graphs to stu- dents and considers the implications of graph comprehension research for the teaching of graphical literacy skills. Finally, this article discusses unresolved questions and directions for future research relevant to data presentation and the teaching of graphical literacy skills.}
}

@article{shaltayev2010,
  title = {{{VISA}}: {{Reducing}} Technological Impact on Student Learning in an Introductory Statistics Course},
  author = {Shaltayev, Dmitriy S. and Hodges, Harland and Hasbrouk, Robert B.},
  year = {2010},
  journal = {Technology Innovations in Statistics Education},
  volume = {4},
  number = {1},
  abstract = {In this empirical study we compare student performance using two different teaching methods in introductory business statistics course. Two groups were taught in the computer lab with software available at students' fingertips while one was taught in the regular classroom with only a computer workstation for the instructor. VISA (Visual Interactive Statistical Analysis), an Excel- based analysis software package was used in classroom to perform computational analysis of the data in all three groups. Exam data and final course grades indicate that student performance between the two methods was not affected by presence of the software in classroom for use by students. This leads us to conclude that VISA is an intuitive enough tool, which does not require a major learning curve, and can be mastered by students with minimal supervision. Second, we conclude that if the software used for statistics instruction is ``teaching-friendly'', then technology availability in the classroom does not affect learning efficiency. This allows instructors to concentrate more efforts in class teaching conceptually important material.},
  keywords = {to read}
}

@article{shanahanetal2015,
  title = {Ten {{Salient Practices}} of {{Undergraduate Research Mentors}}: {{A Review}} of the {{Literature}}},
  shorttitle = {Ten {{Salient Practices}} of {{Undergraduate Research Mentors}}},
  author = {Shanahan, Jenny Olin and {Ackley-Holbrook}, Elizabeth and Hall, Eric and Stewart, Kearsley and Walkington, Helen},
  year = {2015},
  month = oct,
  journal = {Mentoring \& Tutoring: Partnership in Learning},
  volume = {23},
  number = {5},
  pages = {359--376},
  issn = {1361-1267, 1469-9745},
  doi = {10.1080/13611267.2015.1126162},
  urldate = {2021-07-14},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/H5B7YJQD/Shanahan et al. - 2015 - Ten Salient Practices of Undergraduate Research Me.pdf}
}

@book{sharpeetal2010,
  title = {Rethinking {{Learning}} for a {{Digital Age}}: {{How Learners Are Shaping Their Own Experiences}}},
  shorttitle = {Rethinking {{Learning}} for a {{Digital Age}}},
  author = {Sharpe, Rhona and Beetham, Helen and {de Freitas}, Sara},
  year = {2010},
  publisher = {Taylor \& Francis Group},
  address = {London, UNITED KINGDOM},
  urldate = {2024-01-16},
  isbn = {978-0-203-85206-4},
  keywords = {Education - Effect of technological innovations on},
  file = {/Users/amcnamara/Zotero/storage/SKPHXS9M/reader.html}
}

@incollection{shaughnessy2007,
  title = {Second Handbook of Research on Mathematics Teaching and Learning},
  author = {Shaughnessy, J. Michael},
  year = {2007},
  publisher = {National Council of Teachers of Mathematics},
  chapter = {Research on statistics learning and reasoning}
}

@book{shneiderman1980,
  title = {Software {{Psychology}}: {{Human Factors}} in {{Computer}} and {{Information Systems}}},
  author = {Shneiderman, Ben},
  year = {1980},
  publisher = {Winthrop Publishers, Inc}
}

@article{shneiderman1994,
  title = {Dynamic {{Queries}} for {{Visual Information Seeking}}},
  author = {Shneiderman, Ben},
  year = {1994},
  journal = {IEEE Software}
}

@article{silvertown2009,
  title = {A New Dawn for Citizen Science},
  author = {Silvertown, Jonathan},
  year = {2009},
  journal = {Trends in Ecology \& Evolution},
  volume = {24},
  number = {9},
  pages = {467--471}
}

@inproceedings{siminenko2011,
  title = {A Model for Analyzing and Visualizing Tabular Data},
  booktitle = {{{FQAS}} 2011},
  author = {Siminenko, Eketerina and Spyratos, Nicolas and Sugibuchi, Tsuyoshi},
  editor = {Christiansen, H.},
  year = {2011},
  abstract = {Abstract. We present a model to visual analysis of tabular data based on functional dependencies, and a Web based tool that supports casual users in doing the following: (a) construct an analytic query visually, in a flexible, interactive manner, (b) visualize the aggregate result in a user selected mode (histogram, pie, etc.), (c) explore the query result by viewing equivalent representations at different aggregation levels or for different parameter values.},
  keywords = {to read}
}

@article{slota2020,
  title = {Prospecting (in) the Data Sciences},
  author = {Slota, Stephen C. and Hoffman, Andrew S. and Ribes, David and Bowker, Geoffrey C.},
  year = {2020},
  journal = {Big Data \& Society}
}

@book{smithcollege2016,
  title = {The Major in {{Statistical}} \& {{Data Sciences}}},
  author = {{Smith College}},
  year = {2016}
}

@misc{smithdavid2016,
  title = {Welcome to the {{Tidyverse}}},
  author = {Smith, David},
  year = {2016},
  month = sep,
  journal = {Revolutions}
}

@article{soloway1994,
  title = {Learner-{{Centered Design}}: {{The Challenge}} for {{HCI}} in the 21st {{Century}}},
  author = {Soloway, Elliot and Guzdial, Mark and Hay, Kenneth E.},
  year = {1994},
  month = apr,
  journal = {Interactions},
  volume = {1},
  number = {2},
  pages = {36--48}
}

@inproceedings{sorokina2009,
  title = {Detecting and {{Interpreting Variable Interactions}} in {{Observational Ornithology Data}}},
  booktitle = {{{ICDMW}}'09},
  author = {Sorokina, Daria and Caruana, Rich and Riedewald, Mirek and Hochachka, Wesley M.},
  year = {2009},
  publisher = {IEEE}
}

@phdthesis{sovak2010,
  title = {The Effect of Student-Driven Projects on the Development of Statistical Reasoning},
  author = {Sovak, Melissa M.},
  year = {2010},
  month = aug,
  school = {University of Pittsburgh},
  keywords = {education,statistics}
}

@book{statacorp2015,
  title = {Stata {{Statistical Software}}: {{Release}} 14},
  author = {{StataCorp}},
  year = {2015},
  annotation = {Published: College Station, TX: StataCorp LP}
}

@book{steen2001,
  title = {Mathematics and {{Democracy}}: {{The Case}} for {{Quantitative Literacy}}},
  editor = {Steen, Lynn Arthur},
  year = {2001},
  publisher = {{National Council on Education and the Disciplines}},
  keywords = {education,mathematics,quantitative literacy},
  annotation = {Backup Publisher: National Council on Education and the Disciplines}
}

@book{stefik2011a,
  title = {On the {{Design}} of an {{Educational Infrastructure}} for the {{Blind}} and {{Visually Impaired}} in {{Computer Science}}},
  author = {Stefik, Andreas and Hundhausen, Christopher and Smith, Derrick},
  year = {2011},
  annotation = {Published: SIGCSE'11, https://dl.acm.org/citation.cfm?id=1953323}
}

@inproceedings{stefiketal2011,
  title = {An {{Empirical Comparison}} of the {{Accuracy Rates}} of {{Novices}} Using the {{Quorum}}, {{Perl}} and {{Randomo Programming Languages}}},
  booktitle = {{{PLATAEU}} 2011},
  author = {Stefik, Andreas and Siebert, Susanna and Stefik, Melissa and Slattery, Kim},
  year = {2011},
  abstract = {We present here an empirical study comparing the accuracy rates of novices writing software in three programming languages: Quorum, Perl, and Randomo. The first language, Quorum, we call an evidence-based programming language, where the syntax, semantics, and API designs change in correspondence to the latest academic research and literature on programming language usability. Second, while Perl is well known, we call Randomo a Placebo-language, where some of the syntax was chosen with a random number generator and the ASCII table. We compared novices that were programming for the first time using each of these languages, testing how accurately they could write simple programs us- ing common program constructs (e.g., loops, conditionals, functions, variables, parameters). Results showed that while Quorum users were afforded significantly greater accuracy compared to those using Perl and Randomo, Perl users were unable to write programs more accurately than those using a language designed by chance.}
}

@inproceedings{stefikhanenberg2014,
  title = {The {{Programming Language Wars}}: {{Questions}} and {{Responsibilities}} for the {{Programming Language Community}}},
  shorttitle = {The {{Programming Language Wars}}},
  booktitle = {Proceedings of the 2014 {{ACM International Symposium}} on {{New Ideas}}, {{New Paradigms}}, and {{Reflections}} on {{Programming}} \& {{Software}}},
  author = {Stefik, Andreas and Hanenberg, Stefan},
  year = {2014},
  month = oct,
  series = {Onward! 2014},
  pages = {283--299},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2661136.2661156},
  urldate = {2021-07-19},
  abstract = {The discipline of computer science has a long and complicated history with computer programming languages. Historically, inventors have created language products for a wide variety of reasons, from attempts at making domain specific tasks easier or technical achievements, to economic, social, or political reasons. As a consequence, the modern programming language industry now has a large variety of incompatible programming languages, each of which with unique syntax, semantics, toolsets, and often their own standard libraries, lifetimes, and costs. In this paper, we suggest that the programming language wars, a term which describes the broad divergence and impact of language designs, including often pseudo-scientific claims made that they are good or bad, may be negatively impacting the world. This broad problem, which is almost completely ignored in computer science, needs to be acted upon by the community.},
  isbn = {978-1-4503-3210-1},
  keywords = {evidence standards,stability of the academic literature,the programming language wars},
  file = {/Users/amcnamara/Zotero/storage/FWM4ZZMZ/Stefik and Hanenberg - 2014 - The Programming Language Wars Questions and Respo.pdf}
}

@inproceedings{stefikladner2017,
  title = {The {{Quorum Programming Language}}},
  booktitle = {Proceedings of the 2017 {{ACM SIGCSE Technical Symposium}} on {{Computer Science Education}}},
  author = {Stefik, Andreas and Ladner, Richard},
  year = {2017},
  month = mar,
  series = {{{SIGCSE}} '17},
  pages = {641},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3017680.3022377},
  urldate = {2021-07-21},
  abstract = {Quorum is a relatively new programming language that was originally designed for students with disabilities. In recent years, as its adoption has increased worldwide in K-12 (largely in middle/high school) and at universities, it has expanded to be a powerful, commercial-grade, programming language that includes support for 3D gaming, music, and other fun and creative activities. While new features are designed for all, they maintain compatibility for people with disabilities, including a novel way for individuals who are blind to create 3D games. Finally, Quorum is the first language to use human-factors evidence from both field data and randomized controlled trials in its design. This approach provides the broader research community an organized way to influence the design of the language over time according to evidence based practices. We call this approach evidence-oriented programming. A laptop would help participants follow along with the session and handouts will be provided. Quorum can be found at https://www.quorumlanguage.com/.},
  isbn = {978-1-4503-4698-6},
  keywords = {accessibility,computer games,evidence-oriented programming,programming languages}
}

@article{stefiksiebert2013,
  title = {An {{Empirical Investigation}} into {{Programming Language Syntax}}},
  author = {Stefik, Andreas and Siebert, Susanna},
  year = {2013},
  journal = {ACM Transactions on Computing Education},
  volume = {13},
  number = {4},
  abstract = {Recent studies in the literature have shown that syntax remains a significant barrier to novice computer science students in the field. While this syntax barrier is known to exist, whether and how it varies across programming languages has not been carefully investigated. For this article, we conducted four empirical studies on programming language syntax as part of a larger analysis into the, so called, programming language wars. We first present two surveys conducted with students on the intuitiveness of syntax, which we used to garner formative clues on what words and symbols might be easy for novices to understand. We followed up with two studies on the accuracy rates of novices using a total of six programming languages: Ruby, Java, Perl, Python, Randomo, and Quorum. Randomo was designed by randomly choosing some keywords from the ASCII table (a metaphorical placebo). To our surprise, we found that languages using a more traditional C-style syntax (both Perl and Java) did not afford accuracy rates significantly higher than a language with randomly generated keywords, but that languages which deviate (Quorum, Python, and Ruby) did. These results, including the specifics of syntax that are particularly problematic for novices, may help teachers of introductory programming courses in choosing appropriate first languages and in helping students to overcome the challenges they face with syntax.}
}

@techreport{stephenson2005,
  title = {The {{New Educational Imperative}}: {{Improving High School Computer Science Education}}},
  author = {Stephenson, Chris and {Gal-Ezer}, Judith and Haberman, Bruria and Verno, Anita},
  year = {2005},
  institution = {Computer Science Teachers Association}
}

@article{stevens1946,
  title = {On the Theory of Scales of Measurement},
  author = {Stevens, S. S.},
  year = {1946},
  journal = {Science},
  volume = {103},
  number = {2684},
  pages = {677--680}
}

@book{stiny1978,
  title = {Algorithmic {{Aesthetics}}},
  author = {Stiny, George and Gips, James},
  year = {1978},
  publisher = {University of California Press}
}

@book{stodden2014,
  title = {Implementing {{Reproducible Research}}},
  editor = {Stodden, Victoria and Leisch, Friedrich and Peng, Roger D.},
  year = {2014},
  publisher = {Chapman \& Hall/CRC}
}

@article{stodden2018,
  title = {An Empirical Analysis of Journal Policy Effectiveness for Computational Reproducibility},
  author = {Stodden, Victoria and Seiler, Jennifer and Ma, Zhaokun},
  year = {2018},
  journal = {PNAS},
  volume = {115},
  number = {11},
  abstract = {A key component of scientific communication is sufficient infor- mation for other researchers in the field to reproduce published findings. For computational and data-enabled research, this has often been interpreted to mean making available the raw data from which results were generated, the computer code that gen- erated the findings, and any additional information needed such as workflows and input parameters. Many journals are revising author guidelines to include data and code availability. This work evaluates the effectiveness of journal policy that requires the data and code necessary for reproducibility be made available postpub- lication by the authors upon request. We assess the effectiveness of such a policy by (i) requesting data and code from authors and (ii) attempting replication of the published findings. We chose a random sample of 204 scientific papers published in the jour- nal Science after the implementation of their policy in February 2011. We found that we were able to obtain artifacts from 44\% of our sample and were able to reproduce the findings for 26\%. We find this policy---author remission of data and code postpublica- tion upon request---an improvement over no policy, but currently insufficient for reproducibility.}
}

@inproceedings{storeyetal2003,
  title = {Improving the Usability of {{Eclipse}} for Novice Programmers},
  booktitle = {Proceedings of the 2003 {{OOPSLA Workshop}} on {{Eclipse Technology}}},
  author = {Storey, M.-A. and Michaud, Jeff and Mindel, Marcellus and Sanseverino, Mary and Damian, Daniela and Myers, Del and German, Daniel and Hargreaves, Elizabeth},
  year = {2003},
  keywords = {computer science,education}
}

@article{sturges1926,
  title = {The Choice of a Class Interval},
  author = {Sturges, Herbert A.},
  year = {1926},
  journal = {Journal of the American Statistical Association},
  volume = {21},
  number = {153},
  pages = {65--66}
}

@inproceedings{sugibuchi2009,
  title = {A Framework to Analyze Information Visualization Based on the Functional Data Model},
  booktitle = {2009 13th {{International Conference Information Visualization}}},
  author = {Sugibuchi, Tsuyoshi and Spyratos, Nicolas and Siminenko, Eketerina},
  year = {2009},
  abstract = {e propose a framework for analyzing information visualization (infovis) based on the concept of Functional Dependency (FD). Although functional dependencies express important semantic information of data, they are rarely taken into account by general purpose infovis tools --- a fact that may cause problems in the visualization process. The main idea of our approach is to use the concept of FD for modeling the invariant structures of all three components of information visualization that is data, visual representations, and visual mappings.}
}

@article{sullivan2008,
  title = {Data Exploration through Visualization Tools},
  author = {Sullivan, Brian and Kelling, Steven T. and Wood, Christopher and Iliff, Marshall and Fink, Daniel and Herzog, Mark and Moody, Doug and Ballard, Grant},
  year = {2008},
  journal = {Proceedings of the fourth international partners in flight conference: Tundra to tropics},
  pages = {415--418}
}

@article{sullivan2009,
  title = {{{eBird}}: {{A}} Citizen-Based Bird Observation Network in the Biological Sciences},
  author = {Sullivan, Brian and Wood, Christopher and Iliff, Marshall and Bonney, Rick and Fink, Daniel and Kelling, Steve},
  year = {2009},
  journal = {Biological Conservation},
  abstract = {New technologies are rapidly changing the way we collect, archive, analyze, and share scientific data. For example, over the next several years it is estimated that more than one billion autonomous sensors will be deployed over large spatial and temporal scales, and will gather vast quantities of data. Networks of human observers play a major role in gathering scientific data, and whether in astronomy, meteorology, or observations of nature, they continue to contribute significantly. In this paper we present an innovative use of the Internet and information technologies that better enhances the opportunity for citizens to contribute their observations to science and the conservation of bird populations. eBird is building a web- enabled community of bird watchers who collect, manage, and store their observations in a globally accessible unified database. Through its development as a tool that addresses the needs of the birding community, eBird sustains and grows participation. Birders, scientists, and conservationists are using eBird data worldwide to better understand avian biological patterns and the environmental and anthropogenic factors that influence them. Developing and shaping this network over time, eBird has created a near real-time avian data resource producing millions of observations per year.}
}

@article{sun2012,
  title = {Hybrid {{Course Design}}: {{Leading}} a {{New Direction}} in {{Learning Programming Languages}}},
  author = {Sun, Lulu and Kindy, Matthew and Liron, Caroline and Grant, Christopher D. and Waterhouse, Shirley Anne},
  year = {2012},
  journal = {American Society for Engineering Education},
  keywords = {to read}
}

@techreport{sutherland1963,
  title = {Sketchpad: {{A}} Man-Made Graphical Communication System},
  author = {Sutherland, Ivan E.},
  year = {1963},
  institution = {Massachusetts Institute of Technology}
}

@techreport{sutherland1965,
  title = {The Ultimate Display},
  author = {Sutherland, Ivan E.},
  year = {1965},
  institution = {Information Processing Techniques Office, ARPA, OSD}
}

@techreport{sutherland1996,
  title = {Technology and Courage},
  author = {Sutherland, Ivan E.},
  year = {1996},
  institution = {SunLabs}
}

@incollection{swain2006,
  title = {Languaging, {{Agency}} and {{Collaboration}} in {{Advanced Second Language Proficiency}}},
  booktitle = {Advanced {{Language Learning}} : {{The Contribution}} of {{Halliday}} and {{Vygotsky}}},
  author = {Swain, Merrill},
  year = {2006},
  publisher = {Bloomsbury Academic},
  doi = {10.5040/9781474212113},
  urldate = {2021-07-20},
  isbn = {978-1-4742-1211-3}
}

@article{swayne2003,
  title = {{{GGobi}}: Evolving from {{XGobi}} into an Extensible Framework for Interactive Data Visualization},
  author = {Swayne, Deborah F. and Lang, Duncan Temple and Buja, Andreas and Cook, Dianne},
  year = {2003},
  journal = {Computational Statistics \& Data Analysis},
  volume = {43},
  pages = {423--444},
  abstract = {GGobi is a direct descendent of a data visualization system called XGobi that has been around since the early 1990s. GGobi's new features include multiple plotting windows, a color lookup table manager, and an Extensible Markup Language 􏰀le format for data. Perhaps the biggest advance is that GGobi can be easily extended, either by being embedded in other software or by the addition of plugins; either way, it can be controlled using an Application Programming Interface. An illustration of its extensibility is that it can be embedded in R. The result is a full marriage between GGobi's direct manipulation graphical environment and R's familiar extensible environment for statistical data analysis.},
  keywords = {to read}
}

@article{sweeney2002,
  title = {K-{{Anonymity}}: {{A}} Model for Protecting Privacy},
  author = {Sweeney, Latanya},
  year = {2002},
  journal = {International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  volume = {10},
  number = {5},
  pages = {557--570}
}

@incollection{sweller2011,
  title = {Cognitive {{Load Theory}}},
  booktitle = {Psychology of {{Learning}} and {{Motivation}}},
  author = {Sweller, John},
  year = {2011},
  volume = {55},
  pages = {37--76},
  publisher = {Elsevier},
  doi = {10.1016/B978-0-12-387691-1.00002-8},
  urldate = {2021-07-21},
  isbn = {978-0-12-387691-1},
  langid = {english}
}

@inproceedings{swidanhermans2019,
  title = {The {{Effect}} of {{Reading Code Aloud}} on {{Comprehension}}: {{An Empirical Study}} with {{School Students}}},
  booktitle = {{{CompEd}} '19},
  author = {Swidan, Alaaeddin and Hermans, Felienne},
  year = {2019}
}

@article{tal2014,
  title = {Blinded with Science: {{Trivial}} Graphs and Formulas Increase Ad Persuasiveness and Belief in Product Efficacy},
  author = {Tal, Aner and Wansink, Brian},
  year = {2014},
  journal = {Public Understanding of Science},
  volume = {25},
  number = {1}
}

@article{tangmunarunkit2013,
  title = {Ohmage: {{A General}} and {{Extensible End-to-end}} Participatory Sensing Platform},
  author = {Tangmunarunkit, H. and Hsieh, C. K. and Jenkins, J. and Ketcham, C. and Selsky, J. and Alquaddoomi, F. and Kang, J. and Khalapyan, Z. and Longstaff, B. and Nolen, S. and Ooms, J. and Ramanathan, N. and Estrin, D.},
  year = {2013},
  journal = {ACM Transactions on Intelligent Systems and Technology}
}

@article{tealetal2015,
  title = {Data {{Carpentry}}: {{Workshops}} to {{Increase Data Literacy}} for {{Researchers}}},
  shorttitle = {Data {{Carpentry}}},
  author = {Teal, Tracy K. and Cranston, Karen A. and Lapp, Hilmar and White, Ethan and Wilson, Greg and Ram, Karthik and Pawlik, Aleksandra},
  year = {2015},
  month = feb,
  journal = {International Journal of Digital Curation},
  volume = {10},
  number = {1},
  pages = {135--143},
  issn = {1746-8256},
  doi = {10.2218/ijdc.v10i1.351},
  urldate = {2022-07-20},
  abstract = {In many domains the rapid generation of large amounts of data is fundamentally changing how research is done. The deluge of data presents great opportunities, but also many challenges in managing, analyzing and sharing data. However, good training resources for researchers looking to develop skills that will enable them to be more effective and productive researchers are scarce and there is little space in the existing curriculum for courses or additional lectures. To address this need we have developed an introductory two-day intensive workshop, Data Carpentry, designed to teach basic concepts, skills, and tools for working more effectively and reproducibly with data. These workshops are based on Software Carpentry: two-day, hands-on, bootcamp style workshops teaching best practices in software development, that have demonstrated the success of short workshops to teach foundational research skills. Data Carpentry focuses on data literacy in particular, with the objective of teaching skills to researchers to enable them to retrieve, view, manipulate, analyze and store their and other's data in an open and reproducible way in order to extract knowledge from data.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {curation,DCC,digital curation,digital preservation,IJDC,International Journal of Digital Curation,preservation},
  file = {/Users/amcnamara/Zotero/storage/6BWYXM89/Teal et al. - 2015 - Data Carpentry Workshops to Increase Data Literac.pdf}
}

@inproceedings{tewguzdial2010,
  title = {Developing a Validated Assessment of Fundamental {{CS1}} Concepts},
  booktitle = {Proceedings of the 41st {{ACM}} Technical Symposium on {{Computer}} Science Education},
  author = {Tew, Allison Elliott and Guzdial, Mark},
  year = {2010},
  month = mar,
  series = {{{SIGCSE}} '10},
  pages = {97--101},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/1734263.1734297},
  urldate = {2022-07-21},
  abstract = {Previous studies of student programming ability have raised questions about students' ability to problem solve, read and analyze code, and understand introductory computing concepts. However, it is unclear whether these results are the product of failures of student comprehension or our inability to accurately measure their performance. We propose a method for creating a language independent CS1 assessment instrument and present the results of our analysis used to define the common conceptual content that will serve as the framework for the exam. We conclude with a discussion of future work and our progress towards developing the assessment.},
  isbn = {978-1-4503-0006-3},
  keywords = {assessment,cs1,programming,validity},
  file = {/Users/amcnamara/Zotero/storage/F3HA6TE7/Tew and Guzdial - 2010 - Developing a validated assessment of fundamental C.pdf}
}

@misc{thecarpentries2021,
  title = {The {{Carpentries Survey Archives}}},
  author = {The Carpentries},
  year = {2021},
  journal = {assessment-archives},
  urldate = {2021-08-26},
  abstract = {An archive of all the versions of the Carpentries surveys},
  howpublished = {https://carpentries.github.io/assessment-archives/},
  langid = {american},
  file = {/Users/amcnamara/Zotero/storage/88UCUXF3/assessment-archives.html}
}

@book{theconcordconsortium2016,
  title = {Common {{Online Data Analysis Platform}}},
  author = {{The Concord Consortium}},
  year = {2016}
}

@article{thedatateam2015,
  title = {The {{Tracks}} of {{Arrears}}},
  author = {{The Data Team}},
  year = {2015},
  month = may,
  journal = {The Economist}
}

@inproceedings{thomaetal2018,
  title = {``{{It Didn}}'t {{Really Go Very Well}}'': {{Epistemological Framing}} and the {{Complexity}} of {{Interdisciplinary Computing Activities}}},
  booktitle = {Rethinking {{Learning}} in the {{Digital Age}}: {{Making}} the {{Learning Sciences Count}}},
  author = {Thoma, Serena and Deltrick, Elise and Wilkerson, Michelle},
  editor = {Kay, J and Luckin, R},
  year = {2018},
  volume = {2}
}

@book{thompson2002,
  title = {Sampling},
  author = {Thompson, Steven K.},
  year = {2002},
  edition = {Second},
  publisher = {Wiley},
  keywords = {to buy}
}

@incollection{thompson2007,
  title = {Intricacies of Statistical Inference and Teachers' Understandings of Them},
  booktitle = {Thinking with {{Data}}},
  author = {Thompson, Patrick W. and Liu, Yan and Saldanha, Luis A.},
  editor = {Lovett, Marsha C. and Shah, Priti},
  year = {2007},
  publisher = {Lawrence Erlbaum Associates},
  keywords = {education,statistics}
}

@book{thorp2021,
  title = {Living in Data: A Citizen's Guide to a Better Information Future},
  shorttitle = {Living in Data},
  author = {Thorp, Jer},
  year = {2021},
  edition = {First edition},
  publisher = {{MCD, Farrar, Straus and Giroux}},
  address = {New York},
  abstract = {"A provocative, eye-opening, example-laden exploration of our current and future relationship with data"--},
  isbn = {978-0-374-18990-7},
  lccn = {QA76.9.I52 T55 2021},
  keywords = {Big data,Information behavior,Information visualization,Quantitative research,Social aspects,Statistics}
}

@misc{tidytuesday,
  title = {Tidy {{Tuesday}}: {{A}} Weekly Data Project Aimed at the {{R}} Ecosystem},
  author = {Mock, Thomas},
  year = {2022}
}

@article{tintle2012,
  title = {Retention of Statistical Concepts in a Preliminary Randomization-Based Introductory Statistics Curriculum},
  author = {Tintle, Nathan and Topliff, Kylie and Vanderstoep, Jill and Holmes, Vicki-Lynn and Swanson, Todd},
  year = {2012},
  journal = {Statistics Education Research Journal},
  volume = {11},
  number = {1},
  pages = {21--40},
  keywords = {to read}
}

@book{tintle2014,
  title = {Introduction to {{Statistical Investigations}}},
  author = {Tintle, Nathan and Chance, Beth and Cobb, George and Rossman, Allan and Roy, Soma and Swanson, Todd and VanderStoep, Jill},
  year = {2014},
  publisher = {Wiley}
}

@article{tol2009,
  title = {An Integrated Model of Soil-Canopy Spectral Radiances, Photosynthesis, Fluorescence, Temperature and Energy Balance},
  author = {van der Tol, Christiaan and Verhoef, W. and Timmermans, J. and Verhoef, A. and Su, Z.},
  year = {2009},
  journal = {Biogeosciences},
  volume = {6},
  pages = {3109--3129}
}

@book{tol2017,
  title = {{{SCOPE}}},
  author = {van der Tol, Christiaan},
  year = {2017},
  month = oct
}

@article{trafimow2015,
  title = {Editorial},
  author = {Trafimow, David and Marks, Michael},
  year = {2015},
  journal = {Basic and Applied Social Psychology},
  volume = {37},
  number = {1},
  pages = {1--2}
}

@article{traftonetal2005,
  title = {Connecting {{Internal}} and {{External Representations}}: {{Spatial Transformations}} of {{Scientific Visualizations}}},
  shorttitle = {Connecting {{Internal}} and {{External Representations}}},
  author = {Trafton, J. Gregory and Trickett, Susan B. and Mintz, Farilee E.},
  year = {2005},
  month = mar,
  journal = {Foundations of Science},
  volume = {10},
  number = {1},
  pages = {89--106},
  issn = {1572-8471},
  doi = {10.1007/s10699-005-3007-4},
  urldate = {2023-07-10},
  abstract = {Many scientific discoveries have depended on external diagrams or visualizations. Many scientists also report to use an internal mental representation or mental imagery to help them solve problems and reason. How do scientists connect these internal and external representations? We examined working scientists as they worked on external scientific visualizations. We coded the number and type of spatial transformations (mental operations that scientists used on internal or external representations or images) and found that there were a very large number of comparisons, either between different visualizations or between a visualization and the scientists' internal mental representation. We found that when scientists compared visualization to visualization, the comparisons were based primarily on features. However, when scientists compared a visualization to their mental representation, they were attempting to align the two representations. We suggest that this alignment process is how scientists connect internal and external representations.},
  langid = {english},
  keywords = {diagramatic reasoning,graph comprehension,scientific reasoning,scientific visualization,spatial transformations}
}

@article{tripodi2021,
  title = {Ms. {{Categorized}}: {{Gender}}, Notability, and Inequality on {{Wikipedia}}},
  shorttitle = {Ms. {{Categorized}}},
  author = {Tripodi, Francesca},
  year = {2021},
  month = jun,
  journal = {New Media \& Society},
  pages = {146144482110237},
  issn = {1461-4448, 1461-7315},
  doi = {10.1177/14614448211023772},
  urldate = {2021-06-28},
  abstract = {Gender is one of the most pervasive and insidious forms of inequality. For example, English-language Wikipedia contains more than 1.5 million biographies about notable writers, inventors, and academics, but less than 19\% of these biographies are about women. To try and improve these statistics, activists host ``edit-a-thons'' to increase the visibility of notable women. While this strategy helps create several biographies previously inexistent, it fails to address a more inconspicuous form of gender exclusion. Drawing on ethnographic observations, interviews, and quantitative analysis of web-scraped metadata, this article demonstrates that biographies about women who meet Wikipedia's criteria for inclusion are more frequently considered non-notable and nominated for deletion compared to men's biographies. This disproportionate rate is another dimension of gender inequality previously unexplored by social scientists and provides broader insights into how women's achievements are (under)valued.},
  langid = {english}
}

@techreport{trouche2003,
  title = {Managing the {{Complexity}} of {{Human}}/{{Machine Interactions}} in {{Computerized Learning Environments}}: {{Guiding Students}}' {{Command Process Through Instrumental Orchestrations}}},
  author = {Trouche, Luc},
  year = {2003},
  institution = {LIRDEF \& LIRMM, Universit{\'e} Montpellier II}
}

@article{trouche2004,
  title = {Managing the Complexity of Human/Machine Interactions in Computerized Learning Environments: {{Guiding}} Students' Command Process through Instrumental Orchestrations},
  author = {Trouche, Luc},
  year = {2004},
  journal = {International Journal of Computers for Mathematical Learning},
  volume = {9},
  pages = {281--307},
  abstract = {After an introduction which addresses some basic questions, this article is organized around three points: (1) The theoretical framework of the so-called ``instrumental approach'' which has been a theme in the last two CAME symposia; (2) A consideration of two processes (instrumentalization and instrumentation) which interact in the instrumental genesis; and (3) The introduction of the idea of instrumental orchestration as a way of allowing the teacher to assist the student's instrumental genesis.},
  keywords = {to read}
}

@article{tsui2007,
  title = {Effective Strategies to Increase Diversity in {{STEM}} Fields: {{A}} Review of Research Literature},
  author = {Tsui, Lisa},
  year = {2007},
  journal = {Journal of Negro Education},
  volume = {76},
  number = {4},
  pages = {555--581}
}

@article{tuckeretal2023,
  title = {Teaching {{Statistics}} and {{Data Analysis}} with {{R}}},
  author = {Tucker, Mary C. and Shaw, Stacy T. and Son, Ji Y. and Stigler, James W.},
  year = {2023},
  month = jan,
  journal = {Journal of Statistics and Data Science Education},
  volume = {31},
  number = {1},
  pages = {18--32},
  publisher = {Taylor \& Francis},
  issn = {null},
  doi = {10.1080/26939169.2022.2089410},
  urldate = {2024-03-01},
  abstract = {We developed an interactive online textbook that interleaves R programming activities with text as a way to facilitate students' understanding of statistical ideas while minimizing the cognitive and emotional burden of learning programming. In this exploratory study, we characterize the attitudes and experiences of 672 undergraduate students as they used our online textbook as part of a 10-week introductory course in statistics. Students expressed negative attitudes and concerns related to R at the beginning of the course, but most developed more positive attitudes after engaging with course materials, regardless of demographic characteristics or prior programming experience. Analysis of a subgroup of students revealed that change in attitudes toward R may be linked to students' patterns of engagement over time and students' perceptions of the learning environment.},
  keywords = {Data science education,Higher education,R,Statistics education research}
}

@inproceedings{tufekci2014,
  title = {Big Questions for Social Media Big Data: {{Representativeness}}, Validity and Other Methodological Pitfalls},
  booktitle = {{{ICWSM}} '14: {{Proceedings}} of the 8th {{International AAAI Conference}} on {{Weblogs}} and {{Social Media}}},
  author = {Tufekci, Zeynep},
  year = {2014},
  abstract = {Large-scale databases of human activity in social media have captured scientific and policy attention, producing a flood of research and discussion. This paper considers methodological and conceptual challenges for this emergent field, with special attention to the validity and representativeness of social media big data analyses. Persistent issues include the over-emphasis of a single platform, Twitter, sampling biases arising from selection by hashtags, and vague and unrepresentative sampling frames. The socio- cultural complexity of user behavior aimed at algorithmic invisibility (such as subtweeting, mock-retweeting, use of ``screen captures'' for text, etc.) further complicate interpretation of big data social media. Other challenges include accounting for field effects, i.e. broadly consequential events that do not diffuse only through the network under study but affect the whole society. The application of network methods from other fields to the study of human social activity may not always be appropriate. The paper concludes with a call to action on practical steps to improve our analytic capacity in this promising, rapidly-growing field.}
}

@book{tufte1990,
  title = {Envisioning {{Information}}},
  author = {Tufte, Edward},
  year = {1990},
  publisher = {Graphics Pr}
}

@book{tufte2001,
  title = {The {{Visual Display}} of {{Quantitative Information}}},
  author = {Tufte, Edward},
  year = {2001},
  publisher = {Graphics Pr}
}

@book{tufte2014,
  title = {Sparkline {{Theory}} and {{Practice}}},
  author = {Tufte, Edward},
  year = {2014}
}

@book{tukey,
  title = {Prim-9},
  author = {Tukey, John}
}

@article{tukey1962,
  title = {The Future of Data Analysis},
  author = {Tukey, John},
  year = {1962},
  journal = {The Annals of Mathematical Statistics},
  keywords = {to read}
}

@article{tukey1965,
  title = {The Technical Tools of Statistics},
  author = {Tukey, John W.},
  year = {1965},
  month = apr,
  journal = {The American Statistician},
  volume = {19},
  number = {2},
  pages = {23--28},
  keywords = {statistics,technology}
}

@book{tukey1977,
  title = {Exploratory {{Data Analysis}}},
  author = {Tukey, John W.},
  year = {1977},
  publisher = {Addison-Wesley Publishing Company},
  keywords = {data visualization,statistics}
}

@article{turkle1990,
  title = {Epistemological {{Pluralism}}: {{Styles}} and {{Voices}} within the {{Computer Culture}}},
  author = {Turkle, Sherry and Papert, Seymour},
  year = 1990,
  journal = {Signs: Journal of Women in Culture and Society},
  volume = {16},
  number = {1},
  pages = {128--157}
}

@book{twosigmaopensourcellc2016,
  title = {Beaker: {{The Data Scientist}}'s {{Laboratory}}},
  author = {{Two Sigma Open Source, LLC}},
  year = {2016}
}

@misc{tycdsswritingteametal2018,
  title = {The {{Two-Year College Data Science Summit}}. {{A}} Report on {{NSF DUE-1735199}}},
  author = {{TYCDSS Writing Team} and Gould, Robert and Peck, Roxy and Hanson, Julie and Horton, Nicholas J. and Kotz, Brian and Kubo, Kathy and {Malyn-Smith}, Joyce and Rudis, Mary and Thompson, Brad and Ward, Mark Daniel and Wong, Rebecca},
  year = {2018},
  urldate = {2023-07-07},
  file = {/Users/amcnamara/Zotero/storage/YKV75RWH/2018TYCDS-Final-Report.pdf}
}

@misc{u.s.departmentofeducation2017,
  type = {Reference {{Materials}}},
  title = {Community {{College Facts}} at a {{Glance}}},
  author = {{U.S. Department of Education}},
  year = {2017},
  month = feb,
  publisher = {US Department of Education (ED)},
  urldate = {2023-07-17},
  abstract = {This page contains facts and figures related to community colleges.},
  howpublished = {https://www2.ed.gov/about/offices/list/ovae/pi/cclo/ccfacts.html},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/JBFXEG22/ccfacts.html}
}

@book{ucberkeleyschoolofinformation2017,
  title = {Master of {{Information}} and {{Data Science}}: {{Curriculum}}},
  author = {{UC Berkeley School of Information}},
  year = {2017}
}

@article{uekawa2007,
  title = {Student Engagement in {{U}}.{{S}}. Urban High School Mathematics and Science Classrooms: {{Findings}} on Social Organization, Race, and Ethnicity},
  author = {Uekawa, Kazuaki and Borman, Kathryn and Lee, Reginald},
  year = {2007},
  journal = {The Urban Review},
  volume = {39},
  number = {1}
}

@misc{universityofstthomas2022,
  title = {Undergraduate {{Research Opportunities Program}} -- {{University}} of {{St}}. {{Thomas}} -- {{Minnesota}}},
  author = {{University of St Thomas}},
  year = {2022},
  urldate = {2022-07-21},
  howpublished = {www.stthomas.edu/urop/},
  langid = {american},
  file = {/Users/amcnamara/Zotero/storage/ZHE739SD/urop.html}
}

@article{unwin2013,
  title = {Let Graphics Tell the Story - {{Datasets}} in {{R}}},
  author = {Unwin, Antony and Hofmann, Heike and Cook, Dianne},
  year = {2013},
  month = jun,
  journal = {The R Journal},
  volume = {5},
  number = {1},
  pages = {117--130},
  abstract = {Graphics are good for showing the information in datasets and for complementing modelling. Sometimes graphics show information models miss, sometimes graphics help to make model results more understandable, and sometimes models show whether information from graphics has statistical support or not. It is the interplay of the two approaches that is valuable. Graphics could be used a lot more in R examples and we explore this idea with some datasets available in R packages.}
}

@book{ushey2015,
  title = {Packrat: {{A}} Dependency Management System for Projects and Their {{R}} Package Dependencies},
  author = {Ushey, Kevin and McPherson, Jonathan and Cheng, Joe and Allaire, J. J.},
  year = {2015}
}

@article{uttingetal2010,
  title = {Alice, {{Greenfoot}}, and {{Scratch}} -- {{A Discussion}}},
  author = {Utting, Ian and Cooper, Stephen and K{\"o}lling, Michael and Maloney, John and Resnick, Mitchel},
  year = {2010},
  journal = {ACM Transactions on Computing Education},
  volume = {10},
  number = {4},
  keywords = {computer science,education}
}

@article{vance2009,
  title = {Data Analysts Captivated by {{R}}'s Power},
  author = {Vance, Ashlee},
  year = {2009},
  month = jan,
  journal = {New York Times}
}

@article{vanderplas2016,
  title = {Spatial Reasoning and Data Displays},
  author = {VanderPlas, Susan and Hofmann, Heike},
  year = {2016},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {22},
  number = {1},
  abstract = {Graphics convey numerical information very efficiently, but rely on a different set of mental processes than tabular displays. Here, we present a study relating demographic characteristics and visual skills to perception of graphical lineups. We conclude that lineups are essentially a classification test in a visual domain, and that performance on the lineup protocol is associated with general aptitude, rather than specific tasks such as card rotation and spatial manipulation. We also examine the possibility that specific graphical tasks may be associated with certain visual skills and conclude that more research is necessary to understand which visual skills are required in order to understand certain plot types.},
  keywords = {experimental,IEEEviz}
}

@book{velleman1989,
  title = {Data {{Desk}}: {{Handbook}}, {{Volume}} 1},
  author = {Velleman, Paul F.},
  year = {1989},
  publisher = {Data Description, Inc}
}

@article{vellom2000,
  title = {{{EarthVision}} 2000: {{Examining Students}}' {{Representations}} of {{Complex Data Sets}}},
  author = {Vellom, R. Paul and Pape, Stephen J.},
  year = {2000},
  month = dec,
  journal = {School Science and Mathematics},
  volume = {100},
  number = {8},
  pages = {426--439},
  abstract = {Graphic representations are powerful tools used by scientists and other professionals to help them understand multifaceted natural phenomena. They can also serve teachers and students as they attempt to understand complex data sets. This study examines pencil-and-paper graphs produced by students at the beginning of a 1-week summer teacher/student institute, as well as computer-based graphs produced by the same students at the end of the institute. Initial problems with managing the data set and producing meaningful graphs disappeared quickly as students used a process of "building up){$>$} to handle the complexity of web-based data on water quality. This process is examined, as are findings from the institute related to (a) barriers to accessing web-based data, (b) students ' problem-solving processes, and (c) the promise of this approach for learning about environmental science issues.},
  keywords = {data,education,statistics}
}

@book{verborgh2013,
  title = {Using {{OpenRefine}}},
  author = {Verborgh, Ruben and Wilde, Max De},
  year = {2013},
  publisher = {Packt Publishing}
}

@book{verzani2005,
  title = {{{simpleR- Using R}} for Introductory Statistics},
  author = {Verzani, John},
  year = {2005},
  annotation = {Published: Chapman \& Hall/CRC}
}

@book{victor2006,
  title = {Magic {{Ink}}},
  author = {Victor, Bret},
  year = {2006},
  month = mar
}

@book{victor2011,
  title = {Explorable {{Explanations}}},
  author = {Victor, Bret},
  year = {2011},
  month = mar
}

@book{victor2012,
  title = {Inventing on Principle},
  author = {Victor, Bret},
  year = {2012},
  journal = {CUSEC 2012}
}

@book{vigen2013,
  title = {Spurious {{Correlations}}},
  author = {Vigen, Tyler},
  year = {2013}
}

@book{visualizingurbandataidealab,
  title = {Simpson's {{Paradox}}: {{Girls}} Gone Average. {{Averages}} Gone Wild},
  author = {{Visualizing Urban Data ideaLab} and Lehe, Lewis and Powell, Victor}
}

@article{waller2018,
  title = {Documenting and {{Evaluating Data Science Contributions}} in {{Academic Promotion}} in {{Departments}} of {{Statistics}} and {{Biostatistics}}},
  author = {Waller, Lance A.},
  year = {2018},
  journal = {The American Statistician},
  volume = {72},
  number = {1},
  pages = {11--19}
}

@article{walnyetal2015,
  title = {An {{Exploratory Study}} of {{Data Sketching}} for {{Visual Representation}}},
  author = {Walny, J. and Huron, S. and Carpendale, S.},
  year = {2015},
  journal = {Computer Graphics Forum},
  volume = {34},
  number = {3},
  pages = {231--240},
  issn = {1467-8659},
  doi = {10.1111/cgf.12635},
  urldate = {2023-07-10},
  abstract = {Hand-drawn sketching on napkins or whiteboards is a common, accessible method for generating visual representations. This practice is shared by experts and non-experts and is probably one of the faster and more expressive ways to draft a visual representation of data. In order to better understand the types of and variations in what people produce when sketching data, we conducted a qualitative study. We asked people with varying degrees of visualization expertise, from novices to experts, to manually sketch representations of a small, easily understandable dataset using pencils and paper and to report on what they learned or found interesting about the data. From this study, we extract a data sketching representation continuum from numeracy to abstraction; a data report spectrum from individual data items to speculative data hypothesis; and show the correspondence between the representation types and the data reports from our results set. From these observations we discuss the participants' representations in relation to their data reports, indicating implications for design and potentially fruitful directions for research.},
  copyright = {{\copyright} 2015 The Author(s) Computer Graphics Forum {\copyright} 2015 The Eurographics Association and John Wiley \& Sons Ltd. Published by John Wiley \& Sons Ltd.},
  langid = {english},
  keywords = {Categories and Subject Descriptors (according to ACM CCS),H.5.2 Information Interfaces and Presentation: User Interfaces---},
  file = {/Users/amcnamara/Zotero/storage/6AZ9VFTQ/Walny et al. - 2015 - An Exploratory Study of Data Sketching for Visual .pdf;/Users/amcnamara/Zotero/storage/AEAFB69R/cgf.html}
}

@misc{walsh2022,
  title = {Ravelry.Com Data},
  author = {Walsh, Alice},
  year = {2022},
  journal = {GitHub},
  urldate = {2023-06-21},
  abstract = {Official repo for the \#tidytuesday project. Contribute to rfordatascience/tidytuesday development by creating an account on GitHub.},
  howpublished = {https://github.com/rfordatascience/tidytuesday},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/IJ22ZI6Q/2022-10-11.html}
}

@article{walshaw2012,
  title = {Teacher Knowledge as Fundamental to Effective Teaching Practice},
  author = {Walshaw, Margaret},
  year = {2012},
  journal = {Journal of Math Teacher Education},
  volume = {15},
  pages = {181--185},
  keywords = {to read}
}

@article{wand1997,
  title = {Data-Based Choice of Histogram Bin Width},
  author = {Wand, M. P.},
  year = {1997},
  journal = {The American Statistician},
  volume = {51},
  number = {1},
  pages = {59--64}
}

@article{wang2014,
  title = {Forecasting {{Elections}} with {{Non-representative}} Polls},
  author = {Wang, Wei and Rothschild, David and Goel, Sharad and Gelman, Andrew},
  year = {2014},
  journal = {International Journal of Forecasting},
  abstract = {Election forecasts have traditionally been based on representative polls, in which randomly sampled individuals are asked for whom they intend to vote. While representative polling has historically proven to be quite effective, it comes at considerable financial and time costs. Moreover, as response rates have declined over the past several decades, the statistical benefits of representative sampling have diminished. In this paper, we show that with proper statistical adjustment, non-representative polls can be used to generate accurate election forecasts, and often faster and at less expense than traditional survey methods. We demonstrate this approach by creating forecasts from a novel and highly non-representative survey dataset: a series of daily voter intention polls for the 2012 presidential election conducted on the Xbox gaming platform. After adjusting the Xbox responses via multilevel regression and post-stratification, we obtain estimates in line with forecasts from leading poll analysts, which were based on aggregating hundreds of traditional polls conducted during the election cycle. We conclude by arguing that non-representative polling shows promise not only for election forecasting, but also for measuring public opinion on a broad range of social, economic and cultural issues.}
}

@inproceedings{wang2015,
  title = {Visualization as the Gateway Drug to Statistics in Week One},
  booktitle = {{{USCOTS}} 2015},
  author = {Wang, Xiaofei (Susan) and Rush, Cynthia},
  year = {2015}
}

@article{wangetal2017,
  title = {Data {{Visualization}} on {{Day One}}: {{Bringing Big Ideas}} into {{Intro Stats Early}} and {{Often}}},
  shorttitle = {Data {{Visualization}} on {{Day One}}},
  author = {Wang, Xiaofei and Rush, Cynthia and Horton, Nicholas Jon},
  year = {2017},
  journal = {Technology Innovations in Statistics Education},
  volume = {10},
  number = {1},
  issn = {1933-4214},
  doi = {10.5070/T5101031737},
  urldate = {2022-10-21},
  file = {/Users/amcnamara/Zotero/storage/BA3DMREI/Wang et al. - 2017 - Data Visualization on Day One Bringing Big Ideas .pdf}
}

@book{ware2000,
  title = {Information {{Visualization}}: {{Perception}} for Design},
  author = {Ware, Colin},
  year = {2000},
  publisher = {Morgan Kaufmann Publishers},
  keywords = {to buy}
}

@incollection{warschauer2010,
  title = {Review of {{Research}} in {{Education}}},
  author = {Warschauer, Mark and Matuchiak, Tina},
  year = {2010},
  publisher = {American Educational Research Association},
  chapter = {New technology and digital worlds: Analyzing evidence of equity in access, use, and outcomes}
}

@techreport{warth2011,
  title = {Worlds: {{Controlling}} the {{Scope}} of {{Side Effects}}},
  author = {Warth, Alessando and Ohshima, Yoshiki and Kaehler, Ted and Kay, Alan},
  year = {2011},
  address = {1209 Grand Central Avenue, Glendale CA 91201},
  institution = {Viewpoints Research Institute}
}

@inproceedings{wassong2010,
  title = {A Model for Teacher Knowledge as a Basis for Online Courses for Professional Development of Statistics Teachers},
  booktitle = {Research Papers from {{ICOTS}} 8},
  author = {Wassong, Thomas and Biehler, Rolf},
  editor = {Reading, C.},
  year = {2010},
  keywords = {education,statistics,technology}
}

@book{watson2005,
  title = {Variation and Expectation as Foundations for the Chance and Data Curriculum},
  author = {Watson, Jane M.},
  year = {2005},
  keywords = {education,statistics}
}

@inproceedings{watson2019,
  title = {R at the {{ACLU}}: {{Joining}} Tables to to Reunite Families},
  shorttitle = {R at the {{ACLU}}},
  booktitle = {Rstudio::Conf 2019},
  author = {Watson, Brooke},
  year = {2019},
  month = jan,
  urldate = {2022-03-30},
  abstract = {Last year, over 2500 immigrant children were separated from their family while in government custody. Information about their status is scattered across several government agencies, and throughout...},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/IAQB6E6P/r-at-the-aclu-joining-tables-to-to-reunite-families.html}
}

@article{watsoncallingham2014,
  title = {Two-{{Way Tables}}: {{Issues}} at the {{Heart}} of {{Statistics}} and {{Probability}} for {{Students}} and {{Teachers}}},
  shorttitle = {Two-{{Way Tables}}},
  author = {Watson, Jane and Callingham, Rosemary},
  year = {2014},
  month = oct,
  journal = {Mathematical Thinking and Learning},
  volume = {16},
  number = {4},
  pages = {254--284},
  issn = {1098-6065, 1532-7833},
  doi = {10.1080/10986065.2014.953019},
  urldate = {2021-07-27},
  langid = {english}
}

@article{watsondonne2009,
  title = {{{TinkerPlots}} as a {{Research Tool}} to {{Explore Student Understanding}}},
  author = {Watson, Jane and Donne, Julie},
  year = {2009},
  month = mar,
  journal = {Technology Innovations in Statistics Education},
  volume = {3},
  number = {1},
  issn = {1933-4214},
  doi = {10.5070/T531000034},
  urldate = {2021-06-24},
  file = {/Users/amcnamara/Zotero/storage/CAVRIXGN/Watson and Donne - 2009 - TinkerPlots as a Research Tool to Explore Student .pdf}
}

@article{watsonetal2008,
  title = {The {{Representational Value}} of {{HATS}}},
  author = {Watson, Jane M. and Fitzallen, Noleine E. and Wilson, Karen G. and Creed, Julie F.},
  year = {2008},
  journal = {Mathematics Teaching in the Middle School},
  volume = {14},
  number = {1},
  eprint = {41182617},
  eprinttype = {jstor},
  pages = {4--10},
  publisher = {National Council of Teachers of Mathematics},
  issn = {1072-0839},
  urldate = {2022-07-17},
  file = {/Users/amcnamara/Zotero/storage/8JZXX2LS/Watson et al. - 2008 - The Representational Value of HATS.pdf}
}

@techreport{watsonfitzallen2010,
  title = {The {{Development}} of {{Graph Understanding}} in the {{Mathematics Curriculum}}},
  author = {Watson, Jane and Fitzallen, Noleine},
  year = {2010},
  institution = {{New South Wales Department of Education and Training}},
  keywords = {education,mathematics,statistics}
}

@incollection{watsonfitzallen2015,
  title = {Statistical {{Software}} and {{Mathematics Education}}: {{Affordances}} for {{Learning}}},
  booktitle = {Handbook of {{International Research}} in {{Mathematics Education}}},
  author = {Watson, Jane and Fitzallen, Noleine},
  year = {2015},
  edition = {3}
}

@article{watsonneal2012,
  title = {Preparing Students for Decision-Making in the 21st Century - {{Statistics}} and Probability in the {{Australian Curriculum}}: {{Mathematics}}},
  shorttitle = {Preparing Students for Decision-Making in the 21st Century - {{Statistics}} and Probability in the {{Australian Curriculum}}},
  author = {Watson, J. and Neal, D.},
  year = {2012},
  urldate = {2022-07-17},
  abstract = {The place of statistics in The Australian Curriculum: Mathematics has been controversial from several perspectives. These perspectives are considered throughout the chapter. Discussion includes a comparison of the content of the Statistics and Probability strand with the model of a statistical investigation leading to beginning inference. Relevant research that underlies the current or alternative content is presented. An important focus of the chapter is the relevance of context to the study of statistics and probability, along with the need for numeracy across the entire curriculum and the contribution of statistical literacy to this goal. Interviews with classroom teachers are used to document implementation of Statistics and Probability in the classroom and to gain suggestions on changes to the curriculum and professional learning required to assist further implementation. Work samples illustrate some of the implementation that has taken place. Synthesizing these aspects results in recommendations for further research in classrooms with teachers and students in order to obtain outcomes that will assist students in the 21 st century to make meaningful decisions in relation to data and risk.},
  langid = {australian},
  file = {/Users/amcnamara/Zotero/storage/CJE2RLRX/80626.html}
}

@book{webber2011,
  title = {Modern Programming Languages: A Practical Introduction},
  shorttitle = {Modern Programming Languages},
  author = {Webber, Adam Brooks},
  year = {2011},
  edition = {2nd ed},
  publisher = {Franklin, Beedle \& Associates},
  address = {Sherwood, Or},
  isbn = {978-1-59028-250-2},
  langid = {english}
}

@inproceedings{weintropwilensky2015,
  title = {To {{Block}} or {{Not To Block}}, {{That Is}} the {{Question}}: {{Students}}' {{Perceptions}} of {{Blocks-Based Programming}}},
  shorttitle = {To Block or Not to Block, That Is the Question},
  booktitle = {Proceedings of the 14th {{International Conference}} on {{Interaction Design}} and {{Children}}},
  author = {Weintrop, David and Wilensky, Uri},
  year = {2015},
  month = jun,
  pages = {199--208},
  publisher = {ACM},
  address = {Boston Massachusetts},
  doi = {10.1145/2771839.2771860},
  urldate = {2021-06-15},
  isbn = {978-1-4503-3590-4},
  langid = {english}
}

@article{weisberg2005,
  title = {Lost {{Opportunities}}: {{Why}} We Need a Variety of Statistical Languages},
  author = {Weisberg, Sanford},
  year = {2005},
  journal = {Journal of Statistical Software},
  volume = {13},
  number = {1},
  pages = {1--12}
}

@inproceedings{weiss2014,
  title = {{{MUCK}}: {{A}} Toolkit for Extracting and Visualizing Semantic Dimensions of Large Text Collections},
  booktitle = {Proceedings of the {{Workshop}} on {{Interactive Language Learning}}, {{Visualization}}, and {{Interfaces}}},
  author = {Weiss, Rebecca},
  year = {2014},
  abstract = {Users with large text collections are of- ten faced with one of two problems; either they wish to retrieve a semantically- relevant subset of data from the collection for further scrutiny (needle-in-a-haystack) or they wish to glean a high-level understanding of how a subset compares to the parent corpus in the context of aforementioned semantic dimensions (forest- for-the-trees). In this paper, I describe MUCK1, an open-source toolkit that ad- dresses both of these problems through a distributed text processing engine with an interactive visualization interface.},
  keywords = {to read}
}

@article{weiss2017,
  title = {Perspectives: {{Teaching}} Chemists to Code},
  author = {Weiss, Charles J.},
  year = {2017},
  journal = {Chemical \& Engineering News},
  volume = {95},
  number = {35},
  pages = {30--31}
}

@inproceedings{welinder2010,
  title = {The {{Multidimensional Wisdom}} of {{Crowds}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Welinder, Peter and Branson, Steve and Belongie, Serge and Perona, Pietro},
  year = {2010},
  abstract = {Distributing labeling tasks among hundreds or thousands of annotators is an increasingly important method for annotating large datasets. We present a method for estimating the underlying value (e.g. the class) of each image from (noisy) annotations provided by multiple annotators. Our method is based on a model of the image formation and annotation process. Each image has different characteristics that are represented in an abstract Euclidean space. Each annotator is modeled as a multidimensional entity with variables representing competence, expertise and bias. This allows the model to discover and represent groups of annotators that have different sets of skills and knowledge, as well as groups of images that differ qualitatively. We find that our model predicts ground truth labels on both synthetic and real data more accurately than state of the art methods. Experiments also show that our model, starting from a set of binary labels, may discover rich information, such as different ``schools of thought'' amongst the annotators, and can group together images belonging to separate categories.},
  keywords = {crowdsourcing}
}

@article{west2004,
  title = {An Introduction to {{StatCrunch}} 3.0},
  author = {West, Webster and Wu, Yuping and Heydt, Duane},
  year = {2004},
  journal = {Journal of Statistical Software},
  volume = {9},
  number = {5}
}

@article{wickham2007,
  title = {Reshaping Data with the Reshape Package},
  author = {Wickham, Hadley},
  year = {2007},
  journal = {Journal of Statistical Software},
  volume = {21},
  number = {12},
  pages = {1--20}
}

@phdthesis{wickham2008,
  title = {Practical Tools for Exploring Data and Models},
  author = {Wickham, Hadley},
  year = {2008},
  school = {Iowa State University}
}

@book{wickham2009,
  title = {Ggplot2: {{Elegant}} Graphics for Data Analysis},
  author = {Wickham, Hadley},
  year = {2009},
  publisher = {Springer New York}
}

@article{wickham2010,
  title = {Graphical Inference for Infovis},
  author = {Wickham, Hadley and Cook, Dianne and Hofmann, Heike and Buja, Andreas},
  year = {2010},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {16},
  number = {6},
  pages = {973--979},
  abstract = {How do we know if what we see is really there? When visualizing data, how do we avoid falling into the trap of apophenia where we see patterns in random noise? Traditionally, infovis has been concerned with discovering new relationships, and statistics with preventing spurious relationships from being reported. We pull these opposing poles closer with two new techniques for rigorous statistical inference of visual discoveries. The ``Rorschach'' helps the analyst calibrate their understanding of uncertainty and the ``line-up'' provides a protocol for assessing the significance of visual discoveries, protecting against the discover y of spurious structure.},
  keywords = {data visualization,statistics}
}

@inproceedings{wickham2010a,
  title = {Using {{Visualization}} to Teaching Data Analysis and Programming},
  booktitle = {{{ICOTS8}}},
  author = {Wickham, Hadley},
  year = {2010},
  publisher = {International Association of Statistical Education},
  abstract = {Modern data analysis demands computing skills that most potential statisticians lack. This paper discusses my approach to teaching data analysis and programming focused around the potential of visualization to engage students with the data and give them a flexible toolbox with which to attack many potential problems.}
}

@book{wickham2011,
  title = {40 Years of Boxplots},
  author = {Wickham, Hadley and Stryjewski, Lisa},
  year = {2011}
}

@article{wickham2011a,
  title = {Product Plots},
  author = {Wickham, Hadley and Hofmann, Heike},
  year = {2011},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {17},
  number = {12},
  abstract = {We propose a new framework for visualising tables of counts, proportions and probabilities. We call our framework product plots, alluding to the computation of area as a product of height and width, and the statistical concept of generating a joint distribution from the product of conditional and marginal distributions. The framework, with extensions, is sufficient to encompass over 20 visualisations previously described in fields of statistical graphics and infovis, including bar charts, mosaic plots, treemaps, equal area plots and fluctuation diagrams.},
  keywords = {to read}
}

@article{wickham2011b,
  title = {The {{Split-Apply-Combine}} Strategy for Data Analysis},
  author = {Wickham, Hadley},
  year = {2011},
  journal = {Journal of Statistical Software},
  volume = {40},
  number = {1},
  pages = {1--29}
}

@article{wickham2011c,
  title = {Tourr: {{An R}} Package for Exploring Multidimensional Data with Projections},
  author = {Wickham, Hadley and Cook, Dianne and Hofmann, Heike and Buja, Andreas},
  year = {2011},
  journal = {Journal of Statistical Software},
  volume = {40},
  number = {2}
}

@book{wickham2014,
  title = {Advanced {{R}}},
  author = {Wickham, Hadley},
  year = {2014},
  publisher = {Chapman \& Hall/CRC The R Series}
}

@article{wickham2014a,
  title = {Tidy {{Data}}},
  author = {Wickham, Hadley},
  year = {2014},
  journal = {Journal of Statistical Software},
  volume = {59},
  number = {10},
  pages = {1--23},
  abstract = {A huge amount of effort is spent cleaning data to get it ready for analysis, but there has been little research on how to make data cleaning as easy and effective as possible. This paper tackles a small, but important, component of data cleaning: data tidying. Tidy datasets are easy to manipulate, model and visualize, and have a specific structure: each variable is a column, each observation is a row, and each type of observational unit is a table. This framework makes it easy to tidy messy datasets because only a small set of tools are needed to deal with a wide range of un-tidy datasets. This structure also makes it easier to develop tidy tools for data analysis, tools that both input and output tidy datasets. The advantages of a consistent data structure and matching tools are demonstrated with a case study free from mundane data manipulation chores.}
}

@book{wickham2014b,
  title = {Why Dplyr?},
  author = {Wickham, Hadley},
  year = {2014},
  annotation = {Published: useR! Conference, https://www.youtube.com/watch?v=dWjSYqI7Vog}
}

@book{wickham2015,
  title = {Dplyr: {{A}} Grammar of Data Manipulation},
  author = {Wickham, Hadley and Francois, Romain},
  year = {2015},
  publisher = {R package version 0.4.1}
}

@book{wickham2015a,
  title = {Readr: {{Read}} Tabular Data},
  author = {Wickham, Hadley and Francois, Romain},
  year = {2015}
}

@book{wickham2015b,
  title = {R Packages},
  author = {Wickham, Hadley},
  year = {2015},
  edition = {First edition},
  publisher = {O'Reilly Media},
  address = {Sebastopol, CA},
  abstract = {Turn your R code into packages that others can easily download and use. This practical book shows you how to bundle reusable R functions, sample data, and documentation together by applying author Hadley Wickham's package development philosophy. In the process, you'll work with devtools, roxygen, and testthat, a set of R packages that automate common development tasks. Devtools encapsulates best practices that Hadley has learned from years of working with this programming language.Ideal for developers, data scientists, and programmers with various backgrounds, this book starts you with the basics and shows you how to improve your package writing over time. You'll learn to focus on what you want your package to do, rather than think about package structure. Ideal for developers, data scientists, and programmers with various backgrounds, this book starts with the basics and shows you how to improve your package writing over time. You'll learn to focus on what you want your package to do, rather than think about package structure},
  isbn = {978-1-4919-1059-7},
  lccn = {QA76.73.R3 W53 2015},
  keywords = {R (Computer program language)},
  annotation = {OCLC: ocn898161451}
}

@book{wickham2016,
  title = {Introduction to Dplyr},
  author = {Wickham, Hadley},
  year = {2016},
  month = jun
}

@book{wickham2016a,
  title = {Stringr: {{Simple}}, Consistent Wrappers for Common String Operations},
  author = {Wickham, Hadley},
  year = {2016}
}

@book{wickham2019,
  title = {@hadleywickham ``{{Please}} Help Me Figure out Good Names for the New Pivot Verbs in Tidyr by Filling out This (Very Short!) Survey: {{https://forms.gle/vvYgBw1EwHK69gA17}} \#rstats``},
  author = {Wickham, Hadley},
  year = {2019},
  month = mar
}

@misc{wickham2021,
  title = {Pivot Function Names},
  author = {Wickham, Hadley},
  year = {2021},
  month = nov,
  urldate = {2023-07-07}
}

@book{wickham2022,
  title = {The \texttt{tidyverse} Style Guide},
  author = {Wickham, Hadley},
  year = {2022}
}

@article{wickhametal2019,
  title = {Welcome to the \texttt{tidyverse}},
  author = {Wickham, Hadley and Averick, Mara and Bryan, Jennifer and Chang, Winston and McGowan, Lucy D'Agostino and François, Romain and Grolemund, Garrett and Hayes, Alex and Henry, Lionel and Hester, Jim and Kuhn, Max and Pedersen, Thomas Lin and Miller, Evan and Bache, Stephan Milton and Müller, Kirill and Ooms, Jeroen and Robinson, David and Seidel, Dana Paige and Spinu, Vitalie and Takahashi, Kohske and Vaughan, Davis and Wilke, Claus and Woo, Kara and Yutani, Hiroaki},
  year = {2019},
  journal = {Journal of Open Source Software},
  volume = {4},
  number = {43},
  pages = {1686},
  doi = {10.21105/joss.01686}
}

@manual{wickhametal2023,
  type = {Manual},
  title = {Tidyr: {{Tidy}} Messy Data},
  author = {Wickham, Hadley and Vaughan, Davis and Girlich, Maximilian},
  year = {2023}
}

@book{wickhamgrolemund2017,
  title = {R for {{Data Science}}: {{Import}}, {{Tidy}}, {{Transform}}, {{Visualize}}, and {{Model Data}}},
  author = {Wickham, Hadley and Grolemund, Garrett},
  year = {2017},
  publisher = {O'Reilly}
}

@inproceedings{wiggins2012,
  title = {Goals and Tasks: {{Two}} Typologies of Citizen Science Projects},
  booktitle = {Fourty-Fifth {{Hawaii International Conference}} on {{System Science}}},
  author = {Wiggins, Andrea and Crowston, Kevin},
  year = {2012},
  abstract = {Abstract---Citizen science is a form of research collaboration involving members of the public in scientific research projects to address real-world problems. Often organized as a virtual collaboration, these projects are a type of open movement, with collective goals addressed through open participation in research tasks. We conducted a survey of citizen science projects to elicit multiple aspects of project design and operation. We then clustered projects based on the tasks performed by participants and on the project's stated goals. The clustering results group projects that show similarities along other dimensions, suggesting useful divisions of the projects.}
}

@book{wikipedia,
  title = {Feist {{Publications}}, {{Inc}}., v. {{Rural Telephone Service Co}}},
  author = {{wikipedia}}
}

@book{wild2011,
  title = {Guidelines for ``{{How}} to Make the Call''},
  author = {Wild, Chris and Horton, Nicholas J. and Pfannkuch, Maxine and Regan, Matt},
  year = {2011}
}

@book{wild2012,
  title = {Bootstrap Animations},
  author = {Wild, Chris and Chang, Keng Hao},
  year = {2012}
}

@book{wild2015,
  title = {{{VIT}}: {{Visual Inference Tools}}},
  author = {Wild, Chris and Grolemund, Garrett and Stevenson, Ben and Potter, Simon and Chang, Keng Hao and Li, Jieping},
  year = {2015}
}

@book{wildelliott2016,
  title = {{{iNZight}}},
  author = {Wild, Chris and Elliott, Tom},
  year = {2016}
}

@article{wildetal2011,
  title = {Towards More Accessible Conceptions of Statistical Inference},
  author = {Wild, C. J. and Pfannkuch, M. and Regan, M. and Horton, N. J.},
  year = {2011},
  journal = {Journal of the Royal Statistical Society},
  volume = {174},
  number = {2},
  pages = {247--295},
  keywords = {to read}
}

@article{wildpfannkuch1999,
  title = {Statistical {{Thinking}} in {{Empirical Enquiry}}},
  author = {Wild, C. J. and Pfannkuch, M.},
  year = {1999},
  journal = {International Statistical Review},
  volume = {67},
  number = {3},
  pages = {223--265}
}

@article{wilkinson1973,
  title = {Symbolic Description of Factorial Models for Analysis of Variance},
  author = {Wilkinson, G. N. and Rogers, C. E.},
  year = {1973},
  journal = {Journal of the Royal Statistical Society},
  volume = {22},
  number = {3},
  pages = {392--399},
  abstract = {The paper describes the symbolic notation and syntax for specifying factorial models for analysis of variance in the control language of the GENSTAT statistical program system at Rothamsted. The notation generalizes that of Nelder (1965). Algorithm AS 65 (Rogers,1973) converts factorial model formulae in this notation to a list of model terms represented as binary integers. A further extension of the syntax is discussed for specifying models generally (including non-linear forms)}
}

@book{wilkinson2005,
  title = {The {{Grammar}} of {{Graphics}}},
  author = {Wilkinson, Leland},
  year = {2005},
  series = {Statistics and Computing},
  publisher = {Springer Science + Business Media}
}

@article{willett2007,
  title = {Scented {{Widgets}}: {{Improving Navigation Cues}} with {{Embedded Visualizations}}},
  author = {Willett, Wesley and Heer, Jeffrey and Agrawala, Maneesh},
  year = {2007},
  journal = {IEEE Information Visualization},
  abstract = {This paper presents scented widgets, graphical user interface controls enhanced with embedded visualizations that facilitate navigation in information spaces. We describe design guidelines for adding visual cues to common user interface widgets such as radio buttons, sliders, and combo boxes and contribute a general software framework for applying scented widgets within applications with minimal modifications to existing source code. We provide a number of example applications and describe a controlled experiment which finds that users exploring unfamiliar data make up to twice as many unique discoveries using widgets imbued with social navigation data. However, these differences equalize as familiarity with the data increases.}
}

@article{williams2009,
  title = {Rattle: {{A Data Mining GUI}} for {{R}}},
  author = {Williams, Graham J.},
  year = {2009},
  month = dec,
  journal = {The R Journal},
  volume = {1},
  number = {2},
  abstract = {Data mining delivers insights, patterns, and descriptive and predictive models from the large amounts of data available today in many organizations. The data miner draws heavily on methodologies, techniques and algorithms from statistics, machine learning, and computer science. R increasingly provides a powerful platform for data mining. However, scripting and programming is sometimes a challenge for data analysts moving into data mining. The Rattle package provides a graphical user interface specifically for data mining using R. It also provides a stepping stone toward using R as a programming language for data analysis.}
}

@phdthesis{williams2023,
  title = {It's a {{Jungle Out There}}: {{Data Abstraction Elephants}} and {{Mental Models}}},
  shorttitle = {It's a {{Jungle Out There}}},
  author = {Williams, Kathryn},
  year = {2023},
  address = {United States -- Arizona},
  urldate = {2023-07-10},
  abstract = {This dissertation explores how mental models of data influence visualization design. A mental model of a dataset is the user's understanding of the data, encompassing their prior experiences, interests, and knowledge of the data. In this dissertation, I demonstrate how the flexibility of mental models can allow for changes in the visualization, while paradoxically the inflexibility of personal mental models indicates designers should prioritize aligning the visualization with the mental model. To connect the internal (i.e., mental models) with the external (i.e., data visualizations), we suggest using data abstractions to link the mental model to a data-related structure. This gives the user and designer a common language to start from and guides visualization design. Choosing the data abstraction has been recognized as an important part of the design process, but this abstraction is typically based on the data itself, not on users' mental models of the data. Mental model research has been centered around mental models that arise after seeing a visualization and how users utilize their mental models of the existing visualization. This dissertation addresses this gap in the visualization design methodology from three perspectives: (1) how mental models of data are created from non-tabular data without prior visualizations, (2) describing data abstractions and exploring alternatives, and (3) how changing data abstractions requires changing the visualization to fit the mental model. I first describe a design study of a tree visualization in which my collaborators and I considered changing the data abstraction to fit the data, but ultimately chose the abstraction that fit our users' mental models. Second, I explore types of data abstractions, how they are related, and the effects of changing data abstractions. Finally, I investigate how different mental models can arise from the same dataset via a study involving sketching non-tabular data. I conclude by asking how to better facilitate these discussions of mental models before and during a visualization design study. Understanding and formalizing what questions to ask to elicit useful descriptions of mental models will allow designers to create visualizations that better suit users' mental models of the data.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9798379492274},
  langid = {english},
  school = {The University of Arizona},
  keywords = {Data abstractions,Data visualization,Mental models,Visualization design}
}

@incollection{wills2008,
  title = {Handbook of {{Data Visualization}}},
  author = {Wills, Graham},
  year = {2008},
  publisher = {Springer Handbooks},
  chapter = {Linked Data Views}
}

@book{wills2016,
  title = {Brunel {{Visualization}}},
  author = {Wills, Graham},
  year = {2016}
}

@incollection{wilson2008,
  title = {Teacher {{Education}}: {{A}} Conduit to the Classroom},
  booktitle = {Research on {{Technology}} and the {{Teaching}} and {{Learning}} of {{Mathematics}}},
  author = {Wilson, Patricia S.},
  editor = {Heid, M. Kathleen and Blume, Glendon W.},
  year = {2008},
  volume = {2},
  publisher = {National Council of Teachers of Mathematics},
  keywords = {education,mathematics}
}

@article{wilson2014,
  title = {Best {{Practices}} for {{Scientific Computing}}},
  author = {Wilson, Greg and Aruliah, D. A. and Brown, C. Titus and Hong, Neil P. Chue and Davis, Matt and Guy, Richard T. and Haddock, Steven H. D. and Huff, Kathryn D. and Mitchell, Ian and Plumbley, Mark and Waugh, Ben and White, Ethan P. and Wilson, Paul},
  year = {2014},
  journal = {PLoS Biology},
  volume = {12},
  number = {1}
}

@book{wilson2016,
  title = {Good Enough Practices for Scientific Computing},
  author = {Wilson, Greg and Bryan, Jennifer and Cranston, Karen and Kitzes, Justin and Nederbragt, Lex and Teal, Tracy K.},
  year = {2016}
}

@article{wing2006,
  title = {Computational {{Thinking}}},
  author = {Wing, Jeannette M.},
  year = {2006},
  month = mar,
  journal = {Communications of the ACM},
  volume = {49},
  number = {3},
  pages = {33--35},
  keywords = {computational thinking}
}

@book{wing2011,
  title = {Computational {{Thinking}}},
  author = {Wing, Jeannette M.},
  year = {2011},
  month = mar,
  keywords = {computational thinking},
  annotation = {Published: Beamer Presentation}
}

@article{wise2020,
  title = {Educating {{Data Scientists}} and {{Data Literate Citizens}} for a {{New Generation}} of {{Data}}},
  author = {Wise, Alyssa Friend},
  year = {2020},
  month = jan,
  journal = {Journal of the Learning Sciences},
  volume = {29},
  number = {1},
  pages = {165--181},
  publisher = {Routledge},
  issn = {1050-8406},
  doi = {10.1080/10508406.2019.1705678},
  urldate = {2023-01-10}
}

@article{wobbrockkientz2016,
  title = {Research Contributions in Human-Computer Interaction},
  author = {Wobbrock, Jacob O. and Kientz, Julie A.},
  year = {2016},
  month = apr,
  journal = {Interactions},
  volume = {23},
  number = {3},
  pages = {38--44},
  issn = {1072-5520, 1558-3449},
  doi = {10.1145/2907069},
  urldate = {2022-09-29},
  langid = {english}
}

@article{woodardlee2021,
  title = {How {{Students Use Statistical Computing}} in {{Problem Solving}}},
  author = {Woodard, Victoria and Lee, Hollylynne},
  year = {2021},
  month = jan,
  journal = {Journal of Statistics and Data Science Education},
  volume = {29},
  number = {sup1},
  pages = {S145-S156},
  issn = {2693-9169},
  doi = {10.1080/10691898.2020.1847007},
  urldate = {2021-07-20},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/5TWKTX7C/Woodard and Lee - 2021 - How Students Use Statistical Computing in Problem .pdf}
}

@techreport{wright2010,
  title = {Increasing {{Student}} and {{School Interest}} in {{Engineering Education}} by {{Using}} a {{Hands-on Inquiry Based Programming Curriculum}}},
  author = {Wright, Geoffrey and Rich, Peter and Leatham, Keith},
  year = {2010},
  institution = {American Society for Engineering Education}
}

@inproceedings{wuetal2021,
  title = {Understanding {{Data Accessibility}} for {{People}} with {{Intellectual}} and {{Developmental Disabilities}}},
  booktitle = {Proceedings of the 2021 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Wu, Keke and Petersen, Emma and Ahmad, Tahmina and Burlinson, David and Tanis, Shea and Szafir, Danielle Albers},
  year = {2021},
  month = may,
  series = {{{CHI}} '21},
  pages = {1--16},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3411764.3445743},
  urldate = {2023-07-10},
  abstract = {Using visualization requires people to read abstract visual imagery, estimate statistics, and retain information. However, people with Intellectual and Developmental Disabilities (IDD) often process information differently, which may complicate connecting abstract visual information to real-world quantities. This population has traditionally been excluded from visualization design, and often has limited access to data related to their well being. We explore how visualizations may better serve this population. We identify three visualization design elements that may improve data accessibility: chart type, chart embellishment, and data continuity. We evaluate these elements with populations both with and without IDD, measuring accuracy and efficiency in a web-based online experiment with time series and proportion data. Our study identifies performance patterns and subjective preferences for people with IDD when reading common visualizations. These findings suggest possible solutions that may break the cognitive barriers caused by conventional design guidelines.},
  isbn = {978-1-4503-8096-6},
  keywords = {graphical perception \& cognition,human-subjects quantitative studies},
  file = {/Users/amcnamara/Zotero/storage/BJM843ZB/Wu et al. - 2021 - Understanding Data Accessibility for People with I.pdf}
}

@book{xie2014,
  title = {Dynamic {{Documents}} with {{R}} and Knitr},
  author = {Xie, Yihui},
  year = {2014},
  publisher = {Chapman \& Hall/CRC The R Series}
}

@techreport{yamamiya2009,
  title = {Active {{Essays}} on the Web},
  author = {Yamamiya, Takashi and Warth, Alessando and Kaehler, Ted},
  year = {2009},
  institution = {Viewpoints Research Institute}
}

@book{yau2011,
  title = {Visualize {{This}}: {{The FlowingData Guide}} to {{Design}}, {{Visualization}}, and {{Statistics}}},
  author = {Yau, Nathan},
  year = {2011},
  publisher = {Wiley}
}

@phdthesis{yau2013,
  title = {An {{Online Tool}} for {{Personal Data Collection}} and {{Exploration}}},
  author = {Yau, Nathan},
  year = {2013},
  school = {UCLA}
}

@book{yau2013a,
  title = {Data {{Points}}: {{Visualization}} That {{Means Something}}},
  author = {Yau, Nathan},
  year = {2013},
  publisher = {Wiley}
}

@book{yau2014,
  title = {How to {{Read Histograms}} and {{Use Them}} in {{R}}},
  author = {Yau, Nathan},
  year = {2014}
}

@incollection{zbiek2008,
  title = {A Research-Informed View of the Process of Incorporating Mathematics Technology into Classroom Practice by in-Service and Prospective Teachers},
  booktitle = {Research on Technology and the Teaching and Learning of Mathematics},
  author = {Zbiek, Rose Mary and Hollebrands, Karen},
  editor = {Heid, M. Kathleen and Blume, Glendon W.},
  year = {2008},
  volume = {1},
  publisher = {National Council of Teachers of Mathematics},
  keywords = {education,mathematics,technology}
}

@article{zieffleretal2008,
  title = {A {{Framework}} to {{Support Research}} on {{Informal Inferential Reasoning}}},
  author = {Zieffler, Andrew and Garfield, Joan and {delMas}, Robert C. and Reading, Chris},
  year = {2008},
  journal = {Statistics Education Research Journal},
  volume = {7},
  number = {2},
  pages = {40--58}
}

@article{zieffleretal2008a,
  title = {What {{Does Research Suggest About}} the {{Teaching}} and {{Learning}} of {{Introductory Statistics}} at the {{College Level}}? {{A Review}} of the {{Literature}}},
  shorttitle = {What {{Does Research Suggest About}} the {{Teaching}} and {{Learning}} of {{Introductory Statistics}} at the {{College Level}}?},
  author = {Zieffler, Andrew and Garfield, Joan and Alt, Shirley and Dupuis, Danielle and Holleque, Kristine and Chang, Beng},
  year = {2008},
  month = jul,
  journal = {Journal of Statistics Education},
  volume = {16},
  number = {2},
  pages = {null},
  publisher = {Taylor \& Francis},
  issn = {null},
  doi = {10.1080/10691898.2008.11889566},
  urldate = {2022-01-31},
  abstract = {Since the first studies on the teaching and learning of statistics appeared in the research literature, the scholarship in this area has grown dramatically. Given the diversity of disciplines, methodology, and orientation of the studies that may be classified as ``statistics education research,'' summarizing and critiquing this body of work for teachers of statistics is a challenging and important endeavor. In this paper, a representative subset of studies related to the teaching and learning of statistics in introductory, non-calculus based college courses is reviewed. As a result of this review, and in an effort to improve the teaching and learning of statistics at the introductory college level, some guidelines to help advance future research in statistics education are offered.},
  keywords = {College students,Statistics Education Research,Teaching and learning},
  file = {/Users/amcnamara/Zotero/storage/K6BD8N2A/Zieffler et al. - 2008 - What Does Research Suggest About the Teaching and .pdf;/Users/amcnamara/Zotero/storage/88K7YMSK/10691898.2008.html}
}

@article{ziemann2016,
  title = {Gene Name Errors Are Widespread in the Scientific Literature},
  author = {Ziemann, Mark and Eren, Yotam and {El-Osta}, Assam},
  year = {2016},
  journal = {Genome Biology},
  volume = {17},
  number = {177}
}

@incollection{ziemkiewicz2009,
  title = {Advances in {{Information}} and {{Intelligent Systems}}},
  author = {Ziemkiewicz, Caroline and Kosara, Robert},
  year = {2009},
  publisher = {Springer},
  chapter = {Embedding Information Visualization within Visual Representation},
  keywords = {to read}
}

@article{zongetal2022,
  title = {Rich {{Screen Reader Experiences}} for {{Accessible Data Visualization}}},
  author = {Zong, Jonathan and Lee, Crystal and Lundgard, Alan and Jang, JiWoong and Hajas, Daniel and Satyanarayan, Arvind},
  year = {2022},
  month = jun,
  urldate = {2022-07-12},
  abstract = {Abstract Current web accessibility guidelines ask visualization designers to support screen readers via basic non-visual alternatives like textual descriptions and access to raw data tables. But charts do more than summarize data or reproduce tables; they afford interactive data exploration at varying levels of granularity\,---\,from fine-grained datum-by-datum reading to skimming and surfacing high-level trends. In response to the lack of comparable non-visual affordances, we present a set of rich screen reader experiences for accessible data visualization and exploration. Through an iterative co-design process, we identify three key design dimensions for expressive screen reader accessibility: structure, or how chart entities should be organized for a screen reader to traverse; navigation, or the structural, spatial, and targeted operations a user might perform to step through the structure; and, description, or the semantic content, composition, and verbosity of the screen reader's narration. We operationalize these dimensions to prototype screen-reader-accessible visualizations that cover a diverse range of chart types and combinations of our design dimensions. We evaluate a subset of these prototypes in a mixed-methods study with 13 blind and visually impaired readers. Our findings demonstrate that these designs help users conceptualize data spatially, selectively attend to data of interest at different levels of granularity, and experience control and agency over their data analysis process. An accessible HTML version of this paper is available at:~http://vis.csail.mit.edu/pubs/rich-screen-reader-vis-experiences. 1 Introduction Despite decades of visualization research and recent legal requirements to make web-based content accessible~[59, 30], web-based visualizations remain largely inaccessible to people with visual disabilities. Charts on mainstream publications are often completely invisible to screen readers (an assistive technology that transforms text and visual media into speech) or are rendered as incomprehensible strings of ``graphic graphic graphic''~[54, 49]. Current accessibility guidelines ask visualization designers to provide textual descriptions of their graphics via alt text (short for alternative text) and link to underlying data tables~[26, 60]. However, these recommendations do not provide modes of information-seeking comparable to what sighted readers enjoy with interactive visualizations. For instance, well-written alt text can provide a high-level takeaway of what the visualization shows, but it does not allow readers to drill down into the data to explore specific sections. While tables provide readers with the ability to hone in on specific data points, reading data line-by-line quickly becomes tedious and makes it difficult to identify overall trends. Developing rich non-visual screen reader experiences for data visualizations poses several unique challenges. Although visuomotor interactions (like hovering, pointing, clicking, and dragging) have been core to visualization research~[22], screen readers redefine what interaction is for visualization. Rather than primarily manipulating aspects of the visualization or its backing data pipeline~[65, 29, 22], screen readers make reading a visualization an interactive operation as well\,---\,users must intentionally perform actions with their input devices in order to cognize visualized elements. Moreover, as screen readers narrate elements one-at-a-time, they explicitly linearize reading a visualization. As a result, in contrast to sighted readers who can choose to selectively attend to specific elements and have access to the entire visualization during the reading process, screen reader users are limited to the linear steps made available by the visualization author and must remember (or note down) prior output conveyed by the screen reader. Despite these modality differences, studies have found that screen reader users share the same information-seeking goals as sighted readers: an initial holistic overview followed by comparing data points~[54], akin to the information-seeking mantra of ``overview first, zoom and filter, and details on demand''~[57]. In this paper, we begin to bridge this divide by conducting an iterative co-design process (co-author Hajas is a blind researcher with relevant experience) prototyping rich and usable screen reader experiences for web-based visualizations. We identify three design dimensions for enabling an expressive space of experiences: structure, or how the different elements of a chart should be organized for a screen reader to traverse; navigation, which describes the operations a user may perform to move through this structure; and, description, which specifies the semantic content, composition, and verbosity of text conveyed at each step. We demonstrate how to operationalize these design dimensions through diverse accessible reading experiences across a variety of chart types. To evaluate our contribution, we conduct an exploratory mixed-methods study with a subset of our prototypes and 13 blind or visually impaired screen reader users. We identify specific features that make visualizations more useful for screen reader users (e.g., hierarchical and segmented approaches to presenting data, cursors and roadmaps for spatial navigation) and identify behavior patterns that screen reader users follow as they read a visualization (e.g., constant hypothesis testing and validating their mental models). 2 Background and Related Work Screen Reader Assistive Technology. A screen reader is an assistive technology that conveys digital text or images as synthesized speech or braille output. Screen readers are available as standalone third-party software or can be built-in features of desktop and mobile operating systems. A screen reader allows a user to navigate content linearly with input methods native to a given platform (e.g., touch on smartphones, mouse/keyboard input on desktop). Content authors must generate and attach alt text to their visual content like images or charts in order for them to be accessible to screen reader users. Functionality and user experience differs across platforms and screen readers. In this paper, however, we focus on interacting with web-based visualizations with the most widely used desktop screen readers (JAWS/NVDA for Windows, VoiceOver for Mac). Web Accessibility Standards. In 2014, the World Wide Web Consortium (W3C) adopted the Web Accessibility Initiative's Accessible Rich Internet Applications protocol (WAI-ARIA) which introduced a range of semantically-meaningful HTML attributes to allow screen readers to better parse HTML elements~[44]. In particular, these attributes allow a screen reader to convey the state of dynamic widgets (e.g., autocomplete is available for text entry), alert users to live content updates, and identify common sections of a web page for rapid navigation (e.g., banners or the main content). In 2018, the W3C published the WAI-ARIA Graphics Module~[58] with additional attributes to support marking up structured graphics such as charts, maps, and diagrams. These attributes allow designers to annotate individual and groups of graphical elements as well as surface data values and labels for a screen reader to read aloud. Accessible Visualization Design. In a recent survey, Kim et al.~[38] describe the rich body of work that has explored multi-sensory approaches to visualization for multiple disabilities~[66, 28, 36, 6, 64, 41]. Here, we focus on screen reader output native to web-based interfaces for blind users (namely via speech). Sharif et al.~[54] find that many web-based charts are intentionally designed to cause screen readers to skip over them. For charts that a screen reader does detect, blind or visually impaired users nevertheless experience significant difficulties: these users spend 211\% more time interacting with the charts and are 61\% less accurate in extracting information compared to non-screen-reader users~[54]. Despite the availability of ARIA, alt text and data tables remain the most commonly used and recommended methods for making web-based charts accessible to screen readers~[26, 60, 16]. However, each of these three approaches comes with its own limitations. Static alt text requires blind readers to accept the author's interpretation of the data; by not affording exploratory and interactive modes, alt text robs readers of the necessary time and space to interpret the numbers for themselves~[42]. Recent research also suggests that blind people have nuanced preferences for the kinds of visual semantic content conveyed via text~[48, 42], and desire more interactive and exploratory representations of pictorial images~[45]. Data tables, on the other hand, undo the benefits of abstraction that visualizations enable\,---\,they force readers to step sequentially through data values making it difficult to identify larger-scale patterns or trends, and do not leverage the structure inherent to web-based grammars of graphics~[10, 50]. Finally, ARIA labels are not a panacea; even when they are used judiciously\,---\,a non-trivial task which often results in careless designs that cause screen readers to simply read out long sequences of numbers without any other identifiable information~[49]\,---\,they present a fairly low expressive ceiling. The current ARIA specification does not afford rich and nuanced information-seeking opportunities equivalent to those available to sighted readers. There has been some promising progress for improving support for accessibility within visualization toolkits, and vice-versa for improving native support for charts in screen reader technologies. For instance, Vega-Lite~[50] and Highcharts~[31] are beginning to provide ARIA support out-of-the-box. Apple's VoiceOver Data Comprehension feature~[20] affords more granular screen reader navigation within the chart, beyond textual summaries and data tables, via four categories of selectable interactions for charts appearing in Apple's Stocks or Health apps. These interactions include Describe Chart, which describes properties of the chart's construction, such as its encodings, axis labels, and ranges; Summarize Numerical Data, which reports min and max data values, and summary statistics like mean and standard deviation; Describe Data Series, which reports the rate-of-change/growth of a curve, trends, and outliers; and Play Audiograph, which plays a tonal representation of the graph's ascending/descending trend over time~[20]. While Apple's features are presently limited to single-line charts, SAS' Graphics Accelerator~[1] supports a similar featureset (including sonification, textual descriptions, and data tables) but for a broader range of statistical charts including bar charts, box plots, contour plots, and scatter plot matrices. Our work follows in the spirit of these tools but focuses on web-based visualizations rather than standalone- or platform-integrated software. We go beyond what ARIA supports today to enable high-level and fine-grained screen reader interactions, and hope that our work will help inform ongoing discussions on improving web accessibility standards (e.g., via an Accessibility Object Model~[11]). 3 Design Dimensions for Rich Screen Reader Experiences Currently, the most common ways of making a visualization accessible to screen readers include adding a single high-level textual description (via alt text), providing access to low-level data via a table, or tagging visualization elements with ARIA labels to allow screen readers to step through them linearly (e.g., as with Highcharts~[31]). While promising, these approaches do not afford rich information-seeking behaviors akin to what sighted readers enjoy with interactive visualizations. To support systematic thinking about accessible visualization design, we introduce three design dimensions that support rich, accessible reading experiences: structure, or how elements of the visualization should be organized for a screen reader to traverse; navigation, or the mechanisms by which a screen reader user can move from one element to another; and description, or what semantic content the screen reader conveys. Methods. We began by studying the development of multi-sensory graphical systems, covering work in critical cartography~[63, 39], blind education~[2, 25], tactile graphics~[24, 28, 21, 3, 14], and multi-sensory visualization~[43, 17, 13, 5]. Drawing on conventions and literature on crip, reflective, and participatory design~[27, 53, 19], all authors began an iterative co-design process with Hajas, who is a blind researcher with relevant expertise. Hajas is a screen reader user with a PhD in HCI and accessible science communication, but he is not an expert in visualization research. Co-design\,---\,particularly as encapsulated in the disability activism slogan, ``Nothing about us, without us''~[19]\,---\,is important because it can eliminate prototypes that replicate existing tools, solve imaginary problems (i.e., by creating disability dongles~[34]) or unintentionally produce harmful technology~[56]. To balance engaging disabled users while acknowledging academia's traditionally extractive relationship with marginalized populations~[18], we intentionally acknowledge Hajas as both co-designer and co-author. We believe that the distinction between co-designer\,---\,a phrase that often discounts lived experience as insufficiently academic\,---\,and researcher is minimal; technical, qualitative, and experiential expertise are all important components of this research. Hajas' profile is a perfect example of the intersection between lived experience of existing challenges and solutions, academic experience of research procedures, and an interest in the science of visualization. While he does not represent all screen reader users, his academic expertise and lived experience uniquely qualify him to be both researcher and co-designer. Nevertheless, to incorporate a diverse range of perspectives, we recruited additional participants as part of an evaluative study ({\S}~5). Our work unfolded over 6 months and yielded 15 prototypes. All authors met weekly for hour-long video conferences. In each session, we would discuss the structure and affordances of the prototypes, often by observing and recording Hajas' screen as he worked through them. We would also use these meetings to reflect on how the prototypes have evolved, compare their similarities and differences, and whiteboard potential design dimensions to capture these insights. Following these meetings, Hajas wrote memos detailing the motivations for each prototype, tagging its most salient features, summarizing the types of interactions that were available, enumerating questions that the prototype raises, and finally providing high-level feedback about its usefulness and usability. In the following section, we liberally quote these memos to provide evidence and additional context for our design dimensions. 3.1 Structure We define structure to mean an underlying representation of a visualization that organizes its data and visual elements into a format that can be traversed by a screen reader. Through our co-design process, we identified two components important to analyzing accessible structures: their form, or the shape they organize information into; and entities, or which parts of the visualization specification are used to translate a chart into a non-visual structure. Design decisions about form and entities are guided by considerations of information granularity, or how many levels comprise the range between a high-level overview and individual data values. Long Description Figure 1: (a) An accessible visualization structure in the form of a tree and comprised of encoding entities. Solid magenta outlines indicate the location of the screen reader cursor. Solid blue arrows between labels indicate available next steps via keyboard navigability (up, down, left, right). (b) Three ways of navigating accessible visualization structures: structural, spatial, and targeted. Form. Accessible structures organize information about the visualization into different forms, including lists, tables, and trees. Consider existing best practices and common approaches. A rasterized chart with alt text is represented to a screen reader as a single node. SVG-based visualizations can additionally be tagged with ARIA labels to describe the axes, legends, and individual data points. Despite SVG's nesting, screen readers linearize these ARIA labels into a list structure so that the user can step through them sequentially. Data tables, on the other hand, provide a grid structure for screen readers to traverse. At each cell of the grid, the screen reader reads out a different textual description, allowing the user to explore a space by traversing the grid spatially (up, down, left, and right) instead of merely linearly. Accessible visualization research has begun to explore the use of tree structures for storing chart metadata~[62], but they remain relatively rare in practice. Our prototypes primarily use trees as their branching and hierarchical organization allows users to browse different components of a visualization and traverse them at different levels of detail. Entities. Where form refers to how nodes in a structure are arranged, entities instead refers to what aspects of the visualization the nodes represent. These aspects can include: Data, where nodes in the structure represent individual data values or different slices of the data cube (e.g., by field, bins, categories, or interval ranges). For example, in a data table, every node (i.e. cell) represents a data value designated by the row and column coordinates. Depending on the form, data entities can be presented at different levels of detail. For example, one prototype we explored represents a line chart as a binary tree structure (Fig.~2e): the root node represents the entire x-axis domain, and each left and right child node recursively splits the domain in half. Users can traverse the tree downward to binary search for specific values or understand the data distribution. Encodings, where nodes in the structure correspond to visual channels (e.g., position, color, size) that data fields map to. For instance, consider Figure~1a which depicts the encoding structure of a Vega-Lite scatterplot. The visualization is specified as mappings from data fields to three visual encoding channels: x, y, and color. Thus, the encoding structure, which here takes the form of a tree, comprises a root node that represents the entire visualization and then branches for each encoding channel as well as the data rectangle (x-y grid). Descending into these branches yields nodes that select different categories or interval regions, determined by the visual affordances of the channel. For instance, descending into axis branches yields nodes for each interval between major ticks; x-y grid nodes represent cells in the data rectangle as determined by intersections of the axes gridlines; and legend nodes reflect the categories or intervals of the encoding channel (i.e., for nominal or quantitative data respectively). Finally, the leaves of these branches represent individual data values that fall within the selected interval or category. Annotations, where nodes in the structure represent the rhetorical devices a visualization author may use to to shape a visual narrative or guide reader interpretation of data (e.g., by drawing attention to specific data points or visual regions). Surfacing annotations in the visualization structure allows screen reader users to also benefit from and be guided by the author's narrative intent. For example, Figure~2d illustrates an annotation tree structure derived from an example line chart with two annotations highlighting intervals in the temporal x-axis. The root of the tree has two children representing the two annotated regions. The these two annotation nodes have a child node for each data point that is highlighted within the region of interest. Considerations: Information Granularity. When might users prefer nested structures (i.e. trees) over flat structures (i.e., lists and tables)? Like sighted users, screen reader users seek information by looking for an overview before identifying subsets to view in more detail~[54]. Trees allow users to read summary information at the top of the structure, and traverse deeper into branches to acquire details-on-demand. Kim et al. use the term information granularity to refer to the different levels of detail at which an accessible visualization might reveal information~[38]. They organize granularity into three levels: existence, overview, and detail. Existence includes information that a chart is present, but no information about underlying data. Overview includes summary information about data\,---\,e.g. axes, legends, and summary statistics like min, max, or mean\,---\,but not individual data points. Detail includes information about precise data values. We use the root node to signal the existence of the tree, and deeper nodes in the tree reflect finer levels of granularity. Branch nodes give an overview summary about the data underneath, providing information scent~[47], while leaf nodes map to individual data points. In his feedback about the prototype shown in Figure~1, Hajas wrote ``considering how difficult reading a scatterplot with a screen reader is due to its sequential reading nature, the tree structure makes the huge number of data points fairly readable''. Entities are not mutually exclusive, and a structure might opt to surface different entities in parallel branches. We prototyped a version of Figure~2d which placed an encoding tree and annotation tree as sibling branches under the root node. Users could descend down a given branch, and switch to the equivalent location in the other branch at will. These design decisions are motivated by findings in prior work: by placing encodings and annotations as co-equal branches, we produce a structure that preserves the agency of screen reader users either to start with the narrative arc of annotations, or follow it after having the chance to interpret the data for themselves~[42]. As Hajas confirms ``Depending on my task, either the encoding or annotation tree could be more important. If my task involved checking population growth in the last 100 years, I would start with the encodings. If I were to look for sudden changes in population numbers, such war-time mortality effects, I would start exploring the annotations, then tunnel back to the other tree.'' 3.2 Navigation Screen reader users need ways to traverse accessible structures to explore data or locate specific points. When browsing a webpage, screen readers provide a cursor that represents the current location in the page. Users use keyboard commands to step the cursor backward and forward in a sequential list of selectable items on the page, or jump to important locations such as headers and links. Through our prototyping process, we developed three ways of navigating through an accessible structure: structural navigation, spatial navigation, and targeted navigation (Fig.~1b). A key concern across these navigation schemes is reducing a user's cognitive load by affording a sense of the boundaries of the structure. Structural Navigation. Structural navigation refers to ways users move within the accessible structure. We identify two types of structural navigation. Local navigation refers to step-by-step movements between adjacent nodes in the structure. This includes moving up and down levels of a hierarchy, or moving side to side between sibling elements. Lateral navigation refers to movement between equivalent nodes in adjacent sub-structures. For example, Fig.~2a depicts a multi-view visualization with six facets. When the cursor is on a Y-axis interval for the first facet, directly moving to the same Y-axis interval on the second facet is a lateral move. Spatial Navigation. Sometimes users want to traverse the visualization according to directions in the screen coordinate system. We refer to this as spatial navigation. For example, when traversing part of an encoding structure that represents the visualization's X-Y grid, a downward structural navigation would go down a level into the currently selected cell of the grid, showing the data points inside the cell. A downward spatial navigation, in contrast, would move to the grid cell below the current one\,---\,i.e. towards the bottom of the Y-axis. Spatial navigation is also useful when navigating lists of data points, which may not be sorted by X or Y value in the encoding structure. Where a leftward structural navigation would move to the previous data point in the structure, a leftward spatial navigation would move to the point with the next lowest X value. Targeted Navigation. Navigating structurally and spatially requires a user to maintain a mental map of where their cursor is relative to where they want to go. If the user has a specific target location in mind, maintaining this mental map in order to find the correct path in the structure to their target can create unnecessary cognitive load. We use targeted navigation to refer to methods that only require the user to specify a target location, without needing to specify a path to get there. For example, the user might open a list of locations in the structure and select one to jump directly there. Screen readers including JAWS and VoiceOver implement an analogous form of navigation within webpages. Instead of manually stepping through the page to find a specific piece of content, users can open a menu with a list of locations in the page. These locations are defined in HTML using ARIA landmark roles, which can designate parts of the DOM as distinct sections when read by a screen reader. When a screen reader user open the list of landmarks and selects a landmark, their cursor moves directly to that element. Considerations: Boundaries \& Cognitive Load. Screen reader users only read part of the visualization at a time, akin to a sighted user reading a map through a small tube [28]. How do they keep track of where they are? In our co-design process, we found it easiest for a user to remember their location relative to a known starting point, which is corroborated by literature on developing spatial awareness for blind people [63, 40, 17]. Hajas noted the prevalence of the Home and End shortcuts across applications for returning to a known position in a bounded space (e.g. the start/end of a line in a text editor). We also found that grouping data by category or interval was helpful for maintaining position. Hajas noted that exploring data within a bounded region was like entering a room in a house. In his analogy, a house with many smaller rooms with doors is better than a house with one big room and no doors. Bounded spaces alleviate cognitive load by allowing a user to maintain their position relative to entry points. Comparing navigation techniques, Hajas noted that spatial felt ``shallow but broad'' while targeted felt ``deep but narrow.'' While he expressed a personal preference for deep-narrow structures, he nevertheless ``would not give up [spatial navigation] because it makes me believe I'm actually interacting with a visualization.'' This insight demonstrates the value of offering multiple complementary navigation techniques. Moreover, while targeted navigation facilitates quick searching and doesn't require the user to maintain a mental map to find specific data points, structural and spatial exploration enable more open-ended data exploration. It also provides a mechanism for establishing common ground with sighted readers (e.g., allowing both blind and sighted readers to understand a line segment as being ``above'' or ``higher'' than another). 3.3 Description When a user navigates to a node in a structure, the screen reader narrates a description associated with that node. For example, when navigating to the chart's legend, the screen reader output might articulate visual properties of the chart's encoding: ``Category O has color encoding green; X has color encoding orange'' (Figure~1). Or, if that visual semantic content isn't relevant to understanding the data, it might ignore the color: ``each datum belongs to either Category O or X.'' The content, composition, and verbosity of the description can affect a user's comprehension of the data. Designers must consider context \& customization when describing charts. Content. Semantic content is the meaningful information conveyed not only through natural language utterances, but also through the visualization (a graphical language~[8]). Because graphics convey myriad different kinds of content, the challenge of natural language description is to convey information that is not only commensurate with what the chart expresses via graphical language, but also useful to its readers. Accessible chart description guidelines from WGBH~[26], W3C~[60], and others~[35] offer prescriptions for conveying specific content for blind readers (such as the chart's title, axis encodings, and noteworthy trends). Lundgard and Satyanarayan expand the scope of these guidelines with a more general conceptual model of four levels of semantic content: chart construction properties (e.g., axes, encodings, marks, title); statistical concepts and relations (e.g., outliers, correlations, descriptive statistics); perceptual and cognitive phenomena (e.g., complex trends, patterns); and domain-specific insights (e.g., socio-political context relevant to the data)~[42]. Decoupling a chart's semantic content from its visual representation helps us better understand what data representations afford for different readers. For instance, Lundgard and Satyanarayan find that what blind readers report as most useful in a chart description is not a straightforward translation of the visual data representation. Specifically, simply listing the chart's encodings is much less useful to blind readers than conveying summary statistics and overall trends in the data~[42]. As Hajas noted, ``I want to see the global trend, which is why sighted people rely on visualization.'' For instance, for a stock market chart the reader ``might see the overview from first to last data points, and then zoom into an outlier in the middle.'' These findings suggest opportunities interleave different kinds of content at different levels of a hierarchical structure to yield richer, more useful screen reader navigation. For example, injecting summary statistics (say, the existence of outliers within a particular subcategory of the data) higher up in the chart's tree structure (e.g., at the legend encoding node) might afford ``scent'' for ``information foraging''~[47], or further exploration down a particular branch (data subcategory) of the tree. Or, if navigating in a targeted fashion, the user might be afforded the option to directly navigate to outliers without traversing the tree. Composition. The usefulness of a description depends not only on the content conveyed by its constituent sentences, but also on its composition: how those sentences are ordered in relation to each other. For example, during our co-design process, Hajas found that when navigating a chart's tree structure, the screen reader output could quickly become redundant, affecting how quickly and efficiently he could pick out the meaningful information at each node. For instance, the utterance ``Category: O, Point 3 of 15, x = 5, y = 12'' and the utterance ``x = 5, y = 12, Category: O, Point 3 of 15'' afford significantly different experiences for a user who wishes to quickly scan through individual data points. In the first utterance, the reader immediately receives content that helps to situate them in a broader data context, namely data labeled as Category: O at the legend node. In the second utterance, the reader immediately receives datum-specific content that helps to rapidly explore the fine-grained details within that data context. Whether a reader prefers one compositional ordering to another will depend on the task they are attempting to accomplish. As Hajas noted ``I like the label at the beginning of the information, saying at which level of the tree I am at. It is important for knowing where I am. It is also great that this information is only spoken out when I change level, but not when I navigate laterally.'' These compositional choices are highly consequential for readers' experience, when they must repeatedly read nearly-identical utterances while navigating a structure. Verbosity. Whereas composition refers to the ordering of content, verbosity refers to how much content the screen reader conveys. More content is not always better. As Hajas noted of Apple's Data Comprehension feature~[20]: ``It can sometimes be too much information all at once, if it starts reading out all of the data. This is very difficult if you're interested in some data points that are in the middle. It is very play-or-stop.'' Depending on the screen reader software, a user may be afforded control over how much content is conveyed. For instance, JAWS offers high, medium, and low verbosity levels~[52]. At higher verbosity the screen reader announces more structural, wayfinding content (e.g. the start and end of regions). For data tables, verbosity configurations can affect whether the table size is read as part of the description, and whether row and column labels are repeated for every cell. Descriptions of nodes in an encoding structure might analogously include information about the path from the root\,---\,for example, by reminding the user that they are reading Y-axis intervals. These repetitions can help users remember their location within a structure, but additional verbosity is less efficient for comprehending the data quickly. Considerations: Context \& Customization. Apart from its constituent parts (content, composition, verbosity), a description's usefulness also depends on the context in which it is read: namely, the reader's task or intent, and familiarity with the data interface. The same description might be useful in some situations, but relatively useless in others. A reader's information needs are fundamentally context-sensitive. For example, as Hajas noted, when reading a news article, it may be satisfactory to accept a journalist's description of the data on good faith. But, when reviewing scientific research, ``I don't necessarily want to just believe what is said in the text, I want to check and double-check the authors' claims. Go down to the smallest numbers in the analysis. I want to be able to look at the confusion matrix and see if they made a mistake or not.'' This targeted verification requires a description to afford users with precise look-up capabilities, in contrast to descriptions that may be generated when browsing or exploring the data. This context-sensitivity reveals an important aspect of usability: a user's familiarity (or lack thereof) with the data interface. Wayfinding content (e.g., ``Legend. Category: O.'') can help a user remember their location in a structure, and may be useful while they assemble a mental map of the visualization. But, as they become accustomed to the interface and visualization, such descriptions may prove cumbersome. Because user needs depend on their task, preferences, and familiarity, interfaces might afford personalization and customization to facilitate context-sensitive description. 4 Example Gallery Long Description Figure 2: Example structural and navigational schemes generated as part of our co-design process, and applied to diverse chart types. Our co-design process yielded prototypes that demonstrate a breadth of ways to operationalize our design dimensions. Figure~2 excerpts some of our highest-fidelity prototypes, implemented on top of Vega-Lite~[50]. As deeply nested structures and dynamic content are not well-supported by ARIA, we implemented our designs as in-memory data structures. Event listeners update the user's position in the structure on keypress, and write text descriptions to an ARIA-Live region (an ARIA role typically used for temporary notifications). To establish common ground with sighted users, we also render the visualization graphically. The user's position in the tree drives a Vega-Lite selection that highlights points when the screen reader user is attending to them. For every prototype, the up, down, left, and right arrow keys enable structural navigation (moving up or down a level, or stepping through siblings respectively). For example, within the facet level of Fig.~2(a), the user can press left or right keys to move between the six subplots of the multiview chart. On charts that contain a node representing the x-y grid, users can also use the WASD keys to spatially navigate the grid and data points within that branch (mimicking an interaction found in video games). These prototypes highlight different compositions of structures and navigation schemes. Fig.~2(a) includes shift+left and shift+right for lateral navigation across facets: pressing these keys at any node within a facet branch will navigate to the same location under an adjacent branch (subplot). With the chloropleth (Fig.~2(b)), we group data in the encoding structure by U.S. state; users can then drill down into counties across either this branch or the legend one. Fig.~2(c) offers two different paths for drilling down: month first, or weather first. Fig.~2(d) structures the tree by annotations rather than encoding: users can descend into the time intervals designated by the orange and blue rectangles, and view points within those intervals. Finally, Fig.~2(e) organizes its tree in terms of data, offering a binary search structure through the years. The examples shown in Fig.~2 are available as interactive prototypes in the supplementary material. The video below, narrated by Hajas, demonstrates how hierarchical structures and keyboard navigation works in practice.},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/IGM3VFYF/Zong et al. - 2022 - Rich Screen Reader Experiences for Accessible Data.pdf}
}

@misc{zotero-1185,
  title = {{{StatChat}}},
  urldate = {2021-07-21},
  howpublished = {https://zief0002.github.io/statchat/},
  file = {/Users/amcnamara/Zotero/storage/9WTGFP9D/statchat.html}
}

@misc{zotero-1206,
  title = {Data-8.Github.Io},
  journal = {data-8.github.io},
  urldate = {2021-07-18},
  howpublished = {http://data8.org/},
  langid = {american},
  file = {/Users/amcnamara/Zotero/storage/E95I6G5D/data8.org.html}
}

@misc{zotero-1207,
  title = {Data 8: {{Foundations}} of {{Data Science}} {\textbar} {{Computing}}, {{Data Science}}, and {{Society}}},
  urldate = {2021-07-18},
  howpublished = {https://data.berkeley.edu/education/courses/data-8}
}

@misc{zotero-1340,
  title = {{{StatKey}}},
  urldate = {2021-09-27},
  howpublished = {https://www.lock5stat.com/StatKey/index.html},
  file = {/Users/amcnamara/Zotero/storage/LVG6JDAB/index.html}
}

@misc{zotero-1877,
  title = {Digital {{Pedagogy}} {\textbar} {{Code}}},
  urldate = {2024-01-15},
  langid = {english},
  file = {/Users/amcnamara/Zotero/storage/AY7FQGH9/Code.html}
}

@misc{zotero-643,
  title = {Geographically {{Weighted Visualization}}: {{Interactive Graphics}} for {{Scale-Varying Exploratory Analysis}} {\textbar} {{IEEE Journals}} \& {{Magazine}} {\textbar} {{IEEE Xplore}}},
  urldate = {2021-06-10},
  howpublished = {https://ieeexplore.ieee.org/document/4376135}
}

@misc{zotero-644,
  title = {Exploring the {{Sensitivity}} of {{Choropleths}} under {{Attribute Uncertainty}} {\textbar} {{IEEE Journals}} \& {{Magazine}} {\textbar} {{IEEE Xplore}}},
  urldate = {2021-06-10},
  howpublished = {https://ieeexplore.ieee.org/document/8611178}
}

@misc{zotero-645,
  title = {Tilt {{Map}}: {{Interactive Transitions Between Choropleth Map}}, {{Prism Map}} and {{Bar Chart}} in {{Immersive Environments}} {\textbar} {{IEEE Journals}} \& {{Magazine}} {\textbar} {{IEEE Xplore}}},
  urldate = {2021-06-10},
  howpublished = {https://ieeexplore.ieee.org/document/9123548}
}

@misc{zotero-647,
  title = {Revisiting the {{Modifiable Areal Unit Problem}} in {{Deep Traffic Prediction}} with {{Visual Analytics}} {\textbar} {{IEEE Journals}} \& {{Magazine}} {\textbar} {{IEEE Xplore}}},
  urldate = {2021-06-10},
  howpublished = {https://ieeexplore.ieee.org/document/9228894},
  file = {/Users/amcnamara/Zotero/storage/J8PYD9IW/9228894.html}
}

@misc{zotero-648,
  title = {Evaluating {{Cartogram Effectiveness}} {\textbar} {{IEEE Journals}} \& {{Magazine}} {\textbar} {{IEEE Xplore}}},
  urldate = {2021-06-10},
  howpublished = {https://ieeexplore.ieee.org/document/7792176}
}

@article{zuber2016,
  title = {The {{Flipped Classroom}}, a {{Review}} of the {{Literature}}},
  author = {Zuber, William James},
  year = {2016},
  month = jan,
  journal = {Industrial and Commercial Training},
  volume = {48},
  number = {2},
  pages = {97--103},
  publisher = {Emerald Group Publishing Limited},
  issn = {0019-7858},
  doi = {10.1108/ICT-05-2015-0039},
  urldate = {2021-11-05},
  abstract = {Purpose The purpose of this paper is to explore a set of literature in order to clarify the flipped classroom methods (FCM) theoretical frameworks and to determine if the evidence shows improvements in learning for students in comparison with traditional teaching methods. Design/methodology/approach The paper took a literature review approach and explored five articles selected with specific criteria of being published within 2013-2014 and that used comparisons with flipped and traditional classroom methods that employed analysis of student assessment outcomes. Findings The paper shows inconsistent theoretical frameworks and inconclusive evidence of an improvement in assessment outcomes for students. It finds the research undertaken in the literature is limited in scope and suggests further research into the FCM is required to determine consistent theoretical frameworks and methods. Research limitations/implications The findings of the paper may be limited by the selection of literature reviewed and generalisability therefore researchers are encouraged to explore further. Practical implications The paper holds potential implications to question the consistency, validity and benefits of the flipped classroom. Social implications Many anecdotal articles herald the flipped classroom as a method of improving learning outcomes for students, however, academic literature suggests the evidence is inconclusive and there are implications on using educational methods based on technology. Originality/value The paper identifies the need for further research into the flipped classroom and supports the advancement of educational methodology.},
  keywords = {Education,Flipped classroom,Inverted classroom,Technology},
  file = {/Users/amcnamara/Zotero/storage/I224Y28A/Zuber - 2016 - The flipped classroom, a review of the literature.pdf}
}

@inproceedings{zumboochieng2002,
  title = {The {{Effects}} of {{Various Configurations}} of {{Likert}}, {{Ordered Categorical}}, or {{Rating Scale Data}} on the {{Ordinal Logistic Regression Pseudo R-Squared Measure}} of {{Fit}}: {{The Case}} of the {{Cummulative Logit Model}}},
  shorttitle = {The {{Effects}} of {{Various Configurations}} of {{Likert}}, {{Ordered Categorical}}, or {{Rating Scale Data}} on the {{Ordinal Logistic Regression Pseudo R-Squared Measure}} of {{Fit}}},
  booktitle = {American {{Educational Research Association Conference}}},
  author = {Zumbo, Bruno D. and Ochieng, Charles O.},
  year = {2002},
  month = apr,
  address = {New Orleans},
  urldate = {2021-10-01},
  abstract = {Many measures found in educational research are ordered categorical response variables that are empirical realizations of an underlying normally distributed variate. These ordered categorical variables are commonly referred to as Likert or rating scale data. Regression models are commonly fit using these ordered categorical variables as the criterion (i.e., dependent or response) variable; however, a common recommendation in the methodological literature is that researchers make use of ordinal logistic regression when they have these ordered categorical response variables. An advantage of ordinal logistic regression is that it provides a pseudo R-squared measure of fit so that researchers may find this regression model familiar and hence appealing. This study investigated how the  pseudo R-squared fit statistic in ordinal logistic regression operates under a variety of conditions over a varying number of Likert scale points and skewness of the Likert data. The study also demonstrates how the regular ordinary least-squares R-squared statistic operates in the same conditions as the Likert data. The pseudo R-squared fit statistic operates well in a majority of conditions, and so it is recommended that educational researchers begin to explore the use of ordinal logistic regression in their modeling practice with ordered categorical data. Using the pseudo R-squared index will ease the transition to ordinal logistic regression because it provides a sense of familiarity to the researcher. (Contains 4 figures, 11 tables, and 6 references.) (Author/SLD)},
  langid = {english},
  keywords = {Educational Research,Goodness of Fit,Likert Scales,Monte Carlo Methods,Rating Scales,Regression (Statistics),Simulation},
  file = {/Users/amcnamara/Zotero/storage/BPWC6PTC/Zumbo and Ochieng - 2002 - The Effects of Various Configurations of Likert, O.pdf;/Users/amcnamara/Zotero/storage/4I3DGQ84/eric.ed.gov.html}
}
